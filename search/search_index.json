{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Splice Agent Documentation","text":"<p>Welcome to the Splice Agent documentation! This directory contains comprehensive documentation for the splice site analysis framework.</p>"},{"location":"#documentation-structure","title":"\ud83d\udcda Documentation Structure","text":""},{"location":"#project-organization","title":"Project Organization","text":"<ul> <li>PACKAGE_ORGANIZATION.md - Guide for organizing experimental features</li> <li>Where to put new packages (e.g., foundation models like Evo2)</li> <li>Parallel packages vs <code>src/</code> subdirectories</li> <li>Dependency isolation and deployment strategies</li> <li>Import patterns for examples and notebooks</li> </ul>"},{"location":"#core-documentation","title":"Core Documentation","text":"<ul> <li>architecture/ - System architecture and design patterns</li> <li>Component design</li> <li>Data flow</li> <li>API architecture</li> <li> <p>Extension points</p> </li> <li> <p>installation/ - Installation and setup guides</p> </li> <li>Environment setup</li> <li>Dependencies</li> <li>Configuration</li> <li> <p>Troubleshooting</p> </li> <li> <p>api/ - API reference and usage</p> </li> <li>REST API endpoints</li> <li>Python API</li> <li>Request/response schemas</li> <li> <p>Authentication</p> </li> <li> <p>tutorials/ - Step-by-step tutorials</p> </li> <li>Getting started</li> <li>Common workflows</li> <li>Advanced usage</li> <li> <p>Integration examples</p> </li> <li> <p>biology/ - Biological background and context</p> </li> <li>Splice site biology</li> <li>Alternative splicing</li> <li>Genomic features</li> <li> <p>Domain knowledge</p> </li> <li> <p>workflows/ - Analysis workflows and patterns</p> </li> <li>Predefined analyses</li> <li>Custom workflows</li> <li>Best practices</li> <li>Case studies</li> </ul>"},{"location":"#quick-links","title":"\ud83d\ude80 Quick Links","text":"<ul> <li>Main README - Project overview</li> <li>QUICKSTART - Get started in 5 minutes</li> <li>MIGRATION - Moving to new projects</li> <li>Package Docs - Package-level documentation</li> </ul>"},{"location":"#documentation-types","title":"\ud83d\udcd6 Documentation Types","text":""},{"location":"#1-global-documentation-this-directory","title":"1. Global Documentation (This Directory)","text":"<p>High-level documentation organized by topic. Covers concepts, architecture, and cross-cutting concerns.</p>"},{"location":"#2-package-documentation-agentic_spliceaidocs","title":"2. Package Documentation (<code>agentic_spliceai/docs/</code>)","text":"<p>Package-specific documentation close to the code. API references, module guides, and implementation details.</p>"},{"location":"#3-development-documentation-dev","title":"3. Development Documentation (<code>dev/</code>)","text":"<p>Private development notes - NOT included in git. Contains: - Development notes - Experiments - Tutorials for developers - Refactoring plans - Temporary documents</p>"},{"location":"#finding-what-you-need","title":"\ud83c\udfaf Finding What You Need","text":"<p>I want to...</p> <ul> <li>Get started quickly \u2192 QUICKSTART.md</li> <li>Understand the system \u2192 architecture/</li> <li>Learn splice biology \u2192 biology/</li> <li>Use the API \u2192 api/</li> <li>Follow a tutorial \u2192 tutorials/</li> <li>Set up my environment \u2192 installation/</li> <li>Create custom analyses \u2192 workflows/</li> </ul>"},{"location":"#contributing-to-documentation","title":"\ud83d\udcdd Contributing to Documentation","text":"<p>When adding documentation:</p> <ol> <li>Choose the right location:</li> <li>Conceptual/architectural \u2192 <code>docs/</code></li> <li>Code-specific \u2192 <code>agentic_spliceai/docs/</code></li> <li> <p>Development notes \u2192 <code>dev/</code> (not in git)</p> </li> <li> <p>Follow the structure:</p> </li> <li>Use clear headings</li> <li>Include examples</li> <li>Add cross-references</li> <li> <p>Keep it up-to-date</p> </li> <li> <p>Update this README:</p> </li> <li>Add new sections as needed</li> <li>Keep the structure current</li> <li>Maintain quick links</li> </ol>"},{"location":"#related-resources","title":"\ud83d\udd17 Related Resources","text":"<ul> <li>Splice Agent GitHub</li> <li>Agentic AI Public</li> <li>Chart Agent Docs</li> </ul> <p>Last Updated: 2025-11-19</p>"},{"location":"CONTRIBUTING/","title":"Contributing to Agentic-SpliceAI","text":""},{"location":"CONTRIBUTING/#documentation-structure","title":"Documentation Structure","text":"<p>We follow a clear convention for organizing documentation:</p>"},{"location":"CONTRIBUTING/#1-internal-development-dev","title":"1. Internal Development (<code>dev/</code>)","text":"<p>Purpose: Internal notes, scripts, and development-specific documentation Visibility: NOT pushed to GitHub (in <code>.gitignore</code>) Audience: Project developers only</p> <p>Structure: <pre><code>dev/\n\u251c\u2500\u2500 data/              # Data setup scripts and notes\n\u251c\u2500\u2500 experiments/       # Experimental code and results\n\u251c\u2500\u2500 notes/             # Development notes and ideas\n\u2514\u2500\u2500 scratch/           # Temporary work\n</code></pre></p> <p>Examples: - <code>dev/notes/architecture_ideas.md</code> - Brainstorming notes - <code>dev/experiments/new_feature_test.py</code> - Experimental code - <code>dev/scratch/temp_analysis.ipynb</code> - Temporary notebooks</p>"},{"location":"CONTRIBUTING/#2-public-documentation-docs","title":"2. Public Documentation (<code>docs/</code>)","text":"<p>Purpose: Public-facing documentation, tutorials, and guides Visibility: Pushed to GitHub Audience: External users and contributors</p> <p>Structure: <pre><code>docs/\n\u251c\u2500\u2500 tutorials/         # Step-by-step guides\n\u251c\u2500\u2500 guides/            # How-to guides\n\u251c\u2500\u2500 architecture/      # System architecture docs\n\u2514\u2500\u2500 api/               # API documentation\n</code></pre></p> <p>Examples: - <code>docs/tutorials/getting_started.md</code> - User onboarding - <code>docs/guides/splice_site_analysis.md</code> - Feature guide - <code>docs/architecture/multi_agent_system.md</code> - System design</p>"},{"location":"CONTRIBUTING/#3-scripts-scripts","title":"3. Scripts (<code>scripts/</code>)","text":"<p>Purpose: Public automation scripts and utilities Visibility: Pushed to GitHub Audience: All users</p> <p>Structure: <pre><code>scripts/\n\u251c\u2500\u2500 deployment/        # Deployment automation\n\u251c\u2500\u2500 utils/             # Utility scripts\n\u2514\u2500\u2500 ci/                # CI/CD scripts\n</code></pre></p> <p>Examples: - <code>scripts/deployment/deploy.sh</code> - Deployment automation - <code>scripts/utils/format_code.sh</code> - Code formatting</p>"},{"location":"CONTRIBUTING/#4-package-documentation-packagedocs","title":"4. Package Documentation (<code>&lt;package&gt;/docs/</code>)","text":"<p>Purpose: Package-specific documentation Visibility: Pushed to GitHub Audience: Users of specific packages/modules</p> <p>Structure: <pre><code>server/docs/           # Server package documentation\nagentic_spliceai/docs/ # Core package documentation\n</code></pre></p> <p>Examples: - <code>server/docs/API.md</code> - REST API documentation - <code>server/docs/QUICKSTART.md</code> - Server setup guide</p>"},{"location":"CONTRIBUTING/#5-data-documentation-data","title":"5. Data Documentation (<code>data/</code>)","text":"<p>Purpose: Dataset descriptions and usage Visibility: Pushed to GitHub Audience: Users working with datasets</p> <p>Examples: - <code>data/README.md</code> - Data structure and access guide</p>"},{"location":"CONTRIBUTING/#when-to-use-each","title":"When to Use Each","text":""},{"location":"CONTRIBUTING/#use-dev-for","title":"Use <code>dev/</code> for:","text":"<ul> <li>\u2705 Development notes and brainstorming</li> <li>\u2705 Experimental code</li> <li>\u2705 Personal notes and TODOs</li> <li>\u2705 Temporary files and scratch work</li> <li>\u2705 Work-in-progress that's not ready to share</li> </ul>"},{"location":"CONTRIBUTING/#use-scripts-for","title":"Use <code>scripts/</code> for:","text":"<ul> <li>\u2705 Public automation scripts (deployment, CI/CD)</li> <li>\u2705 Utility scripts for all users</li> <li>\u2705 Build automation</li> <li>\u2705 Code formatting and linting</li> </ul>"},{"location":"CONTRIBUTING/#use-tests-for","title":"Use <code>tests/</code> for:","text":"<ul> <li>\u2705 Private development tests</li> <li>\u2705 Setup scripts with local paths (e.g., <code>tests/data/</code>)</li> <li>\u2705 Test data and fixtures</li> <li>\u2705 Development-specific scripts</li> <li>\u2705 Anything that shouldn't be shared publicly</li> </ul>"},{"location":"CONTRIBUTING/#use-docs-for","title":"Use <code>docs/</code> for:","text":"<ul> <li>\u2705 User-facing tutorials</li> <li>\u2705 Feature documentation</li> <li>\u2705 Architecture and design docs</li> <li>\u2705 Contributing guidelines</li> <li>\u2705 API references</li> </ul>"},{"location":"CONTRIBUTING/#use-packagedocs-for","title":"Use <code>&lt;package&gt;/docs/</code> for:","text":"<ul> <li>\u2705 Package-specific guides</li> <li>\u2705 Module API documentation</li> <li>\u2705 Package quickstart guides</li> </ul>"},{"location":"CONTRIBUTING/#file-organization-best-practices","title":"File Organization Best Practices","text":""},{"location":"CONTRIBUTING/#naming-conventions","title":"Naming Conventions","text":"<p>Development files (in <code>dev/</code>): - Use descriptive names: <code>setup_data_symlinks.sh</code> - Include dates for time-sensitive notes: <code>2025-11-26_meeting_notes.md</code> - Use prefixes for categories: <code>exp_new_feature.py</code>, <code>note_architecture.md</code></p> <p>Public documentation (in <code>docs/</code>): - Use clear, user-friendly names: <code>getting_started.md</code> - Follow consistent naming: <code>tutorial_*.md</code>, <code>guide_*.md</code> - Use lowercase with underscores: <code>api_reference.md</code></p>"},{"location":"CONTRIBUTING/#documentation-standards","title":"Documentation Standards","text":"<p>All public docs should include: 1. Clear title and purpose 2. Table of contents (for long docs) 3. Code examples with syntax highlighting 4. Links to related documentation 5. Last updated date (optional)</p> <p>Internal dev docs can be: - More informal and concise - Work-in-progress - Personal notes style</p>"},{"location":"CONTRIBUTING/#git-workflow","title":"Git Workflow","text":""},{"location":"CONTRIBUTING/#what-gets-committed","title":"What Gets Committed","text":"<p>Always commit: - Public documentation (<code>docs/</code>, <code>&lt;package&gt;/docs/</code>) - README files - Contributing guidelines - Data documentation (<code>data/README.md</code>)</p> <p>Never commit: - Internal development files (<code>dev/</code>) - Personal notes - Experimental code (unless ready for review) - Temporary files</p>"},{"location":"CONTRIBUTING/#gitignore-configuration","title":"<code>.gitignore</code> Configuration","text":"<p>The <code>dev/</code> and <code>tests/</code> directories are configured in <code>.gitignore</code>:</p> <pre><code># Development directory (NOT for public)\ndev/\ndev/**/*\n\n# Tests directory (NOT for public - development only)\ntests/\ntests/**/*\n</code></pre>"},{"location":"CONTRIBUTING/#examples-from-this-project","title":"Examples from This Project","text":""},{"location":"CONTRIBUTING/#private-test-scripts","title":"Private Test Scripts","text":"<ul> <li><code>tests/data/setup_data_symlinks.sh</code> - Data symlink setup (private paths)</li> <li><code>tests/data/README.md</code> - Private script documentation</li> </ul>"},{"location":"CONTRIBUTING/#public-documentation","title":"Public Documentation","text":"<ul> <li><code>README.md</code> - Project overview</li> <li><code>QUICKSTART.md</code> - Quick start guide</li> <li><code>SETUP.md</code> - Environment setup</li> <li><code>data/README.md</code> - Data structure guide</li> </ul>"},{"location":"CONTRIBUTING/#package-documentation","title":"Package Documentation","text":"<ul> <li><code>server/README.md</code> - Server package overview</li> <li><code>server/QUICKSTART.md</code> - Server quick start</li> </ul>"},{"location":"CONTRIBUTING/#contributing-documentation","title":"Contributing Documentation","text":"<p>When adding new documentation:</p> <ol> <li>Determine visibility: Internal or public?</li> <li>Choose location: <code>dev/</code>, <code>docs/</code>, or <code>&lt;package&gt;/docs/</code></li> <li>Follow conventions: Use appropriate naming and structure</li> <li>Link related docs: Cross-reference where helpful</li> <li>Keep it updated: Update docs when code changes</li> </ol>"},{"location":"CONTRIBUTING/#questions","title":"Questions?","text":"<p>If you're unsure where to put something: - Will external users need this? \u2192 <code>docs/</code> or <code>&lt;package&gt;/docs/</code> - Is this a useful public script? \u2192 <code>scripts/</code> or <code>scripts/&lt;topic&gt;/</code> - Is this a private script or test? \u2192 <code>tests/</code> - Is this internal/temporary? \u2192 <code>dev/</code> - Is this about datasets? \u2192 <code>data/README.md</code></p> <p>When in doubt, start in <code>dev/</code> and promote to appropriate location when ready.</p>"},{"location":"PACKAGE_ORGANIZATION/","title":"Package Organization Guide","text":"<p>Date: 2026-01-30 Audience: Developers working on experimental/research features</p>"},{"location":"PACKAGE_ORGANIZATION/#current-project-structure","title":"\ud83d\udcc2 Current Project Structure","text":"<pre><code>agentic-spliceai/\n\u251c\u2500\u2500 src/                          # Core production code (pip installable)\n\u2502   \u2514\u2500\u2500 agentic_spliceai/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 splice_engine/\n\u2502           \u251c\u2500\u2500 base_layer/       # SpliceAI/OpenSpliceAI\n\u2502           \u251c\u2500\u2500 meta_layer/       # Multimodal DL\n\u2502           \u251c\u2500\u2500 prediction/       # Core prediction logic\n\u2502           \u2514\u2500\u2500 resources/        # Resource management\n\u2502\n\u251c\u2500\u2500 examples/                     # Driver scripts (fast iteration)\n\u2502   \u251c\u2500\u2500 base_layer/\n\u2502   \u251c\u2500\u2500 data_preparation/\n\u2502   \u2514\u2500\u2500 ...\n\u2502\n\u251c\u2500\u2500 notebooks/                    # Educational Jupyter notebooks\n\u2502\n\u251c\u2500\u2500 scripts/                      # Utilities, validation, tools\n\u2502\n\u251c\u2500\u2500 tests/                        # Unit &amp; integration tests\n\u2502\n\u251c\u2500\u2500 dev/                          # Private development docs\n\u2502   \u251c\u2500\u2500 sessions/\n\u2502   \u251c\u2500\u2500 planning/\n\u2502   \u2514\u2500\u2500 research/\n\u2502\n\u251c\u2500\u2500 docs/                         # Public documentation\n\u2502\n\u2514\u2500\u2500 (experimental packages)?      # \u2190 WHERE TO PUT EXPERIMENTAL CODE?\n</code></pre>"},{"location":"PACKAGE_ORGANIZATION/#where-to-put-experimental-features","title":"\ud83e\uddea Where to Put Experimental Features?","text":""},{"location":"PACKAGE_ORGANIZATION/#decision-framework","title":"Decision Framework","text":"<p>For experimental features like Evo2 foundation model, consider these factors:</p> Factor Put in <code>src/</code> Put in parallel package Put in <code>examples/</code> Stability Stable, tested Experimental, evolving Demo/prototype Integration Core system Optional add-on Quick test Dependencies Standard deps Heavy deps (GPU models) Minimal deps Audience All users Research users Developers Installation Always installed Optional install Not installed Maintenance Long-term Research cycle Temporary"},{"location":"PACKAGE_ORGANIZATION/#recommendation-for-evo2","title":"\ud83d\udccb Recommendation for Evo2","text":""},{"location":"PACKAGE_ORGANIZATION/#option-1-parallel-package-recommended","title":"\u2705 Option 1: Parallel Package (RECOMMENDED)","text":"<p>Structure: <pre><code>agentic-spliceai/\n\u251c\u2500\u2500 src/                          # Core production code\n\u2502   \u2514\u2500\u2500 agentic_spliceai/\n\u2502\n\u251c\u2500\u2500 foundation_models/            # \u2190 Parallel experimental package\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 pyproject.toml           # Separate dependencies\n\u2502   \u251c\u2500\u2500 environment-evo2.yml     # GPU-specific environment\n\u2502   \u2514\u2500\u2500 evo2/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 model.py             # Evo2 integration\n\u2502       \u251c\u2500\u2500 inference.py         # Inference logic\n\u2502       \u2514\u2500\u2500 adapters/\n\u2502           \u2514\u2500\u2500 spliceai_adapter.py  # Bridge to splice_engine\n\u2502\n\u251c\u2500\u2500 examples/\n\u2502   \u2514\u2500\u2500 foundation_models/       # Evo2 examples\n\u2502       \u2514\u2500\u2500 01_evo2_prediction.py\n\u2502\n\u2514\u2500\u2500 notebooks/\n    \u2514\u2500\u2500 foundation_models/       # Evo2 tutorials\n        \u2514\u2500\u2500 01_evo2_basics.ipynb\n</code></pre></p> <p>Benefits: - \u2705 Optional installation: Users can install core without heavy deps - \u2705 Separate dependencies: Evo2 needs specific GPU libs, separate <code>environment-evo2.yml</code> - \u2705 Clear boundaries: Experimental code doesn't pollute production - \u2705 Easy testing: Can run core tests without Evo2 dependencies - \u2705 Flexible deployment: Deploy to pod without installing on local machine - \u2705 Independent evolution: Update Evo2 without touching core</p> <p>Installation: <pre><code># Core only (local development)\npip install -e .\n\n# Core + Evo2 (pod with A40)\npip install -e .\npip install -e ./foundation_models\n\n# Or with conda\nmamba env create -f foundation_models/environment-evo2.yml\n</code></pre></p> <p>Imports (from examples): <pre><code># Add both packages to path\nsys.path.insert(0, str(project_root / 'src'))\nsys.path.insert(0, str(project_root / 'foundation_models'))\n\n# Import core\nfrom agentic_spliceai.splice_engine.base_layer import ...\n\n# Import experimental\nfrom evo2.model import Evo2SplicePredictor\nfrom evo2.adapters import adapt_to_spliceai_format\n</code></pre></p>"},{"location":"PACKAGE_ORGANIZATION/#option-2-subdirectory-in-src-if-stable","title":"Option 2: Subdirectory in <code>src/</code> (If stable)","text":"<p>Structure: <pre><code>src/\n\u2514\u2500\u2500 agentic_spliceai/\n    \u251c\u2500\u2500 splice_engine/\n    \u251c\u2500\u2500 experimental/             # \u2190 Experimental features\n    \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u251c\u2500\u2500 foundation_models/\n    \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n    \u2502   \u2502   \u251c\u2500\u2500 evo2/\n    \u2502   \u2502   \u2514\u2500\u2500 ...\n    \u2502   \u2514\u2500\u2500 README.md\n    \u2514\u2500\u2500 ...\n</code></pre></p> <p>Use when: - Features are relatively stable - Dependencies are manageable (not too heavy) - You want unified installation - Features will eventually become core</p> <p>Drawbacks: - \u274c All users install experimental dependencies - \u274c Harder to separate GPU-specific code - \u274c More coupling with core system</p>"},{"location":"PACKAGE_ORGANIZATION/#option-3-separate-git-submodule-if-very-independent","title":"Option 3: Separate Git Submodule (If very independent)","text":"<p>Structure: <pre><code>agentic-spliceai/\n\u251c\u2500\u2500 src/\n\u251c\u2500\u2500 submodules/\n\u2502   \u2514\u2500\u2500 agentic-evo2/            # \u2190 Separate git repo as submodule\n\u2502       \u251c\u2500\u2500 .git\n\u2502       \u251c\u2500\u2500 README.md\n\u2502       \u251c\u2500\u2500 pyproject.toml\n\u2502       \u2514\u2500\u2500 evo2/\n\u2514\u2500\u2500 ...\n</code></pre></p> <p>Use when: - Feature is very independent (could be its own project) - Multiple projects might use it - Different development team/cycle - Want separate version control</p>"},{"location":"PACKAGE_ORGANIZATION/#specific-recommendation-for-evo2","title":"\ud83d\udca1 Specific Recommendation for Evo2","text":""},{"location":"PACKAGE_ORGANIZATION/#recommended-structure","title":"Recommended Structure","text":"<p>Use Option 1 (Parallel Package) because: 1. Heavy GPU dependencies (need A40, separate environment) 2. Pod-specific testing (won't run on local Mac) 3. Research-oriented (may change rapidly) 4. Optional feature (not all users need it)</p> <p>Implementation:</p> <pre><code>agentic-spliceai/\n\u251c\u2500\u2500 src/agentic_spliceai/         # Core (always installed)\n\u2502\n\u251c\u2500\u2500 foundation_models/            # Experimental (optional)\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 README.md                 # Installation &amp; usage\n\u2502   \u251c\u2500\u2500 pyproject.toml\n\u2502   \u2502   [project]\n\u2502   \u2502   name = \"agentic-spliceai-foundation-models\"\n\u2502   \u2502   dependencies = [\n\u2502   \u2502       \"agentic-spliceai\",   # Depends on core\n\u2502   \u2502       \"evo-model&gt;=2.0\",     # Evo2 specific\n\u2502   \u2502       \"transformers&gt;=4.30\",\n\u2502   \u2502       \"torch&gt;=2.0+cu118\"\n\u2502   \u2502   ]\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 environment-evo2.yml      # Pod environment\n\u2502   \u2502   name: agentic-spliceai-evo2\n\u2502   \u2502   channels: [nvidia, pytorch, conda-forge]\n\u2502   \u2502   dependencies:\n\u2502   \u2502     - python=3.11\n\u2502   \u2502     - pytorch::pytorch&gt;=2.0\n\u2502   \u2502     - pytorch::pytorch-cuda=11.8\n\u2502   \u2502     - evo-model\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 evo2/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 config.py\n\u2502   \u2502   \u251c\u2500\u2500 model.py             # Evo2ModelWrapper\n\u2502   \u2502   \u251c\u2500\u2500 inference.py         # Inference pipeline\n\u2502   \u2502   \u2514\u2500\u2500 adapters/\n\u2502   \u2502       \u2514\u2500\u2500 spliceai_adapter.py\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 tests/\n\u2502       \u2514\u2500\u2500 test_evo2_integration.py\n\u2502\n\u251c\u2500\u2500 examples/foundation_models/\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 01_evo2_setup.md         # Setup on pod\n\u2502   \u2514\u2500\u2500 02_evo2_prediction.py    # Usage example\n\u2502\n\u2514\u2500\u2500 notebooks/foundation_models/\n    \u2514\u2500\u2500 01_evo2_splice_prediction.ipynb\n</code></pre>"},{"location":"PACKAGE_ORGANIZATION/#integration-pattern","title":"\ud83d\udd27 Integration Pattern","text":""},{"location":"PACKAGE_ORGANIZATION/#core-experimental-bridge","title":"Core \u2192 Experimental Bridge","text":"<p>In <code>src/agentic_spliceai/splice_engine/base_layer/models/</code>: <pre><code># registry.py - Model registry with dynamic loading\n\n_MODEL_REGISTRY = {\n    'spliceai': 'agentic_spliceai.splice_engine.base_layer.prediction.core.load_spliceai',\n    'openspliceai': 'agentic_spliceai.splice_engine.base_layer.prediction.core.load_openspliceai',\n}\n\ndef register_foundation_model(name: str, loader_path: str):\n    \"\"\"Register experimental foundation model (optional).\"\"\"\n    _MODEL_REGISTRY[name] = loader_path\n\ndef load_model(model_name: str, **kwargs):\n    \"\"\"Load model by name (core or experimental).\"\"\"\n    if model_name not in _MODEL_REGISTRY:\n        raise ValueError(f\"Unknown model: {model_name}\")\n\n    module_path, func_name = _MODEL_REGISTRY[model_name].rsplit('.', 1)\n    module = importlib.import_module(module_path)\n    loader = getattr(module, func_name)\n    return loader(**kwargs)\n</code></pre></p> <p>In <code>foundation_models/evo2/__init__.py</code>: <pre><code># Auto-register when foundation_models is imported\nfrom agentic_spliceai.splice_engine.base_layer.models.registry import register_foundation_model\n\nregister_foundation_model(\n    'evo2',\n    'evo2.model.load_evo2_splice_model'\n)\n</code></pre></p> <p>Usage: <pre><code># Works with or without foundation_models installed\n\n# Core models (always available)\nrunner = BaseModelRunner()\nresult = runner.run_single_model(model_name='openspliceai', ...)\n\n# Experimental models (if foundation_models installed)\ntry:\n    import evo2  # Triggers registration\n    result = runner.run_single_model(model_name='evo2', ...)\nexcept ImportError:\n    print(\"Evo2 not available - install foundation_models package\")\n</code></pre></p>"},{"location":"PACKAGE_ORGANIZATION/#installation-workflows","title":"\ud83d\udce6 Installation Workflows","text":""},{"location":"PACKAGE_ORGANIZATION/#local-development-core-only","title":"Local Development (Core Only)","text":"<pre><code>cd agentic-spliceai\npip install -e .                  # Core only\npython examples/base_layer/01_phase1_prediction.py\n</code></pre>"},{"location":"PACKAGE_ORGANIZATION/#pod-development-core-evo2","title":"Pod Development (Core + Evo2)","text":"<pre><code># On pod with A40\ncd agentic-spliceai\n\n# Create Evo2 environment\nmamba env create -f foundation_models/environment-evo2.yml\nconda activate agentic-spliceai-evo2\n\n# Install both packages\npip install -e .                  # Core\npip install -e ./foundation_models  # Evo2\n\n# Test\npython examples/foundation_models/02_evo2_prediction.py\n</code></pre>"},{"location":"PACKAGE_ORGANIZATION/#cicd-testing","title":"CI/CD Testing","text":"<pre><code># .github/workflows/test.yml\njobs:\n  test-core:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - run: pip install -e .\n      - run: pytest tests/\n\n  test-evo2:\n    runs-on: [self-hosted, gpu-a40]  # Pod runner\n    steps:\n      - uses: actions/checkout@v3\n      - run: conda env create -f foundation_models/environment-evo2.yml\n      - run: pip install -e . &amp;&amp; pip install -e ./foundation_models\n      - run: pytest foundation_models/tests/\n</code></pre>"},{"location":"PACKAGE_ORGANIZATION/#summary-decision","title":"\ud83d\udcdd Summary &amp; Decision","text":""},{"location":"PACKAGE_ORGANIZATION/#for-evo2-foundation-model-use-parallel-package","title":"For Evo2 Foundation Model: Use Parallel Package \u2705","text":"<p>Rationale: 1. Deployment flexibility: Run core locally, Evo2 on pod 2. Dependency isolation: Heavy GPU libs separate 3. Development speed: Rapid iteration without core changes 4. Optional feature: Users can choose to install 5. Clear boundaries: Experimental vs production</p> <p>File to Create: <pre><code># Create parallel package\nmkdir -p foundation_models/evo2/adapters\ntouch foundation_models/{__init__.py,README.md,pyproject.toml}\ntouch foundation_models/evo2/{__init__.py,model.py,inference.py}\ntouch foundation_models/evo2/adapters/spliceai_adapter.py\ntouch foundation_models/environment-evo2.yml\n</code></pre></p> <p>Import Pattern (in examples): <pre><code># examples/foundation_models/02_evo2_prediction.py\n\nimport sys\nfrom pathlib import Path\n\n# Add both packages\nproject_root = Path(__file__).parent.parent\nsys.path.insert(0, str(project_root / 'src'))\nsys.path.insert(0, str(project_root / 'foundation_models'))\n\n# Import core\nfrom agentic_spliceai.splice_engine.base_layer import ...\n\n# Import experimental\nfrom evo2.model import Evo2SplicePredictor\n</code></pre></p>"},{"location":"PACKAGE_ORGANIZATION/#next-steps","title":"\ud83c\udfaf Next Steps","text":"<ol> <li>Create <code>foundation_models/</code> package</li> <li>Set up <code>environment-evo2.yml</code> with GPU dependencies</li> <li>Implement <code>evo2/model.py</code> with Evo2 wrapper</li> <li>Create adapter to SpliceAI format</li> <li>Write example in <code>examples/foundation_models/</code></li> <li>Document pod setup in <code>foundation_models/README.md</code></li> </ol> <p>This guide follows the pattern established by <code>genai-lab</code> and other research-oriented projects in your workspace.</p>"},{"location":"QUICKSTART/","title":"Agentic SpliceAI - Quick Start Guide","text":"<p>Get started with splice site analysis and research capabilities in 5 minutes!</p>"},{"location":"QUICKSTART/#prerequisites","title":"\ud83d\udccb Prerequisites","text":"<ol> <li>Python 3.10+</li> <li>API Keys:</li> <li>OpenAI API Key (required) - Get one at https://platform.openai.com/api-keys</li> <li>Tavily API Key (optional, for Nexus web search) - Get one at https://tavily.com</li> <li>Splice Site Dataset - TSV/CSV file with genomic coordinates (for splice analysis)</li> <li>LaTeX (optional, for Nexus PDF generation) - MacTeX, BasicTeX, or TeX Live</li> </ol>"},{"location":"QUICKSTART/#installation","title":"\ud83d\ude80 Installation","text":""},{"location":"QUICKSTART/#step-1-set-up-environment","title":"Step 1: Set Up Environment","text":"<p>Option A: Use Existing <code>agentic-ai</code> Environment (Recommended)</p> <pre><code># Activate the existing environment\nmamba activate agentic-ai\n\n# Navigate to agentic_spliceai directory\ncd agentic_spliceai\n\n# All dependencies are already installed!\n</code></pre> <p>Option B: Create Standalone Environment</p> <pre><code># Create new conda environment from environment.yml\nmamba env create -f environment.yml\nmamba activate agentic-spliceai\n\n# Install package in editable mode (for development)\npip install -e .\n</code></pre> <p>Option C: Active Development Mode</p> <p>For active development where you're modifying the code:</p> <pre><code># Create environment manually\nmamba create -n agentic-spliceai python=3.11\nmamba activate agentic-spliceai\n\n# Install in editable mode with dependencies\npip install -e .\n\n# Or install with optional dev dependencies\npip install -e \".[dev,bio]\"\n</code></pre> <p>Alternative: Using requirements.txt (backward compatibility)</p> <pre><code># Create environment manually\nmamba create -n agentic-spliceai python=3.11\nmamba activate agentic-spliceai\n\n# Install dependencies via pip\npip install -r requirements.txt\n</code></pre> <p>Note: The <code>agentic-ai</code> environment already contains all required dependencies (OpenAI, FastAPI, DuckDB, matplotlib, pandas, seaborn, etc.), so you can use it directly for <code>agentic_spliceai</code>. The <code>environment.yml</code> is the recommended approach for new installations. Use <code>pip install -e .</code> for development to enable import of <code>agentic_spliceai</code> modules from anywhere.</p>"},{"location":"QUICKSTART/#step-2-configure-api-key","title":"Step 2: Configure API Key","text":"<p>Option A: Use Existing Project .env (Recommended)</p> <pre><code># The .env file at the project root (agentic-ai-public/.env) is already used\n# No action needed if OPENAI_API_KEY is already set there\n</code></pre> <p>Option B: Create Local .env</p> <pre><code># Copy environment template\ncp .env.example .env\n\n# Edit .env and add your OpenAI API key\n# OPENAI_API_KEY=sk-your-actual-key-here\n</code></pre> <p>Note: Python's <code>dotenv</code> will automatically search parent directories for <code>.env</code> files, so the project root <code>.env</code> will be found automatically.</p>"},{"location":"QUICKSTART/#step-3-add-your-data","title":"Step 3: Add Your Data","text":"<pre><code># Place your splice site dataset in the data/ directory\n# Example: data/splice_sites_enhanced.tsv\n</code></pre>"},{"location":"QUICKSTART/#usage-options","title":"\ud83c\udfaf Usage Options","text":""},{"location":"QUICKSTART/#option-1-rest-api-recommended","title":"Option 1: REST API (Recommended)","text":"<p>Start the service:</p> <pre><code>cd server\npython splice_service.py\n</code></pre> <p>Access Swagger UI:</p> <p>Open http://localhost:8004/docs in your browser</p> <p>Try an analysis:</p> <ol> <li>Click on <code>/analyze/template</code></li> <li>Click \"Try it out\"</li> <li>Use this example request:</li> </ol> <pre><code>{\n  \"dataset_path\": \"data/splice_sites_enhanced.tsv\",\n  \"analysis_type\": \"high_alternative_splicing\",\n  \"model\": \"gpt-4o-mini\"\n}\n</code></pre> <ol> <li>Click \"Execute\"</li> <li>Copy the generated code from the response</li> <li>Save it to a <code>.py</code> file and run it!</li> </ol>"},{"location":"QUICKSTART/#option-2-python-library","title":"Option 2: Python Library","text":"<pre><code>from agentic_spliceai import create_dataset\nfrom agentic_spliceai.splice_analysis import generate_analysis_insight\nfrom openai import OpenAI\n\n# Load dataset\ndataset = create_dataset(\"data/splice_sites_enhanced.tsv\")\n\n# Generate analysis\nclient = OpenAI()\nresult = generate_analysis_insight(\n    dataset=dataset,\n    analysis_type=\"high_alternative_splicing\",\n    client=client\n)\n\n# Save code\nwith open(\"analysis.py\", \"w\") as f:\n    f.write(result[\"chart_code\"])\n\n# Execute\nexec(result[\"chart_code\"])\n</code></pre>"},{"location":"QUICKSTART/#option-3-command-line-tool","title":"Option 3: Command-Line Tool","text":"<pre><code># Run quick start examples\npython examples/quick_start.py\n\n# Run full analysis suite\npython -m agentic_spliceai.examples.analyze_splice_sites \\\n    --data data/splice_sites_enhanced.tsv \\\n    --analysis all \\\n    --output-dir output/analyses\n</code></pre>"},{"location":"QUICKSTART/#available-analyses","title":"\ud83d\udcca Available Analyses","text":""},{"location":"QUICKSTART/#1-high-alternative-splicing","title":"1. High Alternative Splicing","text":"<p>Identifies genes with the most splice sites (potential for alternative splicing)</p> <pre><code>analysis_type = \"high_alternative_splicing\"\n</code></pre>"},{"location":"QUICKSTART/#2-genomic-distribution","title":"2. Genomic Distribution","text":"<p>Visualizes splice site distribution across chromosomes</p> <pre><code>analysis_type = \"splice_site_genomic_view\"\n</code></pre>"},{"location":"QUICKSTART/#3-exon-complexity","title":"3. Exon Complexity","text":"<p>Analyzes transcript structure by exon count</p> <pre><code>analysis_type = \"exon_complexity\"\n</code></pre>"},{"location":"QUICKSTART/#4-strand-bias","title":"4. Strand Bias","text":"<p>Analyzes strand distribution of splice sites</p> <pre><code>analysis_type = \"strand_bias\"\n</code></pre>"},{"location":"QUICKSTART/#5-transcript-diversity","title":"5. Transcript Diversity","text":"<p>Identifies genes with most transcript isoforms</p> <pre><code>analysis_type = \"gene_transcript_diversity\"\n</code></pre>"},{"location":"QUICKSTART/#custom-research-questions","title":"\ud83d\udd2c Custom Research Questions","text":"<p>Ask your own questions:</p> <pre><code>from agentic_spliceai.splice_analysis import generate_exploratory_insight\n\nresult = generate_exploratory_insight(\n    dataset=dataset,\n    research_question=\"How do splice sites distribute by gene biotype?\",\n    client=client\n)\n</code></pre> <p>Example questions: - \"What is the relationship between gene length and splice site density?\" - \"Which chromosomes have the highest alternative splicing rates?\" - \"How do donor and acceptor sites differ in their genomic distribution?\"</p>"},{"location":"QUICKSTART/#nexus-research-agent-new","title":"\ud83d\udcda Nexus Research Agent (NEW)","text":"<p>Generate comprehensive research reports on splicing topics:</p>"},{"location":"QUICKSTART/#cli-usage","title":"CLI Usage","text":"<pre><code># Basic research report\nnexus \"Alternative Splicing Mechanisms in Cancer\"\n\n# With PDF generation\nnexus \"SpliceAI Deep Learning Architecture\" --pdf\n\n# Comprehensive report\nnexus \"Splice Site Recognition by U1 snRNP\" \\\n  --model openai:gpt-4o \\\n  --length comprehensive \\\n  --pdf\n\n# Quick literature review\nnexus \"Recent advances in splice site prediction\" \\\n  --model openai:gpt-4o-mini \\\n  --length brief\n</code></pre>"},{"location":"QUICKSTART/#python-api","title":"Python API","text":"<pre><code>from nexus.agents.research import ResearchAgent\nfrom nexus.core.config import Config\n\n# Initialize research agent\nconfig = Config()\nagent = ResearchAgent(config)\n\n# Generate research report\nresult = agent.research(\n    topic=\"Splice Site Recognition by U1 snRNP\",\n    length=\"standard\",\n    generate_pdf=True\n)\n\nprint(f\"Report saved to: {result['output_path']}\")\n</code></pre>"},{"location":"QUICKSTART/#web-interface","title":"Web Interface","text":"<pre><code># Start Nexus web server\nnexus-server\n\n# Access at http://localhost:8004\n</code></pre>"},{"location":"QUICKSTART/#use-cases","title":"Use Cases","text":"<ul> <li>Literature Review: Research latest splicing mechanisms before analysis</li> <li>Grant Proposals: Generate comprehensive background sections</li> <li>Method Validation: Validate analysis approaches with current research</li> <li>Stay Updated: Keep up with latest splice prediction methods</li> <li>Self-Improvement: Learn from research to enhance analysis methods</li> </ul>"},{"location":"QUICKSTART/#api-examples","title":"\ud83c\udf10 API Examples","text":""},{"location":"QUICKSTART/#list-available-analyses","title":"List Available Analyses","text":"<pre><code>curl http://localhost:8004/analyses\n</code></pre>"},{"location":"QUICKSTART/#template-analysis","title":"Template Analysis","text":"<pre><code>curl -X POST http://localhost:8004/analyze/template \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"dataset_path\": \"data/splice_sites_enhanced.tsv\",\n    \"analysis_type\": \"high_alternative_splicing\",\n    \"model\": \"gpt-4o-mini\"\n  }'\n</code></pre>"},{"location":"QUICKSTART/#exploratory-analysis","title":"Exploratory Analysis","text":"<pre><code>curl -X POST http://localhost:8004/analyze/exploratory \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"dataset_path\": \"data/splice_sites_enhanced.tsv\",\n    \"research_question\": \"What is the distribution of splice sites across chromosomes?\",\n    \"model\": \"gpt-4o-mini\"\n  }'\n</code></pre>"},{"location":"QUICKSTART/#data-format","title":"\ud83d\udcc1 Data Format","text":"<p>Your dataset should have these columns:</p> <p>Required: - <code>chrom</code> - Chromosome (chr1, chr2, ..., chrX, chrY) - <code>position</code> - Genomic position - <code>site_type</code> - donor or acceptor - <code>strand</code> - + or -</p> <p>Optional: - <code>gene_name</code> - Gene symbol (TP53, BRCA1, etc.) - <code>transcript_id</code> - Transcript identifier - <code>exon_rank</code> - Exon number</p> <p>Example:</p> <pre><code>chrom  position  site_type  strand  gene_name  transcript_id  exon_rank\nchr1   12345     donor      +       TP53       NM_000546.6    5\nchr1   12678     acceptor   +       TP53       NM_000546.6    6\n</code></pre>"},{"location":"QUICKSTART/#next-steps","title":"\ud83c\udf93 Next Steps","text":"<ol> <li>Try the examples - Run <code>python examples/quick_start.py</code></li> <li>Explore the API - Open http://localhost:8004/docs</li> <li>Read the docs - See README.md for detailed information</li> <li>Customize analyses - Modify generated code to fit your needs</li> <li>Add new templates - Extend <code>splice_analysis.py</code> with your own analyses</li> </ol>"},{"location":"QUICKSTART/#troubleshooting","title":"\ud83d\udc1b Troubleshooting","text":""},{"location":"QUICKSTART/#api-key-not-found","title":"API Key Not Found","text":"<pre><code># Make sure .env file exists and contains:\nOPENAI_API_KEY=sk-your-actual-key-here\n\n# Or export it:\nexport OPENAI_API_KEY=sk-your-actual-key-here\n</code></pre>"},{"location":"QUICKSTART/#dataset-not-found","title":"Dataset Not Found","text":"<pre><code># Check the path is relative to project root\n# Example: data/splice_sites_enhanced.tsv\n# NOT: /full/path/to/data/splice_sites_enhanced.tsv\n</code></pre>"},{"location":"QUICKSTART/#port-already-in-use","title":"Port Already in Use","text":"<pre><code># Change port in .env:\nSPLICE_AGENT_PORT=8005\n\n# Or in splice_service.py:\nuvicorn.run(..., port=8005)\n</code></pre>"},{"location":"QUICKSTART/#import-errors","title":"Import Errors","text":"<pre><code># Make sure you're in the right environment\nmamba activate agentic-spliceai\n\n# Reinstall dependencies\npip install -r requirements.txt\n</code></pre>"},{"location":"QUICKSTART/#tips","title":"\ud83d\udca1 Tips","text":"<ol> <li>Start with templates - Use predefined analyses before custom questions</li> <li>Review generated code - Always check the code before executing</li> <li>Use appropriate models - <code>gpt-4o-mini</code> for speed, <code>gpt-4o</code> for quality</li> <li>Cache datasets - The API caches loaded datasets for performance</li> <li>Batch processing - Generate multiple analyses at once for efficiency</li> </ol>"},{"location":"QUICKSTART/#more-resources","title":"\ud83d\udcda More Resources","text":"<ul> <li>Full README - Complete documentation</li> <li>API Reference - Detailed API documentation</li> <li>Biology Background - Splice site biology primer</li> <li>Examples - More example scripts</li> </ul> <p>Questions? Open an issue on GitHub or check the documentation!</p>"},{"location":"RUNPODS_SETUP/","title":"RunPods Setup Guide","text":"<p>For: Setting up GPU instances on RunPods for agentic-spliceai training Audience: First-time RunPods users Time: ~15 minutes</p>"},{"location":"RUNPODS_SETUP/#overview","title":"\ud83c\udfaf Overview","text":"<p>RunPods provides on-demand GPU compute for training. This guide shows you how to: 1. Set up SSH access to RunPods instances 2. Configure your environment on the pod 3. Transfer data and start training</p>"},{"location":"RUNPODS_SETUP/#prerequisites","title":"\ud83d\udce6 Prerequisites","text":""},{"location":"RUNPODS_SETUP/#on-your-local-machine","title":"On Your Local Machine","text":"<ul> <li>\u2705 SSH key (<code>~/.ssh/id_ed25519</code> or <code>~/.ssh/id_rsa</code>)</li> <li>\u2705 agentic-spliceai repository cloned</li> <li>\u2705 Bash shell (macOS, Linux, WSL)</li> </ul>"},{"location":"RUNPODS_SETUP/#runpods-account","title":"RunPods Account","text":"<ul> <li>\u2705 RunPods account created</li> <li>\u2705 Payment method added</li> <li>\u2705 SSH public key uploaded to RunPods</li> </ul>"},{"location":"RUNPODS_SETUP/#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"RUNPODS_SETUP/#step-1-set-up-runpods-scripts-one-time","title":"Step 1: Set Up RunPods Scripts (One Time)","text":"<pre><code>cd ~/work/agentic-spliceai\n\n# Copy example templates\ncp -r runpods.example runpods\n\n# Make scripts executable\nchmod +x runpods/scripts/*.sh\n</code></pre> <p>Note: The <code>runpods/</code> directory is NOT tracked in git (it's in <code>.gitignore</code>). This is intentional - it contains your personal configuration.</p>"},{"location":"RUNPODS_SETUP/#step-2-acquire-runpods-instance","title":"Step 2: Acquire RunPods Instance","text":"<ol> <li>Go to runpods.io</li> <li>Click Deploy</li> <li>Select GPU (e.g., A40 48GB, H100 80GB)</li> <li>Use template: PyTorch or Fast Stable Diffusion</li> <li>Click Deploy On-Demand or Deploy Spot</li> <li>Wait for pod to start (~1-2 minutes)</li> </ol>"},{"location":"RUNPODS_SETUP/#step-3-get-connection-info","title":"Step 3: Get Connection Info","text":"<p>From RunPods dashboard:</p> <ol> <li>Click Connect on your pod</li> <li>Select SSH over exposed TCP</li> <li>Copy the connection command:    <pre><code>ssh root@ssh.runpods.io -p 12345 -i ~/.ssh/id_ed25519\n</code></pre></li> <li>Extract:</li> <li>Hostname: <code>ssh.runpods.io</code></li> <li>Port: <code>12345</code></li> </ol>"},{"location":"RUNPODS_SETUP/#step-4-configure-ssh-access-on-your-machine","title":"Step 4: Configure SSH Access (On Your Machine)","text":"<pre><code>cd ~/work/agentic-spliceai/runpods/scripts\n./runpod_ssh_manager.sh add agentic-spliceai\n</code></pre> <p>Enter when prompted: - Hostname: <code>ssh.runpods.io</code> - Port: <code>12345</code> - Nickname: <code>a40-48gb</code> (or whatever helps you remember) - SSH Key: Press Enter for default</p> <p>Result: SSH config entry created</p>"},{"location":"RUNPODS_SETUP/#step-5-test-connection","title":"Step 5: Test Connection","text":"<pre><code>ssh runpod-agentic-spliceai-a40-48gb\n</code></pre> <p>Expected: You're now connected to the pod! \ud83c\udf89</p>"},{"location":"RUNPODS_SETUP/#step-6-setup-environment-on-pod","title":"Step 6: Setup Environment (On Pod)","text":"<p>Now that you're SSH'd into the pod:</p> <pre><code># Install Miniforge\ncd /workspace\nwget https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh\nbash Miniforge3-Linux-x86_64.sh -b -p /workspace/miniforge3\n/workspace/miniforge3/bin/conda init bash\nsource ~/.bashrc\n\n# Clone repository\ncd /workspace\ngit clone https://github.com/YOUR-USERNAME/agentic-spliceai.git\ncd agentic-spliceai\n\n# Create environment\nmamba env create -f runpods/environment-runpods-minimal.yml\nmamba activate agenticspliceai\n\n# Install PyTorch with CUDA\npip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n\n# Install additional packages\npip install transformers einops accelerate safetensors\n\n# Install agentic-spliceai\npip install -e .\n\n# Verify\npython -c \"import torch; print(f'CUDA available: {torch.cuda.is_available()}')\"\npython -c \"from agentic_spliceai.splice_engine.base_layer import BaseModelRunner; print('\u2705 OK')\"\n</code></pre>"},{"location":"RUNPODS_SETUP/#step-7-transfer-data-from-local-machine","title":"Step 7: Transfer Data (From Local Machine)","text":"<p>Open a new terminal on your local machine:</p> <pre><code># Transfer genomic data\nrsync -avzP ~/work/agentic-spliceai/data/ \\\n  runpod-agentic-spliceai-a40-48gb:/workspace/data/\n\n# This can take 10-30 minutes depending on data size\n</code></pre>"},{"location":"RUNPODS_SETUP/#step-8-start-training-on-pod","title":"Step 8: Start Training (On Pod)","text":"<p>Back in your SSH session:</p> <pre><code># Use tmux (survives disconnection)\ntmux new -s training\n\n# Activate environment\ncd /workspace/agentic-spliceai\nmamba activate agenticspliceai\n\n# Run training\npython train.py --config configs/meta_layer_training.yaml\n\n# Detach from tmux: Ctrl-B, then D\n# Reattach later: tmux attach -t training\n</code></pre>"},{"location":"RUNPODS_SETUP/#execution-model","title":"\ud83d\udccb Execution Model","text":""},{"location":"RUNPODS_SETUP/#local-your-machine","title":"LOCAL (Your Machine)","text":"<p>These scripts/commands run on your local machine:</p> Script Purpose <code>runpod_ssh_manager.sh</code> Configure SSH access <code>quick_pod_setup.sh</code> Automated setup <code>rsync</code> commands Transfer data to/from pod <p>Location: <code>~/work/agentic-spliceai/runpods/scripts/</code> Modifies: <code>~/.ssh/config</code> on your machine</p>"},{"location":"RUNPODS_SETUP/#pod-runpods-instance","title":"POD (RunPods Instance)","text":"<p>These run ON the pod (after SSH'ing):</p> <ul> <li>Installing Miniforge</li> <li>Cloning repository</li> <li>Creating conda environment</li> <li>Installing packages</li> <li>Running training scripts</li> </ul>"},{"location":"RUNPODS_SETUP/#tips","title":"\ud83d\udca1 Tips","text":""},{"location":"RUNPODS_SETUP/#use-tmux-always","title":"Use tmux Always","text":"<pre><code># Start session\nssh runpod-agentic-spliceai-a40-48gb -t \"tmux new -s work || tmux attach -t work\"\n\n# Why? Training continues even if SSH drops\n</code></pre>"},{"location":"RUNPODS_SETUP/#monitor-gpu","title":"Monitor GPU","text":"<pre><code># Watch GPU usage\nwatch -n 1 nvidia-smi\n\n# Or from local machine\nssh runpod-agentic-spliceai-a40-48gb \"nvidia-smi\"\n</code></pre>"},{"location":"RUNPODS_SETUP/#check-costs","title":"Check Costs","text":"<ul> <li>RunPods dashboard shows $/hour</li> <li>Set up billing alerts</li> <li>Terminate when not training (pay only for compute time)</li> </ul>"},{"location":"RUNPODS_SETUP/#privacy-security","title":"\ud83d\udd12 Privacy &amp; Security","text":""},{"location":"RUNPODS_SETUP/#why-runpods-is-not-in-git","title":"Why runpods/ is NOT in Git","text":"<p>The <code>runpods/</code> directory contains: - \u274c User-specific paths (<code>~/work/...</code>) - \u274c SSH configuration history - \u274c Personal workflow customizations - \u274c Potentially sensitive information</p>"},{"location":"RUNPODS_SETUP/#what-is-shared","title":"What IS Shared","text":"<ul> <li>\u2705 <code>runpods.example/</code> - Templates you can copy</li> <li>\u2705 <code>docs/RUNPODS_SETUP.md</code> - This guide</li> <li>\u2705 <code>environment-runpods-minimal.yml</code> - Public conda env template</li> </ul>"},{"location":"RUNPODS_SETUP/#faq","title":"\u2753 FAQ","text":""},{"location":"RUNPODS_SETUP/#q-where-do-the-scripts-run","title":"Q: Where do the scripts run?","text":"<p>A: Scripts in <code>runpods/scripts/</code> run LOCALLY (your machine). They configure SSH access TO pods. They do NOT run on the pod itself.</p>"},{"location":"RUNPODS_SETUP/#q-what-about-workscriptsrunpod_managersh","title":"Q: What about ~/work/scripts/runpod_manager.sh?","text":"<p>A: That's from an old design. Ignore those references in documentation. All scripts are now self-contained in <code>runpods/scripts/</code>.</p>"},{"location":"RUNPODS_SETUP/#q-can-i-share-my-runpods-directory","title":"Q: Can I share my runpods/ directory?","text":"<p>A: NO - it contains personal configuration. Share the <code>runpods.example/</code> directory instead, which is tracked in git.</p>"},{"location":"RUNPODS_SETUP/#q-what-if-my-workspace-is-not-work","title":"Q: What if my workspace is not ~/work/?","text":"<p>A: The scripts work from any location. Just <code>cd</code> to your project and use relative paths.</p>"},{"location":"RUNPODS_SETUP/#updating-your-setup","title":"\ud83d\udd04 Updating Your Setup","text":"<p>If we update the RunPods scripts:</p> <pre><code>cd ~/work/agentic-spliceai\n\n# Get latest from git\ngit pull\n\n# Update your runpods/ from example\ncp -r runpods.example/scripts/* runpods/scripts/\ncp runpods.example/environment-runpods-minimal.yml runpods/\n</code></pre>"},{"location":"RUNPODS_SETUP/#additional-resources","title":"\ud83d\udcda Additional Resources","text":"<ul> <li>Complete Workflow: <code>runpods.example/AGENTIC_SPLICEAI_QUICK_START.md</code></li> <li>Customization: <code>runpods.example/CUSTOMIZATION_NOTES.md</code></li> <li>RunPods Docs: docs.runpods.io</li> </ul> <p>Created: January 28, 2026 Status: Production-ready for agentic-spliceai</p>"},{"location":"SETUP/","title":"Agentic-SpliceAI Setup Guide","text":"<p>Quick setup instructions for development and deployment</p>"},{"location":"SETUP/#overview","title":"Overview","text":"<p>Agentic-SpliceAI builds upon the Meta-SpliceAI framework, combining:</p> <ol> <li>Extensible Base Layer - Foundation models (SpliceAI, OpenSpliceAI, + extensible)</li> <li>Adaptive Meta-Learning - Foundation-Adaptor framework via multimodal deep learning</li> <li>Agentic Workflows - AI agents for validation and evidence synthesis</li> </ol>"},{"location":"SETUP/#environment-setup","title":"Environment Setup","text":""},{"location":"SETUP/#1-create-dedicated-conda-environment","title":"1. Create Dedicated Conda Environment","text":"<pre><code>cd /Users/pleiadian53/work/agentic-spliceai\n\n# Create environment from yml\nmamba env create -f environment.yml\n\n# Activate environment\nmamba activate agentic-spliceai\n</code></pre>"},{"location":"SETUP/#2-install-package-in-development-mode","title":"2. Install Package in Development Mode","text":"<pre><code># Install agentic-spliceai package\npip install -e .\n\n# Verify installation\npython -c \"import agentic_spliceai; print('\u2713 Package installed')\"\n</code></pre>"},{"location":"SETUP/#3-test-cli-commands","title":"3. Test CLI Commands","text":"<pre><code># Test CLI entry points\nagentic-spliceai --help\nagentic-spliceai-server --help\n</code></pre>"},{"location":"SETUP/#architecture","title":"Architecture","text":""},{"location":"SETUP/#three-layer-system","title":"Three-Layer System","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    AGENTIC-SPLICEAI                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 Nexus Research   \u2502  \u2502 Meta-SpliceAI    \u2502  \u2502 Agentic  \u2502 \u2502\n\u2502  \u2502 Agent            \u2502  \u2502 Prediction       \u2502  \u2502 Workflow \u2502 \u2502\n\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524 \u2502\n\u2502  \u2502 \u2022 Literature     \u2502  \u2502 \u2022 Base SpliceAI  \u2502  \u2502 \u2022 LLM    \u2502 \u2502\n\u2502  \u2502   synthesis      \u2502  \u2502 \u2022 Meta-learning  \u2502  \u2502   agents \u2502 \u2502\n\u2502  \u2502 \u2022 arXiv/PubMed   \u2502  \u2502 \u2022 Ensemble       \u2502  \u2502 \u2022 Tool   \u2502 \u2502\n\u2502  \u2502 \u2022 LaTeX reports  \u2502  \u2502   models         \u2502  \u2502   use    \u2502 \u2502\n\u2502  \u2502 \u2022 Web search     \u2502  \u2502 \u2022 Feature eng.   \u2502  \u2502 \u2022 Multi- \u2502 \u2502\n\u2502  \u2502                  \u2502  \u2502                  \u2502  \u2502   agent  \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2502                                                             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"SETUP/#dependency-breakdown","title":"Dependency Breakdown","text":"<p>From Nexus (Research): - LLM clients: <code>openai</code>, <code>anthropic</code>, <code>mistralai</code>, <code>aisuite</code> - Research tools: <code>tavily-python</code>, <code>wikipedia</code>, <code>beautifulsoup4</code> - PDF generation: <code>weasyprint</code>, <code>pandoc</code>, <code>tectonic</code>, <code>pypandoc</code> - Web framework: <code>fastapi</code>, <code>uvicorn</code>, <code>pydantic</code></p> <p>From Meta-SpliceAI (Prediction): - Deep learning: <code>tensorflow</code>, <code>pytorch</code>, <code>keras</code> - SpliceAI: <code>spliceai==1.3.1</code> - Genomics: <code>biopython</code>, <code>pysam</code>, <code>pybedtools</code>, <code>pybigwig</code> - Genomic formats: <code>bcbio-gff</code>, <code>gffutils</code>, <code>gtftools</code>, <code>pyfastx</code> - ML optimization: <code>optuna</code>, <code>hyperopt</code>, <code>hpbandster</code> - Feature engineering: <code>category-encoders</code>, <code>feature-engine</code></p> <p>Shared (Both): - Data: <code>pandas</code>, <code>polars</code>, <code>numpy</code>, <code>duckdb</code> - Visualization: <code>matplotlib</code>, <code>seaborn</code>, <code>plotly</code> - ML: <code>scikit-learn</code>, <code>lightgbm</code>, <code>xgboost</code> - Utilities: <code>tqdm</code>, <code>joblib</code>, <code>requests</code></p>"},{"location":"SETUP/#development-workflow","title":"Development Workflow","text":""},{"location":"SETUP/#phase-1-port-nexus-research-capability","title":"Phase 1: Port Nexus Research Capability","text":"<pre><code># Copy Nexus research agent code\ncp -r /Users/pleiadian53/work/agentic-ai-lab/src/nexus/agents/research/* \\\n      /Users/pleiadian53/work/agentic-spliceai/agentic_spliceai/research/\n\n# Test research capability\npython -c \"from agentic_spliceai.research import pipeline; print('\u2713 Research imported')\"\n</code></pre>"},{"location":"SETUP/#phase-2-integrate-meta-spliceai-prediction","title":"Phase 2: Integrate Meta-SpliceAI Prediction","text":"<pre><code># Copy meta-spliceai core modules\ncp -r /Users/pleiadian53/work/meta-spliceai/meta_spliceai/splice_engine/* \\\n      /Users/pleiadian53/work/agentic-spliceai/agentic_spliceai/prediction/\n\n# Test prediction capability\npython -c \"from agentic_spliceai.prediction import base_model; print('\u2713 Prediction imported')\"\n</code></pre>"},{"location":"SETUP/#phase-3-build-agentic-workflow","title":"Phase 3: Build Agentic Workflow","text":"<pre><code># Combine research + prediction with LLM agents\n# Create workflow that:\n# 1. Uses Nexus to research latest splice site methods\n# 2. Uses Meta-SpliceAI to make predictions\n# 3. Uses LLM to interpret results and generate insights\n</code></pre>"},{"location":"SETUP/#environment-comparison","title":"Environment Comparison","text":"Feature agentic-ai agentic-spliceai Purpose General research Splice site research Nexus \u2713 Full \u2713 Ported Deep Learning \u2717 \u2713 TensorFlow/PyTorch Genomics \u2717 \u2713 Full bioinformatics SpliceAI \u2717 \u2713 Base + Meta Size ~2GB ~5GB (with models)"},{"location":"SETUP/#next-steps","title":"Next Steps","text":"<ol> <li>\u2705 Create environment: <code>mamba env create -f environment.yml</code></li> <li>\u2705 Install package: <code>pip install -e .</code></li> <li>\u23f3 Port Nexus research agent</li> <li>\u23f3 Integrate Meta-SpliceAI prediction</li> <li>\u23f3 Build unified agentic workflow</li> <li>\u23f3 Test end-to-end pipeline</li> <li>\u23f3 Create example notebooks</li> <li>\u23f3 Write documentation</li> <li>\u23f3 First commit to GitHub</li> </ol>"},{"location":"SETUP/#troubleshooting","title":"Troubleshooting","text":""},{"location":"SETUP/#tensorflowpytorch-conflicts","title":"TensorFlow/PyTorch Conflicts","text":"<p>If you encounter GPU/CUDA issues: <pre><code># CPU-only versions (lighter)\npip install tensorflow-cpu\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n</code></pre></p>"},{"location":"SETUP/#genomics-tools","title":"Genomics Tools","text":"<p>Some genomics tools require system dependencies: <pre><code># macOS\nbrew install htslib bedtools\n\n# Ubuntu/Debian\nsudo apt-get install libhts-dev bedtools\n</code></pre></p>"},{"location":"SETUP/#memory-issues","title":"Memory Issues","text":"<p>Large models may require significant RAM: - Minimum: 16GB RAM - Recommended: 32GB RAM - With GPU: 64GB+ RAM for large-scale training</p>"},{"location":"SETUP/#verify-installation","title":"Verify Installation","text":"<p>Run the setup verification script:</p> <pre><code>python scripts/setup/verify_setup.py\n</code></pre> <p>This checks: - Required packages installed - Import paths working - Data directories accessible - Environment variables set</p>"},{"location":"SETUP/#resources","title":"Resources","text":"<ul> <li>Documentation: </li> <li><code>README.md</code> - Project overview and vision</li> <li><code>QUICKSTART.md</code> - Quick start guide</li> <li><code>docs/STRUCTURE.md</code> - Complete project structure</li> <li><code>docs/</code> - Comprehensive documentation</li> <li>Meta-SpliceAI: https://github.com/pleiadian53/meta-spliceai</li> <li>Agentic AI Lab: https://github.com/pleiadian53/agentic-ai-lab</li> </ul>"},{"location":"SPLICE_PREDICTION_GUIDE/","title":"Splice Site Prediction Guide","text":"<p>Agentic-SpliceAI Base Layer Integration</p> <p>This guide explains how to use the splice site prediction capabilities in agentic-spliceai, which integrates the base layer from the meta-spliceai project.</p>"},{"location":"SPLICE_PREDICTION_GUIDE/#overview","title":"Overview","text":"<p>The agentic-spliceai system now includes true splice site prediction capabilities through integration with meta-spliceai's base layer. This provides:</p> <ul> <li>Base Model Support: SpliceAI and OpenSpliceAI models</li> <li>Flexible Interface: CLI and Python API</li> <li>Production-Ready: Memory-efficient processing with checkpointing</li> <li>Model-Agnostic: Easy to extend with custom models</li> </ul>"},{"location":"SPLICE_PREDICTION_GUIDE/#quick-start","title":"Quick Start","text":""},{"location":"SPLICE_PREDICTION_GUIDE/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Install meta-spliceai (required dependency):    <pre><code># From the meta-spliceai project directory\ncd /Users/pleiadian53/work/meta-spliceai\nmamba activate agentic-spliceai\npip install -e .\n</code></pre></p> </li> <li> <p>Verify installation:    <pre><code>python -c \"import meta_spliceai; print('meta-spliceai installed')\"\n</code></pre></p> </li> </ol>"},{"location":"SPLICE_PREDICTION_GUIDE/#cli-usage","title":"CLI Usage","text":"<pre><code># Activate environment\nmamba activate agentic-spliceai\n\n# Predict for specific genes\nagentic-spliceai-predict --genes BRCA1 TP53 UNC13A\n\n# Predict for a chromosome\nagentic-spliceai-predict --chromosomes 21\n\n# Use SpliceAI (GRCh37) instead of OpenSpliceAI (GRCh38)\nagentic-spliceai-predict --base-model spliceai --genes BRCA1\n\n# Test mode with sample data\nagentic-spliceai-predict --mode test --coverage sample --genes BRCA1\n</code></pre>"},{"location":"SPLICE_PREDICTION_GUIDE/#python-api-usage","title":"Python API Usage","text":"<pre><code>from agentic_spliceai.splice_engine import predict_splice_sites\n\n# Simple prediction\nresults = predict_splice_sites(genes=[\"BRCA1\", \"TP53\"])\n\n# Access results\npositions = results[\"positions\"]  # Polars DataFrame\nprint(f\"Found {positions.height} splice positions\")\n\n# Get high-confidence predictions\nhigh_conf = positions.filter(\n    (pl.col(\"donor_score\") &gt; 0.9) | (pl.col(\"acceptor_score\") &gt; 0.9)\n)\n</code></pre>"},{"location":"SPLICE_PREDICTION_GUIDE/#entry-points","title":"Entry Points","text":""},{"location":"SPLICE_PREDICTION_GUIDE/#option-1-cli-entry-point-simplest","title":"Option 1: CLI Entry Point (Simplest)","text":"<p>Command: <code>agentic-spliceai-predict</code></p> <p>Pros: - \u2705 Zero code needed - \u2705 Automatic data path resolution - \u2705 Production-ready - \u2705 Clear CLI interface</p> <p>Usage: <pre><code>agentic-spliceai-predict --base-model openspliceai --mode test --coverage sample\n</code></pre></p>"},{"location":"SPLICE_PREDICTION_GUIDE/#option-2-python-api-entry-point-most-flexible","title":"Option 2: Python API Entry Point (Most Flexible)","text":"<p>Module: <code>agentic_spliceai.splice_engine</code></p> <p>Function: <code>predict_splice_sites()</code> or <code>run_base_model_predictions()</code></p> <p>Pros: - \u2705 Full programmatic control - \u2705 Integration with custom workflows - \u2705 Access to in-memory results - \u2705 Gene-level filtering</p> <p>Usage: <pre><code>from agentic_spliceai.splice_engine import run_base_model_predictions\n\nresults = run_base_model_predictions(\n    base_model='openspliceai',\n    target_genes=['UNC13A', 'STMN2'],\n    verbosity=1\n)\n</code></pre></p>"},{"location":"SPLICE_PREDICTION_GUIDE/#option-3-high-level-api-object-oriented","title":"Option 3: High-Level API (Object-Oriented)","text":"<p>Module: <code>agentic_spliceai.splice_engine.api</code></p> <p>Class: <code>SplicePredictionAPI</code></p> <p>Pros: - \u2705 Clean object-oriented interface - \u2705 Built-in filtering and export methods - \u2705 Convenient for interactive use</p> <p>Usage: <pre><code>from agentic_spliceai.splice_engine.api import SplicePredictionAPI\n\n# Initialize API\napi = SplicePredictionAPI(base_model=\"openspliceai\")\n\n# Predict for genes\nresults = api.predict_genes([\"BRCA1\", \"TP53\"])\n\n# Get high-confidence predictions\nhigh_conf = api.get_high_confidence_predictions(results, threshold=0.9)\n\n# Export to file\napi.export_predictions(results, \"predictions.csv\", format=\"csv\")\n</code></pre></p>"},{"location":"SPLICE_PREDICTION_GUIDE/#examples","title":"Examples","text":""},{"location":"SPLICE_PREDICTION_GUIDE/#example-1-basic-gene-prediction","title":"Example 1: Basic Gene Prediction","text":"<pre><code>from agentic_spliceai.splice_engine import predict_splice_sites\n\n# Predict splice sites for ALS-related genes\nresults = predict_splice_sites(\n    genes=[\"UNC13A\", \"STMN2\", \"TARDBP\"],\n    base_model=\"openspliceai\"\n)\n\n# Access predictions\npositions = results[\"positions\"]\nprint(f\"Total positions: {positions.height}\")\n\n# Filter to high-confidence donors\nimport polars as pl\ndonors = positions.filter(\n    (pl.col(\"splice_type\") == \"donor\") &amp; \n    (pl.col(\"donor_score\") &gt; 0.9)\n)\nprint(f\"High-confidence donors: {donors.height}\")\n</code></pre>"},{"location":"SPLICE_PREDICTION_GUIDE/#example-2-chromosome-wide-analysis","title":"Example 2: Chromosome-Wide Analysis","text":"<pre><code>from agentic_spliceai.splice_engine.api import SplicePredictionAPI\n\n# Initialize API\napi = SplicePredictionAPI(base_model=\"openspliceai\", verbosity=1)\n\n# Predict for chromosome 21\nresults = api.predict_chromosomes([\"21\"])\n\n# Get error positions (false positives and false negatives)\nerrors = api.get_error_positions(results)\nprint(f\"Total errors: {errors.height}\")\n\n# Export results\napi.export_predictions(results, \"chr21_predictions.tsv\", format=\"tsv\")\n</code></pre>"},{"location":"SPLICE_PREDICTION_GUIDE/#example-3-quick-interactive-analysis","title":"Example 3: Quick Interactive Analysis","text":"<pre><code>from agentic_spliceai.splice_engine.api import quick_predict, predict_and_filter\n\n# Quick prediction\nresults = quick_predict(genes=[\"BRCA1\"])\n\n# Or predict and filter in one step\nhigh_conf = predict_and_filter(\n    genes=[\"BRCA1\", \"TP53\"],\n    confidence_threshold=0.95\n)\n\nprint(high_conf.head())\n</code></pre>"},{"location":"SPLICE_PREDICTION_GUIDE/#example-4-custom-configuration","title":"Example 4: Custom Configuration","text":"<pre><code>from agentic_spliceai.splice_engine import run_base_model_predictions\n\n# Advanced configuration\nresults = run_base_model_predictions(\n    base_model=\"openspliceai\",\n    target_genes=[\"UNC13A\"],\n    mode=\"test\",\n    coverage=\"gene_subset\",\n    threshold=0.5,\n    verbosity=2,\n    no_tn_sampling=False,  # Sample true negatives to reduce memory\n    save_nucleotide_scores=False  # Don't save per-nucleotide scores\n)\n\n# Access different result components\npositions = results[\"positions\"]  # All positions\nerrors = results[\"error_analysis\"]  # TP/FP/FN classifications\nsequences = results[\"analysis_sequences\"]  # Sequence windows\nmanifest = results[\"gene_manifest\"]  # Processing metadata\n</code></pre>"},{"location":"SPLICE_PREDICTION_GUIDE/#cli-reference","title":"CLI Reference","text":""},{"location":"SPLICE_PREDICTION_GUIDE/#basic-options","title":"Basic Options","text":"<pre><code>agentic-spliceai-predict [OPTIONS]\n</code></pre> <p>Target Selection (mutually exclusive): - <code>--genes GENE1 GENE2 ...</code> - Gene symbols or IDs - <code>--chromosomes CHR1 CHR2 ...</code> - Chromosomes to process</p> <p>Model Selection: - <code>--base-model {openspliceai,spliceai}</code> - Base model (default: openspliceai)</p> <p>Mode and Coverage: - <code>--mode {test,production}</code> - Execution mode (default: test) - <code>--coverage {gene_subset,chromosome,full_genome,sample}</code> - Coverage level</p> <p>Output Control: - <code>--output-dir DIR</code> - Output directory - <code>--verbosity {0,1,2}</code> - Output verbosity (default: 1) - <code>--format {summary,json,paths}</code> - Output format</p> <p>Advanced Options: - <code>--threshold FLOAT</code> - Score threshold (default: 0.5) - <code>--no-tn-sampling</code> - Keep all true negatives (memory intensive) - <code>--save-nucleotide-scores</code> - Save per-nucleotide scores (large files)</p>"},{"location":"SPLICE_PREDICTION_GUIDE/#examples_1","title":"Examples","text":"<pre><code># Basic gene prediction\nagentic-spliceai-predict --genes BRCA1 TP53\n\n# Chromosome analysis with detailed output\nagentic-spliceai-predict --chromosomes 21 --verbosity 2\n\n# Production run with custom threshold\nagentic-spliceai-predict \\\n  --mode production \\\n  --coverage full_genome \\\n  --threshold 0.5 \\\n  --output-dir /path/to/output\n\n# Test mode with JSON output\nagentic-spliceai-predict \\\n  --genes UNC13A STMN2 \\\n  --mode test \\\n  --format json &gt; results.json\n</code></pre>"},{"location":"SPLICE_PREDICTION_GUIDE/#python-api-reference","title":"Python API Reference","text":""},{"location":"SPLICE_PREDICTION_GUIDE/#core-functions","title":"Core Functions","text":""},{"location":"SPLICE_PREDICTION_GUIDE/#predict_splice_sites","title":"<code>predict_splice_sites()</code>","text":"<pre><code>def predict_splice_sites(\n    genes: Optional[List[str]] = None,\n    chromosomes: Optional[List[str]] = None,\n    base_model: str = \"openspliceai\",\n    **kwargs\n) -&gt; Dict[str, Any]\n</code></pre> <p>Convenience wrapper for splice site prediction.</p> <p>Parameters: - <code>genes</code>: Gene symbols or IDs - <code>chromosomes</code>: Chromosomes to process - <code>base_model</code>: \"openspliceai\" or \"spliceai\" - <code>**kwargs</code>: Additional configuration</p> <p>Returns: Results dictionary</p>"},{"location":"SPLICE_PREDICTION_GUIDE/#run_base_model_predictions","title":"<code>run_base_model_predictions()</code>","text":"<pre><code>def run_base_model_predictions(\n    base_model: str = \"openspliceai\",\n    target_genes: Optional[List[str]] = None,\n    target_chromosomes: Optional[List[str]] = None,\n    verbosity: int = 1,\n    **kwargs\n) -&gt; Dict[str, Any]\n</code></pre> <p>Full-featured prediction function with complete configuration control.</p>"},{"location":"SPLICE_PREDICTION_GUIDE/#splicepredictionapi-class","title":"SplicePredictionAPI Class","text":"<pre><code>class SplicePredictionAPI:\n    def __init__(self, base_model: str = \"openspliceai\", verbosity: int = 1, **config_kwargs)\n    def predict_genes(self, genes: List[str], **kwargs) -&gt; Dict[str, Any]\n    def predict_chromosomes(self, chromosomes: List[str], **kwargs) -&gt; Dict[str, Any]\n\n    @staticmethod\n    def get_high_confidence_predictions(results: Dict, threshold: float = 0.9) -&gt; DataFrame\n\n    @staticmethod\n    def get_error_positions(results: Dict, error_type: Optional[str] = None) -&gt; DataFrame\n\n    @staticmethod\n    def export_predictions(results: Dict, output_path: str, format: str = \"csv\")\n</code></pre>"},{"location":"SPLICE_PREDICTION_GUIDE/#data-requirements","title":"Data Requirements","text":""},{"location":"SPLICE_PREDICTION_GUIDE/#genomic-resources","title":"Genomic Resources","text":"<p>The system requires:</p> <ol> <li>Reference Genome (FASTA)</li> <li>Gene Annotations (GTF/GFF3)</li> <li>Base Model Weights</li> </ol> <p>These are automatically managed by meta-spliceai's genomic resources system.</p>"},{"location":"SPLICE_PREDICTION_GUIDE/#directory-structure","title":"Directory Structure","text":"<pre><code>data/\n\u251c\u2500\u2500 mane/GRCh38/                 # For OpenSpliceAI\n\u2502   \u251c\u2500\u2500 MANE.GRCh38.v1.3.refseq_genomic.gff\n\u2502   \u251c\u2500\u2500 GCF_000001405.40_GRCh38.p14_genomic.fna\n\u2502   \u2514\u2500\u2500 openspliceai_eval/       # Output directory\n\u2502\n\u2514\u2500\u2500 ensembl/GRCh37/              # For SpliceAI\n    \u251c\u2500\u2500 Homo_sapiens.GRCh37.87.gtf\n    \u251c\u2500\u2500 Homo_sapiens.GRCh37.dna.primary_assembly.fa\n    \u2514\u2500\u2500 spliceai_eval/           # Output directory\n</code></pre>"},{"location":"SPLICE_PREDICTION_GUIDE/#results-structure","title":"Results Structure","text":"<p>The prediction functions return a dictionary with:</p> <pre><code>{\n    'success': bool,                    # Whether workflow completed\n    'positions': pl.DataFrame,          # All analyzed positions\n    'error_analysis': pl.DataFrame,     # TP/FP/FN classifications\n    'analysis_sequences': pl.DataFrame, # Sequence windows (\u00b1250nt)\n    'gene_manifest': pl.DataFrame,      # Processing metadata\n    'nucleotide_scores': pl.DataFrame,  # Per-nucleotide scores (if enabled)\n    'paths': {                          # Output file paths\n        'eval_dir': str,\n        'artifacts_dir': str,\n        'positions_artifact': str,\n        'errors_artifact': str\n    },\n    'manifest_summary': {               # Processing statistics\n        'processed_genes': int,\n        'total_positions': int\n    }\n}\n</code></pre>"},{"location":"SPLICE_PREDICTION_GUIDE/#key-dataframes","title":"Key DataFrames","text":"<p>positions: All splice site predictions - Columns: <code>gene_id</code>, <code>position</code>, <code>splice_type</code>, <code>donor_score</code>, <code>acceptor_score</code>, <code>error_type</code>, etc.</p> <p>error_analysis: Error classifications - Columns: <code>error_type</code> (TP/FP/FN/TN), <code>gene_id</code>, <code>position</code>, <code>splice_type</code>, <code>strand</code></p> <p>analysis_sequences: Sequence windows around each position - Columns: <code>gene_id</code>, <code>position</code>, <code>sequence</code>, <code>splice_type</code>, <code>error_type</code></p>"},{"location":"SPLICE_PREDICTION_GUIDE/#troubleshooting","title":"Troubleshooting","text":""},{"location":"SPLICE_PREDICTION_GUIDE/#issue-importerror-for-meta_spliceai","title":"Issue: ImportError for meta_spliceai","text":"<p>Error: <code>ImportError: meta-spliceai is required for splice prediction</code></p> <p>Solution: Install meta-spliceai: <pre><code>cd /Users/pleiadian53/work/meta-spliceai\nmamba activate agentic-spliceai\npip install -e .\n</code></pre></p>"},{"location":"SPLICE_PREDICTION_GUIDE/#issue-out-of-memory","title":"Issue: Out of Memory","text":"<p>Solution 1: Reduce batch size (via meta-spliceai config) Solution 2: Process fewer genes/chromosomes at once Solution 3: Disable nucleotide score collection (default)</p>"},{"location":"SPLICE_PREDICTION_GUIDE/#issue-wrong-environment","title":"Issue: Wrong Environment","text":"<p>Error: Module not found or import errors</p> <p>Solution: Always activate the correct environment: <pre><code>mamba activate agentic-spliceai\n# OR use mamba run\nmamba run -n agentic-spliceai agentic-spliceai-predict --genes BRCA1\n</code></pre></p>"},{"location":"SPLICE_PREDICTION_GUIDE/#integration-with-nexus-research-agent","title":"Integration with Nexus Research Agent","text":"<p>The splice prediction capabilities can be used by the Nexus Research Agent for:</p> <ol> <li>Literature-Guided Analysis: Research splice mechanisms, then predict sites</li> <li>Validation: Validate predictions against published findings</li> <li>Discovery: Identify novel splice patterns for further investigation</li> </ol> <p>Example workflow: <pre><code># 1. Research splice mechanisms\nfrom nexus.agents.research import ResearchAgent\nagent = ResearchAgent()\nreport = agent.research(\"UNC13A cryptic exon in ALS\")\n\n# 2. Predict splice sites\nfrom agentic_spliceai.splice_engine import predict_splice_sites\nresults = predict_splice_sites(genes=[\"UNC13A\"])\n\n# 3. Analyze predictions in context of research\nhigh_conf = results[\"positions\"].filter(pl.col(\"donor_score\") &gt; 0.9)\n# Compare with literature findings...\n</code></pre></p>"},{"location":"SPLICE_PREDICTION_GUIDE/#next-steps","title":"Next Steps","text":"<ol> <li>Explore Examples: Try the examples in this guide</li> <li>Read Meta-SpliceAI Docs: See <code>/Users/pleiadian53/work/meta-spliceai/docs/base_models/BASE_LAYER_INTEGRATION_GUIDE.md</code></li> <li>Customize: Extend with custom models or analysis workflows</li> <li>Integrate: Combine with Nexus Research Agent for literature-guided analysis</li> </ol>"},{"location":"SPLICE_PREDICTION_GUIDE/#additional-resources","title":"Additional Resources","text":"<ul> <li>Meta-SpliceAI Project: <code>/Users/pleiadian53/work/meta-spliceai</code></li> <li>Base Layer Guide: <code>meta-spliceai/docs/base_models/BASE_LAYER_INTEGRATION_GUIDE.md</code></li> <li>Agentic-SpliceAI README: <code>README.md</code></li> <li>Nexus Documentation: <code>src/nexus/README.md</code></li> </ul> <p>Questions? Open an issue or consult the meta-spliceai documentation.</p>"},{"location":"STRUCTURE/","title":"Agentic-SpliceAI - Project Structure","text":"<p>Complete overview of the agentic-spliceai project organization</p> <p>Note: This file is located at <code>docs/STRUCTURE.md</code>. For quick setup, see <code>SETUP.md</code> at the project root.</p>"},{"location":"STRUCTURE/#directory-structure","title":"\ud83d\udcc1 Directory Structure","text":"<pre><code>agentic-spliceai/\n\u251c\u2500\u2500 README.md                    # Project overview &amp; vision\n\u251c\u2500\u2500 SETUP.md                     # Setup instructions (root for quick access)\n\u251c\u2500\u2500 QUICKSTART.md                # 5-minute getting started\n\u251c\u2500\u2500 LICENSE                      # MIT License\n\u2502\n\u251c\u2500\u2500 environment.yml              # Mamba environment definition\n\u251c\u2500\u2500 pyproject.toml               # Poetry project configuration\n\u251c\u2500\u2500 requirements.txt             # Pip requirements (legacy)\n\u251c\u2500\u2500 .env.example                 # Environment variable template\n\u251c\u2500\u2500 .gitignore                   # Git ignore rules\n\u2502\n\u251c\u2500\u2500 agentic_spliceai/                # Main package\n\u2502   \u251c\u2500\u2500 __init__.py              # Package initialization\n\u2502   \u251c\u2500\u2500 data_access.py           # Dataset loading and querying\n\u2502   \u251c\u2500\u2500 planning.py              # Chart code generation\n\u2502   \u251c\u2500\u2500 llm_client.py            # LLM API client\n\u2502   \u251c\u2500\u2500 utils.py                 # Utility functions\n\u2502   \u251c\u2500\u2500 splice_analysis.py       # Splice-specific analysis\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 server/                  # FastAPI service\n\u2502   \u2502   \u251c\u2500\u2500 splice_service.py    # Main API service\n\u2502   \u2502   \u251c\u2500\u2500 config.py            # Configuration\n\u2502   \u2502   \u251c\u2500\u2500 schemas.py           # Pydantic models\n\u2502   \u2502   \u2514\u2500\u2500 manage.py            # Service management\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 docs/                    # Package-level docs\n\u2502       \u2514\u2500\u2500 README.md            # Package documentation\n\u2502\n\u251c\u2500\u2500 docs/                        # Global documentation\n\u2502   \u251c\u2500\u2500 README.md                # Documentation index\n\u2502   \u251c\u2500\u2500 STRUCTURE.md             # This file - Project structure overview\n\u2502   \u251c\u2500\u2500 base_layer/              # Base layer documentation\n\u2502   \u251c\u2500\u2500 isoform_discovery/       # Isoform discovery vision &amp; roadmap\n\u2502   \u251c\u2500\u2500 installation/            # Setup guides\n\u2502   \u251c\u2500\u2500 PACKAGE_ORGANIZATION.md  # Experimental package guidelines\n\u2502   \u2514\u2500\u2500 SPLICE_PREDICTION_GUIDE.md  # Splice prediction guide\n\u2502\n\u251c\u2500\u2500 examples/                    # Driver scripts for development\n\u2502   \u251c\u2500\u2500 _example_utils.py        # Path resolution utilities\n\u2502   \u251c\u2500\u2500 README.md                # Examples overview\n\u2502   \u251c\u2500\u2500 base_layer/              # Base layer prediction examples\n\u2502   \u251c\u2500\u2500 data_preparation/        # Data prep workflow examples\n\u2502   \u251c\u2500\u2500 meta_layer/              # Meta layer examples (Phase 5)\n\u2502   \u2514\u2500\u2500 variant_analysis/        # Variant analysis examples (Phase 6)\n\u2502\n\u251c\u2500\u2500 scripts/                     # Utility scripts &amp; tools\n\u2502   \u251c\u2500\u2500 setup/                   # Setup &amp; verification\n\u2502   \u2502   \u2514\u2500\u2500 verify_setup.py      # Verify installation\n\u2502   \u251c\u2500\u2500 validation/              # Validation &amp; testing\n\u2502   \u2502   \u2514\u2500\u2500 compare_evaluation.py  # Comparison tools\n\u2502   \u251c\u2500\u2500 docs/                    # Scripts documentation\n\u2502   \u2514\u2500\u2500 README.md                # Scripts overview\n\u2502\n\u251c\u2500\u2500 tests/                       # Unit tests\n\u2502   \u251c\u2500\u2500 test_data_access.py\n\u2502   \u251c\u2500\u2500 test_planning.py\n\u2502   \u251c\u2500\u2500 test_splice_analysis.py\n\u2502   \u2514\u2500\u2500 conftest.py              # Pytest configuration\n\u2502\n\u251c\u2500\u2500 data/                        # Data directory\n\u2502   \u251c\u2500\u2500 README.md                # Data documentation\n\u2502   \u2514\u2500\u2500 .gitkeep                 # Keep directory in git\n\u2502\n\u2514\u2500\u2500 output/                      # Generated outputs\n    \u2514\u2500\u2500 splice_charts/           # Generated charts\n</code></pre>"},{"location":"STRUCTURE/#documentation-hierarchy","title":"\ud83d\udcda Documentation Hierarchy","text":""},{"location":"STRUCTURE/#1-global-documentation-docs","title":"1. Global Documentation (<code>docs/</code>)","text":"<p>Purpose: High-level, topic-based documentation for users and contributors.</p> <p>Structure: <pre><code>docs/\n\u251c\u2500\u2500 README.md                    # Documentation index\n\u251c\u2500\u2500 architecture/                # System design\n\u2502   \u251c\u2500\u2500 overview.md\n\u2502   \u251c\u2500\u2500 components.md\n\u2502   \u2514\u2500\u2500 data_flow.md\n\u251c\u2500\u2500 tutorials/                   # Learning guides\n\u2502   \u251c\u2500\u2500 getting_started.md\n\u2502   \u2514\u2500\u2500 advanced_usage.md\n\u251c\u2500\u2500 installation/                # Setup guides\n\u2502   \u251c\u2500\u2500 environment.md\n\u2502   \u2514\u2500\u2500 troubleshooting.md\n\u251c\u2500\u2500 api/                         # API reference\n\u2502   \u251c\u2500\u2500 rest_api.md\n\u2502   \u2514\u2500\u2500 python_api.md\n\u251c\u2500\u2500 biology/                     # Domain knowledge\n\u2502   \u251c\u2500\u2500 splice_sites.md\n\u2502   \u2514\u2500\u2500 alternative_splicing.md\n\u2514\u2500\u2500 workflows/                   # Analysis patterns\n    \u251c\u2500\u2500 predefined.md\n    \u2514\u2500\u2500 custom.md\n</code></pre></p>"},{"location":"STRUCTURE/#2-package-documentation-agentic_spliceaidocs","title":"2. Package Documentation (<code>agentic_spliceai/docs/</code>)","text":"<p>Purpose: Code-specific documentation close to implementation.</p> <p>Contents: - Module API references - Implementation details - Code examples - Internal architecture</p>"},{"location":"STRUCTURE/#configuration-files","title":"\ud83d\udd27 Configuration Files","text":""},{"location":"STRUCTURE/#environmentyml","title":"<code>environment.yml</code>","text":"<p>Mamba/Conda environment definition with all dependencies.</p> <pre><code>name: agentic-spliceai\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.11\n  - openai\n  - fastapi\n  - duckdb\n  # ... more dependencies\n</code></pre>"},{"location":"STRUCTURE/#pyprojecttoml","title":"<code>pyproject.toml</code>","text":"<p>Poetry project configuration with metadata and dependencies.</p> <pre><code>[project]\nname = \"agentic-spliceai\"\nversion = \"0.1.0\"\ndependencies = [\n    \"openai\",\n    \"fastapi\",\n    # ... more dependencies\n]\n</code></pre>"},{"location":"STRUCTURE/#gitignore","title":"<code>.gitignore</code>","text":"<p>Excludes from version control: - <code>.env</code> - Environment variables - <code>output/</code> - Generated files - <code>data/*.tsv</code> - Data files - Python artifacts</p>"},{"location":"STRUCTURE/#module-responsibilities","title":"\ud83c\udfaf Module Responsibilities","text":""},{"location":"STRUCTURE/#core-modules","title":"Core Modules","text":"<p><code>data_access.py</code> - Dataset loading (TSV, CSV, Parquet, SQLite) - SQL query execution - Schema introspection - Data validation</p> <p><code>planning.py</code> - Chart code generation - Code critique and refinement - Reflection loops - Prompt engineering</p> <p><code>llm_client.py</code> - OpenAI API integration - Retry logic - Error handling - Response parsing</p> <p><code>utils.py</code> - Model recommendations - File operations - Logging utilities - Helper functions</p> <p><code>splice_analysis.py</code> - Domain context - Analysis templates - Template-based generation - Exploratory analysis</p>"},{"location":"STRUCTURE/#server-modules","title":"Server Modules","text":"<p><code>server/splice_service.py</code> - FastAPI application - REST endpoints - Request handling - Response formatting</p> <p><code>server/config.py</code> - Path resolution - Configuration management - Environment variables - Constants</p> <p><code>server/schemas.py</code> - Pydantic models - Request validation - Response schemas - Type definitions</p>"},{"location":"STRUCTURE/#development-workflow","title":"\ud83d\ude80 Development Workflow","text":""},{"location":"STRUCTURE/#1-environment-setup","title":"1. Environment Setup","text":"<pre><code># Create environment\nmamba env create -f environment.yml\nmamba activate agentic-spliceai\n\n# Or use poetry\npoetry install\n</code></pre>"},{"location":"STRUCTURE/#2-development","title":"2. Development","text":"<pre><code># Run tests\npytest\n\n# Format code\nblack .\nruff check .\n\n# Type checking\nmypy agentic_spliceai/\n</code></pre>"},{"location":"STRUCTURE/#3-running-server","title":"3. Running Server","text":"<pre><code>cd agentic_spliceai/server\npython splice_service.py\n# Visit http://localhost:8004/docs\n</code></pre>"},{"location":"STRUCTURE/#4-documentation","title":"4. Documentation","text":"<ul> <li>Add global docs \u2192 <code>docs/&lt;topic&gt;/</code></li> <li>Add package docs \u2192 <code>agentic_spliceai/docs/</code></li> </ul>"},{"location":"STRUCTURE/#package-distribution","title":"\ud83d\udce6 Package Distribution","text":""},{"location":"STRUCTURE/#local-installation","title":"Local Installation","text":"<pre><code># Development mode\npip install -e .\n\n# With optional dependencies\npip install -e \".[dev,bio]\"\n</code></pre>"},{"location":"STRUCTURE/#building","title":"Building","text":"<pre><code># Build distribution\npython -m build\n\n# Install from wheel\npip install dist/agentic_spliceai-0.1.0-py3-none-any.whl\n</code></pre>"},{"location":"STRUCTURE/#integration-points","title":"\ud83d\udd17 Integration Points","text":""},{"location":"STRUCTURE/#with-agentic-ai-public","title":"With agentic-ai-public","text":"<ul> <li>Shares core patterns (reflection, planning)</li> <li>Can use same environment</li> <li>Independent git repositories</li> <li>Manual code syncing</li> </ul>"},{"location":"STRUCTURE/#with-external-tools","title":"With External Tools","text":"<ul> <li>OpenAI API for LLM</li> <li>DuckDB for data access</li> <li>FastAPI for web service</li> <li>Matplotlib/Seaborn for visualization</li> </ul>"},{"location":"STRUCTURE/#design-principles","title":"\ud83c\udfa8 Design Principles","text":"<ol> <li>Modularity - Clear separation of concerns</li> <li>Documentation - Three-tier documentation structure</li> <li>Testability - Comprehensive test coverage</li> <li>Extensibility - Easy to add new analyses</li> <li>Portability - Self-contained and relocatable</li> </ol>"},{"location":"STRUCTURE/#file-naming-conventions","title":"\ud83d\udcdd File Naming Conventions","text":"<ul> <li>Python files: <code>snake_case.py</code></li> <li>Documentation: <code>UPPERCASE.md</code> or <code>lowercase.md</code></li> <li>Tests: <code>test_*.py</code></li> <li>Examples: <code>descriptive_name.py</code></li> </ul>"},{"location":"STRUCTURE/#finding-things","title":"\ud83d\udd0d Finding Things","text":"<p>I want to...</p> <ul> <li>Understand the architecture \u2192 <code>docs/architecture/</code></li> <li>Learn how to use it \u2192 <code>docs/tutorials/</code></li> <li>Check API reference \u2192 <code>docs/api/</code> or <code>agentic_spliceai/docs/</code></li> <li>See examples \u2192 <code>examples/</code></li> <li>Run tests \u2192 <code>tests/</code></li> </ul> <p>Last Updated: 2025-11-19 Version: 0.1.0</p>"},{"location":"agency/","title":"Agentic AI Documentation","text":"<p>Autonomous agents for scientific discovery and validation</p>"},{"location":"agency/#documentation-index","title":"\ud83d\udcda Documentation Index","text":"Document Purpose Audience AGENTIC_MEMORY_TUTORIAL.md Tutorial on persistent memory for agents All users MEMORY_PATTERNS.md Practical patterns and examples Researchers AGENT_WORKFLOWS.md Complete workflow examples Advanced users API_REFERENCE.md API documentation Developers"},{"location":"agency/#what-are-agentic-workflows","title":"\ud83e\udd16 What Are Agentic Workflows?","text":"<p>Agentic AI = Autonomous agents that can: - \ud83d\udd0d Research: Search literature, databases, prior experiments - \u2705 Validate: Assess biological plausibility, gather evidence - \ud83e\udde0 Reason: Connect patterns, propose hypotheses, design experiments - \ud83d\udcca Report: Generate summaries with full provenance - \ud83e\uddec Remember: Build cumulative knowledge over time</p>"},{"location":"agency/#key-features-in-agentic-spliceai","title":"\ud83c\udfaf Key Features in Agentic-SpliceAI","text":""},{"location":"agency/#1-nexus-research-agent","title":"1. Nexus Research Agent","text":"<p>Searches multiple knowledge sources: - PubMed (biomedical literature) - ClinVar (clinical variants) - GTEx (tissue expression) - SpliceVarDB (splice variants) - Your own experiments (via memory!)</p> <p>Example: <pre><code>agent = NexusAgent()\nevidence = agent.research(\n    query=\"BRCA1 cryptic splice sites in breast cancer\",\n    sources=[\"pubmed\", \"clinvar\", \"memory\"]\n)\n</code></pre></p>"},{"location":"agency/#2-validation-agent","title":"2. Validation Agent","text":"<p>Validates predictions with multi-source evidence: <pre><code>validation = agent.validate(\n    site=novel_splice_site,\n    criteria=[\"sequence_motif\", \"conservation\", \"expression\", \"literature\"],\n    confidence_threshold=0.8\n)\n</code></pre></p>"},{"location":"agency/#3-agentic-memory-new","title":"3. Agentic Memory \ud83d\udd2c NEW!","text":"<p>Persistent knowledge across sessions: <pre><code># Store discovery\nagent.memory.store_discovery(site, evidence)\n\n# Query later\nprior_discoveries = agent.memory.query(\n    \"validated BRCA1 isoforms with RNA-seq support\"\n)\n</code></pre></p> <p>Learn more: See <code>AGENTIC_MEMORY_TUTORIAL.md</code></p>"},{"location":"agency/#scientific-use-cases","title":"\ud83d\udd2c Scientific Use Cases","text":""},{"location":"agency/#use-case-1-novel-isoform-discovery","title":"Use Case 1: Novel Isoform Discovery","text":"<p>Challenge: How to validate novel splice sites?</p> <p>Solution: 1. Meta-model predicts high-delta sites (novelty candidates) 2. Agent searches literature for similar cases 3. Agent validates with RNA-seq, databases 4. Agent stores discovery with full provenance 5. Agent remembers for future cross-validation</p> <p>Example: See <code>MEMORY_PATTERNS.md</code> \u2192 Pattern 2</p>"},{"location":"agency/#use-case-2-variant-interpretation","title":"Use Case 2: Variant Interpretation","text":"<p>Challenge: Is this VUS pathogenic?</p> <p>Solution: 1. Agent checks memory for similar variants 2. Agent searches ClinVar, literature 3. Agent assesses splice disruption 4. Agent provides interpretation with confidence score 5. Agent stores decision for future reference</p> <p>Example: See <code>MEMORY_PATTERNS.md</code> \u2192 Template 3</p>"},{"location":"agency/#use-case-3-experiment-design","title":"Use Case 3: Experiment Design","text":"<p>Challenge: What experiment to run next?</p> <p>Solution: 1. Agent queries memory: What have we tried? 2. Agent searches literature: What's been published? 3. Agent identifies gaps 4. Agent proposes optimal next experiment 5. Agent tracks results for future iterations</p> <p>Example: See <code>MEMORY_PATTERNS.md</code> \u2192 Pattern 4</p>"},{"location":"agency/#getting-started","title":"\ud83d\ude80 Getting Started","text":""},{"location":"agency/#quick-start","title":"Quick Start","text":"<pre><code>from agentic_spliceai import NexusAgent, AgenticMemory\n\n# Initialize agent with memory\nagent = NexusAgent(memory=AgenticMemory())\n\n# Research a gene\nfindings = agent.research(gene=\"BRCA1\", focus=\"alternative_splicing\")\n\n# Validate a discovery\nvalidation = agent.validate(site=novel_site)\n\n# Query memory\nprior_work = agent.memory.query(\"BRCA1 splice variants\")\n</code></pre>"},{"location":"agency/#cli-commands","title":"CLI Commands","text":"<pre><code># Research\nagentic-spliceai agentic research \\\n  --gene BRCA1 \\\n  --query \"pathogenic splice variants\"\n\n# Validate\nagentic-spliceai agentic validate \\\n  --predictions predictions.tsv \\\n  --mode comprehensive\n\n# Memory\nagentic-spliceai memory search \\\n  --query \"chromatin modality effectiveness\"\n</code></pre>"},{"location":"agency/#tutorials","title":"\ud83d\udcd6 Tutorials","text":""},{"location":"agency/#beginner","title":"Beginner","text":"<ol> <li>AGENTIC_MEMORY_TUTORIAL.md - Start here!</li> <li>What is agentic memory?</li> <li>Why it matters for genomics</li> <li>Basic usage examples</li> </ol>"},{"location":"agency/#intermediate","title":"Intermediate","text":"<ol> <li>MEMORY_PATTERNS.md - Practical patterns</li> <li>8 memory patterns with code</li> <li>Scientific workflow templates</li> <li>Best practices</li> </ol>"},{"location":"agency/#advanced","title":"Advanced","text":"<ol> <li>Agent Development Guide (Coming soon)</li> <li>Custom agent creation</li> <li>Memory schema design</li> <li>Multi-agent coordination</li> </ol>"},{"location":"agency/#architecture","title":"\ud83d\udd17 Architecture","text":""},{"location":"agency/#how-agents-use-memory","title":"How Agents Use Memory","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Research Question           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502     Agent Queries Memory First      \u2502\n\u2502  \"Have we seen this before?\"        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u251c\u2500 YES \u2192 Use prior knowledge as context\n               \u2502         (faster, more accurate)\n               \u2502\n               \u2514\u2500 NO  \u2192 Search external sources\n                        (PubMed, databases)\n                        Then STORE for future\n</code></pre> <p>Key Benefit: Agent gets faster and smarter over time.</p>"},{"location":"agency/#key-concepts","title":"\ud83c\udf93 Key Concepts","text":""},{"location":"agency/#concept-1-memory-vector-database","title":"Concept 1: Memory \u2260 Vector Database","text":"<p>Traditional RAG: Embed documents \u2192 Semantic search \u2192 Top-K chunks Agentic Memory: Structured facts \u2192 LLM reasoning \u2192 Relevant knowledge</p> <p>Why different: Scientific queries require logical reasoning, not just semantic similarity.</p>"},{"location":"agency/#concept-2-provenance-is-critical","title":"Concept 2: Provenance is Critical","text":"<p>Every fact links back to source: <pre><code>Claim: \"Chromatin improves F1 by 8%\"\n  \u2193\nEvidence: Experiment chromatin_chr21 (2026-02-15)\n  \u2193\nArtifacts: logs/chromatin_chr21.log, results/metrics.json\n  \u2193\nRaw Data: config/chromatin_chr21.yaml, data/chr21_validation.tsv\n</code></pre></p> <p>Why critical: Scientific reproducibility requires full audit trails.</p>"},{"location":"agency/#concept-3-memory-is-collaborative","title":"Concept 3: Memory is Collaborative","text":"<p>Multiple agents and researchers contribute to shared memory:</p> <pre><code>Discovery Agent \u2192 Finds novel sites \u2192 Stores in memory\nValidation Agent \u2192 Reads from memory \u2192 Validates sites \u2192 Updates memory\nClinical Agent \u2192 Reads validated sites \u2192 Assesses pathogenicity \u2192 Updates memory\nResearcher \u2192 Reviews memory \u2192 Approves high-confidence discoveries\n</code></pre> <p>Result: Knowledge accumulates through collective intelligence.</p>"},{"location":"agency/#success-stories-coming-soon","title":"\ud83d\udcca Success Stories (Coming Soon)","text":""},{"location":"agency/#case-study-1-brca1-isoform-discovery","title":"Case Study 1: BRCA1 Isoform Discovery","text":"<ul> <li>Discovered 7 novel cryptic sites using delta scores</li> <li>Validated with memory context in 50% less time</li> <li>Full provenance chain for publication</li> </ul>"},{"location":"agency/#case-study-2-chromatin-modality-optimization","title":"Case Study 2: Chromatin Modality Optimization","text":"<ul> <li>Tracked 47 experiments across 12 tissues</li> <li>Agent learned tissue-specific effectiveness patterns</li> <li>Optimized compute allocation (skip low-ROI modalities)</li> </ul>"},{"location":"agency/#case-study-3-collaborative-lab-memory","title":"Case Study 3: Collaborative Lab Memory","text":"<ul> <li>3 researchers contributed to shared memory</li> <li>Cross-validated discoveries independently</li> <li>Published with complete reproducibility</li> </ul>"},{"location":"agency/#related-documentation","title":"\ud83d\udd17 Related Documentation","text":"<p>In Agentic-SpliceAI: - Isoform Discovery: <code>docs/isoform_discovery/README.md</code> - Meta-Layer: <code>docs/meta_layer/README.md</code> (future) - CLI Reference: <code>docs/cli/README.md</code></p> <p>In Meta-SpliceAI (Reference): - Agentic Workflows: <code>meta-spliceai/docs/agentic_workflows/</code> - Nexus Agent: <code>meta-spliceai/docs/nexus_agent/</code></p> <p>External: - MemU Framework: https://github.com/NevaMind-AI/memU</p>"},{"location":"agency/#quick-reference","title":"\ud83c\udfaf Quick Reference","text":""},{"location":"agency/#common-commands","title":"Common Commands","text":"<pre><code># Initialize memory\nagentic-spliceai memory init\n\n# Store experiment\nagentic-spliceai memory store --category experiments --file result.json\n\n# Search memory\nagentic-spliceai memory search --query \"your question\"\n\n# Export for paper\nagentic-spliceai memory export --gene BRCA1 --output supplementary/\n\n# View statistics\nagentic-spliceai memory stats\n</code></pre>"},{"location":"agency/#python-api","title":"Python API","text":"<pre><code>from agentic_spliceai.memory import AgenticMemory\n\nmemory = AgenticMemory()\n\n# Store\nmemory.store(category=\"...\", item={...})\n\n# Query\nresults = memory.query(\"...\")\n\n# Trace\nprovenance = memory.trace(item_id=\"...\")\n</code></pre>"},{"location":"agency/#support","title":"\ud83c\udd98 Support","text":"<ul> <li>GitHub Issues: https://github.com/pleiadian53/agentic-spliceai/issues</li> <li>Documentation: https://agentic-spliceai.readthedocs.io (future)</li> <li>Discussions: https://github.com/pleiadian53/agentic-spliceai/discussions</li> </ul> <p>Ready to get started? Read <code>AGENTIC_MEMORY_TUTORIAL.md</code> next! \ud83d\ude80</p> <p>\"Transform your AI agents from tools into research collaborators.\"</p>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/","title":"Agentic Memory for Scientific Discovery","text":"<p>A Tutorial on Using Persistent Memory in AI-Powered Research Workflows</p>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#what-is-agentic-memory","title":"\ud83c\udfaf What is Agentic Memory?","text":"<p>Agentic memory enables AI agents to remember, learn, and build knowledge across sessions\u2014transforming them from stateless tools into cumulative research collaborators.</p>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#the-problem","title":"The Problem","text":"<p>Traditional AI agents are like researchers with amnesia: - \u274c Forget everything between sessions - \u274c Can't learn from past experiments - \u274c Can't connect findings across time - \u274c Can't provide historical context</p> <p>Result: You spend time explaining context every session.</p>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#the-solution","title":"The Solution","text":"<p>With agentic memory, agents become knowledge-building partners: - \u2705 Remember experimental results - \u2705 Learn what works (and what doesn't) - \u2705 Connect patterns across studies - \u2705 Provide full provenance for every claim</p> <p>Result: Agents that get smarter over time, just like human researchers.</p>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#why-this-matters-for-genomics","title":"\ud83e\uddec Why This Matters for Genomics","text":""},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#use-case-1-novel-isoform-discovery","title":"Use Case 1: Novel Isoform Discovery","text":"<p>Without Memory: <pre><code>Day 1: \"I found a novel splice site in BRCA1 exon 11\"\nDay 2: Agent forgets \u2192 starts validation from scratch\nDay 3: Repeat validation \u2192 waste time\n</code></pre></p> <p>With Memory: <pre><code>Day 1: Agent stores discovery + evidence\nDay 2: Agent remembers \u2192 \"I validated this last week, confidence 0.92\"\nDay 3: Agent builds on it \u2192 \"Let's check nearby exons for similar patterns\"\n</code></pre></p>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#use-case-2-experiment-tracking","title":"Use Case 2: Experiment Tracking","text":"<p>Without Memory: <pre><code>Researcher: \"Did adding chromatin modality help?\"\nAgent: \"I don't have that information\"\nResearcher: *Searches through 50 log files manually*\n</code></pre></p> <p>With Memory: <pre><code>Researcher: \"Did adding chromatin modality help?\"\nAgent: \"Yes. In 3 experiments (Feb 2026), F1 improved by 8% (0.87\u21920.91).\n        Especially effective for tissue-specific splicing.\n        Evidence: experiments/chromatin_ablation.json\"\n</code></pre></p>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#use-case-3-literature-knowledge","title":"Use Case 3: Literature Knowledge","text":"<p>Without Memory: <pre><code>Researcher: \"Has anyone found cryptic sites in BRCA1 exon 11?\"\nAgent: *Searches PubMed again (5 minutes)*\n</code></pre></p> <p>With Memory: <pre><code>Researcher: \"Has anyone found cryptic sites in BRCA1 exon 11?\"\nAgent: \"Yes. Wang et al. (2023, Cell) reported 127 novel junctions \n        including BRCA1 exon 11. I stored this in \n        memory/literature/Wang2023Cell.md last month.\n        Would you like the full citation?\"\n</code></pre></p>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#how-it-works","title":"\ud83c\udfd7\ufe0f How It Works","text":""},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#architecture-overview","title":"Architecture Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Your Research Workflow              \u2502\n\u2502  (Run experiments, analyze data, write)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502\n                   \u2502 stores results\n                   \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502           Agentic Memory Layer              \u2502\n\u2502                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502  Experiments \u2502  \u2502  Discoveries \u2502       \u2502\n\u2502  \u2502     (.md)    \u2502  \u2502     (.md)    \u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502                                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502  Literature  \u2502  \u2502  Hypotheses  \u2502       \u2502\n\u2502  \u2502     (.md)    \u2502  \u2502     (.md)    \u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502\n                   \u2502 traces to\n                   \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502          Raw Data &amp; Artifacts               \u2502\n\u2502  (logs, configs, papers, plots, tables)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key Principle: Memory preserves the full chain from claim \u2192 evidence \u2192 raw data.</p>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#memory-as-markdown-files","title":"Memory as Markdown Files","text":"<p>Memory is stored as human-readable Markdown files (not opaque databases):</p> <pre><code>memory/\n\u251c\u2500\u2500 experiments/\n\u2502   \u251c\u2500\u2500 chromatin_modality_2026Q1.md\n\u2502   \u2514\u2500\u2500 histone_marks_ablation.md\n\u251c\u2500\u2500 discoveries/\n\u2502   \u251c\u2500\u2500 brca1_novel_isoforms.md\n\u2502   \u2514\u2500\u2500 tp53_cryptic_sites.md\n\u251c\u2500\u2500 literature/\n\u2502   \u251c\u2500\u2500 wang2023_cell_splicing.md\n\u2502   \u2514\u2500\u2500 key_papers_alternative_splicing.md\n\u2514\u2500\u2500 hypotheses/\n    \u2514\u2500\u2500 high_delta_predicts_novel.md\n</code></pre> <p>Benefits: - \u2705 Human-readable: You can open and read any memory file - \u2705 Git-versionable: Track how knowledge evolves over time - \u2705 Editable: Correct mistakes or add notes - \u2705 Transparent: See exactly what the agent remembers</p>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#example-storing-an-experiment","title":"\ud83d\udcdd Example: Storing an Experiment","text":""},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#step-1-run-your-experiment","title":"Step 1: Run Your Experiment","text":"<pre><code>from agentic_spliceai import MetaModel\n\n# Run experiment\nmodel = MetaModel(modalities=[\"base\", \"sequence\", \"chromatin\"])\nresults = model.train(data=\"chr21\", epochs=50)\n\nprint(f\"F1 Score: {results.f1}\")  # 0.91\n</code></pre>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#step-2-store-in-memory","title":"Step 2: Store in Memory","text":"<pre><code>from agentic_spliceai.memory import AgenticMemory\n\nmemory = AgenticMemory()\n\n# Store experiment\nmemory.store_experiment({\n    \"date\": \"2026-02-15\",\n    \"hypothesis\": \"Chromatin modality improves tissue-specific predictions\",\n    \"config\": {\n        \"model\": \"meta-v2\",\n        \"modalities\": [\"base\", \"sequence\", \"chromatin\"],\n        \"test_set\": \"chr21\"\n    },\n    \"results\": {\n        \"f1\": 0.91,\n        \"precision\": 0.89,\n        \"recall\": 0.93\n    },\n    \"conclusion\": \"Chromatin effective (+8% F1 improvement)\",\n    \"artifacts\": [\n        \"logs/chromatin_exp.log\",\n        \"plots/chromatin_roc.png\"\n    ]\n})\n</code></pre>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#step-3-memory-file-created","title":"Step 3: Memory File Created","text":"<p>File: <code>memory/experiments/chromatin_modality_2026Q1.md</code></p> <pre><code># Chromatin Modality Experiment (Q1 2026)\n\n## Experiment: chromatin-chr21-validation\n**Date**: 2026-02-15  \n**Hypothesis**: Chromatin modality improves tissue-specific predictions\n\n### Setup\n- Model: meta-v2\n- Modalities: base + sequence + chromatin (ATAC-seq)\n- Test set: chr21 validation\n\n### Results\n- **F1**: 0.91 (baseline: 0.83, improvement: +8%)\n- **Precision**: 0.89\n- **Recall**: 0.93\n\n### Conclusion\nChromatin accessibility significantly improves predictions for \ntissue-specific splice sites. Most effective in genes with \ndynamic chromatin state (e.g., immune response genes).\n\n### Evidence\n- [logs/chromatin_exp.log]\n- [plots/chromatin_roc.png]\n- [configs/chromatin_exp.yaml]\n\n### Related\n- See: memory/modality_effectiveness/chromatin.md\n</code></pre>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#example-querying-memory","title":"\ud83d\udd0d Example: Querying Memory","text":""},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#natural-language-queries","title":"Natural Language Queries","text":"<pre><code># Ask a question\nresults = memory.query(\n    \"What have we learned about chromatin modality effectiveness?\"\n)\n\n# Agent finds relevant memories\nfor result in results:\n    print(f\"Category: {result.category}\")\n    print(f\"Finding: {result.content}\")\n    print(f\"Evidence: {result.links}\")\n    print(\"---\")\n</code></pre> <p>Output: <pre><code>Category: experiments\nFinding: Chromatin modality improved F1 by 8% (0.83 \u2192 0.91) in chr21 validation\nEvidence: [logs/chromatin_exp.log, plots/chromatin_roc.png]\n---\n\nCategory: modality_effectiveness\nFinding: Chromatin most effective for tissue-specific splicing (F1=0.89)\n         Least effective for housekeeping genes (F1=0.71)\nEvidence: [47 experiments across 12 tissues]\n---\n\nCategory: literature\nFinding: Wang et al. (2023) showed chromatin remodeling activates cryptic sites\nEvidence: [papers/Wang_2023_Cell.pdf]\n---\n</code></pre></p>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#complex-logical-queries","title":"Complex Logical Queries","text":"<p>MemU uses LLM-based search (not just embeddings), enabling complex logic:</p> <pre><code># Find experiments where:\n# - Chromatin was used\n# - AND F1 &gt; 0.9\n# - AND tested on cancer samples\nresults = memory.query(\n    \"experiments with chromatin modality, F1 above 0.9, on cancer samples\"\n)\n</code></pre> <p>Why this matters: Semantic search would miss the logical constraints (F1 &gt; 0.9, cancer-only).</p>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#example-novel-discovery-workflow","title":"\ud83d\udd2c Example: Novel Discovery Workflow","text":""},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#full-workflow-with-memory","title":"Full Workflow with Memory","text":"<pre><code>from agentic_spliceai import MetaModel, NexusAgent, AgenticMemory\n\n# Initialize\nmodel = MetaModel()\nagent = NexusAgent(memory=AgenticMemory())\n\n# 1. Make predictions\npredictions = model.predict(gene=\"BRCA1\")\n\n# 2. Find high-delta sites (potential novel isoforms)\nnovel_sites = [s for s in predictions if s.delta_score &gt; 0.5]\n\n# 3. Validate with memory context\nfor site in novel_sites:\n    # Agent searches memory for similar cases\n    similar = agent.memory.query(\n        f\"novel splice sites in {site.gene} with high chromatin signal\"\n    )\n\n    # Agent validates with historical context\n    validation = agent.validate(\n        site,\n        context=similar,  # \u2190 Uses past discoveries!\n        sources=[\"literature\", \"rnaseq\", \"databases\"]\n    )\n\n    # 4. Store discovery if high confidence\n    if validation.confidence &gt; 0.8:\n        agent.memory.store_discovery(\n            site=site,\n            evidence=validation.evidence,\n            confidence=validation.confidence\n        )\n\n        print(f\"\u2705 Novel site discovered: {site.location}\")\n        print(f\"   Confidence: {validation.confidence}\")\n        print(f\"   Evidence: {len(validation.evidence)} sources\")\n        print(f\"   Similar cases: {len(similar)} in memory\")\n</code></pre> <p>Output: <pre><code>\u2705 Novel site discovered: chr17:43,094,582\n   Confidence: 0.92\n   Evidence: 4 sources (literature + RNA-seq + databases + chromatin)\n   Similar cases: 3 in memory (BRCA1 exon 10, exon 12)\n</code></pre></p>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#what-got-stored","title":"What Got Stored","text":"<p>File: <code>memory/discoveries/brca1_novel_isoforms.md</code></p> <pre><code># BRCA1 Novel Isoforms\n\n## Discovery: BRCA1_exon11_cryptic_donor_20260220\n\n**Location**: chr17:43,094,582 (GRCh38)  \n**Type**: Cryptic donor (GT motif)  \n**Discovery Date**: 2026-02-20\n\n### Prediction Scores\n- Base model: 0.15 (LOW - not in training data)\n- Meta model: 0.88 (HIGH - strong multimodal evidence)\n- **Delta: 0.73** (large gap \u2192 novel signal)\n\n### Evidence\n**Sequence**: AGGTAAGT (canonical GT donor, 0.90 strength)  \n**Chromatin**: 0.82 (open in breast tissue)  \n**Histone**: H3K36me3 = 0.76 (active transcription)  \n**RNA-seq**: 24 junction reads in TCGA-BRCA  \n**Literature**: Confirmed by Venkitaraman (2014, Nature)\n\n### Validation\n- **Confidence**: 0.92 (HIGH)\n- **Agent**: nexus-v2\n- **Status**: \u2705 Validated\n- **Clinical**: Likely pathogenic (familial breast cancer)\n\n### Provenance\n- [rnaseq/TCGA-BRCA/junction_counts.tsv]\n- [papers/Venkitaraman_2014_Nature.pdf]\n- [predictions/meta-v2/BRCA1_predictions.tsv]\n- [igv_screenshots/BRCA1_exon11_junction.png]\n</code></pre>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#scientific-workflows","title":"\ud83e\uddea Scientific Workflows","text":""},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#workflow-1-hypothesis-testing","title":"Workflow 1: Hypothesis Testing","text":"<pre><code># Propose hypothesis\nagent.memory.store_hypothesis({\n    \"title\": \"High-delta sites predict novel isoforms\",\n    \"definition\": \"Sites with delta &gt; 0.5 are enriched for novel sites\",\n    \"rationale\": \"Base model hasn't seen them, meta model has context\",\n    \"proposed\": \"2026-01-30\"\n})\n\n# Run experiments\nfor experiment in [\"chr21\", \"brca1\", \"tp53\"]:\n    results = run_experiment(experiment)\n    agent.memory.add_experiment_to_hypothesis(\n        hypothesis_id=\"high_delta_predicts_novel\",\n        experiment=results\n    )\n\n# Query hypothesis status\nstatus = agent.memory.query(\"status of high-delta hypothesis\")\n# \u2192 \"Validated in 3/3 experiments. Precision = 0.85, Recall = 0.73\"\n</code></pre>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#workflow-2-error-pattern-analysis","title":"Workflow 2: Error Pattern Analysis","text":"<pre><code># Analyze false positives\nfps = model.get_false_positives(test_set=\"chr21\")\n\n# Store pattern\nagent.memory.store_error_pattern({\n    \"type\": \"false_positive\",\n    \"pattern\": \"High FP rate in repetitive regions\",\n    \"frequency\": 0.23,  # 23% of all FPs\n    \"cause\": \"Base model trained on canonical sites\",\n    \"mitigation\": \"Add repeat masking + chromatin filter\",\n    \"effectiveness\": \"78% FP reduction\"\n})\n\n# Later experiments automatically benefit\nagent.memory.query(\"how to reduce false positives?\")\n# \u2192 Returns stored mitigation strategies\n</code></pre>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#workflow-3-cross-study-validation","title":"Workflow 3: Cross-Study Validation","text":"<pre><code># Discovery in Study 1 (BRCA1)\nagent.memory.store_discovery({\n    \"gene\": \"BRCA1\",\n    \"site\": \"chr17:43,094,582\",\n    \"evidence\": [\"TCGA-BRCA RNA-seq\"],\n    \"confidence\": 0.85\n})\n\n# Study 2 (TP53) - Agent checks for similar patterns\nsimilar = agent.memory.query(\n    \"discoveries with cryptic donors and chromatin evidence\"\n)\n\n# Agent: \"Found similar pattern in BRCA1. Let me check TP53...\"\n# \u2192 Cross-validates across studies\n# \u2192 Builds confidence through independent replication\n</code></pre>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#benefits-for-your-research","title":"\ud83d\udcca Benefits for Your Research","text":""},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#1-time-savings","title":"1. Time Savings","text":"<ul> <li>No more searching through old log files</li> <li>No more re-reading papers you've already processed</li> <li>No more explaining context to tools each session</li> </ul> <p>Estimate: Save 2-5 hours/week</p>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#2-better-science","title":"2. Better Science","text":"<ul> <li>Reproducibility: Full provenance for every claim</li> <li>Cumulative learning: Build on past experiments</li> <li>Cross-validation: Connect findings across studies</li> <li>Error avoidance: Remember what didn't work</li> </ul>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#3-publication-ready","title":"3. Publication Ready","text":"<p>Memory files can directly become: - Methods sections (full experimental details) - Supplementary materials (all evidence chains) - Data availability statements (all artifacts linked)</p>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#4-collaborative","title":"4. Collaborative","text":"<ul> <li>Share memory with lab members</li> <li>Build institutional knowledge</li> <li>Onboard new researchers faster</li> </ul>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#getting-started","title":"\ud83d\ude80 Getting Started","text":""},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#installation","title":"Installation","text":"<pre><code># Install Agentic-SpliceAI with memory support\npip install agentic-spliceai[memory]\n\n# Initialize memory\nagentic-spliceai memory init --dir ./memory\n</code></pre>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#basic-usage","title":"Basic Usage","text":"<pre><code>from agentic_spliceai.memory import AgenticMemory\n\n# Create memory\nmemory = AgenticMemory()\n\n# Store something\nmemory.store(\n    category=\"experiments\",\n    item={\"name\": \"My first experiment\", \"result\": \"success\"}\n)\n\n# Query it\nresults = memory.query(\"my first experiment\")\nprint(results)  # Finds it!\n</code></pre>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#cli-commands","title":"CLI Commands","text":"<pre><code># Search memory\nagentic-spliceai memory search \\\n  --query \"chromatin modality effectiveness\"\n\n# Store experiment\nagentic-spliceai memory store \\\n  --category experiments \\\n  --file experiment_results.json\n\n# Export for paper\nagentic-spliceai memory export \\\n  --categories discoveries,literature \\\n  --gene BRCA1 \\\n  --output supplementary_materials/\n</code></pre>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#best-practices","title":"\ud83c\udf93 Best Practices","text":""},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#1-store-early-store-often","title":"1. Store Early, Store Often","text":"<p>Don't wait until \"final results\"\u2014store intermediate findings too.</p> <p>Why: Negative results and failed experiments are valuable knowledge!</p>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#2-always-link-to-raw-data","title":"2. Always Link to Raw Data","text":"<p>Every memory item should link back to source files.</p> <pre><code>memory.store({\n    \"finding\": \"Chromatin improved F1 by 8%\",\n    \"evidence\": [\n        \"logs/experiment_123.log\",  # \u2190 Link to raw\n        \"results/metrics.json\"\n    ]\n})\n</code></pre>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#3-use-descriptive-categories","title":"3. Use Descriptive Categories","text":"<p>Organize memory by scientific theme, not tool structure.</p> <p>Good: <code>memory/tissue_specific_splicing/</code> Bad: <code>memory/model_outputs/</code></p>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#4-review-high-stakes-memories","title":"4. Review High-Stakes Memories","text":"<p>Validate discoveries before storing as \"confirmed.\"</p> <pre><code>if validation.confidence &gt; 0.9:\n    memory.store(item, reviewed=True, reviewer=\"expert@uni.edu\")\n</code></pre>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#5-query-before-you-start","title":"5. Query Before You Start","text":"<p>Check memory before running experiments.</p> <pre><code># Before experiment\nprior_work = memory.query(\"chromatin modality on chr21\")\nif prior_work:\n    print(\"We tried this before! Here's what we learned:\")\n    print(prior_work)\n</code></pre>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#advanced-topics","title":"\ud83d\udcda Advanced Topics","text":""},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#memory-provenance-chains","title":"Memory Provenance Chains","text":"<p>Every claim traces back through full evidence chain:</p> <pre><code>Discovery: \"BRCA1 exon 11 cryptic donor\"\n    \u2193\nEvidence: \"24 RNA-seq junction reads\"\n    \u2193\nExperiment: \"TCGA-BRCA analysis\"\n    \u2193\nRaw Data: \"tcga_brca_junctions.bam\"\n</code></pre> <p>Query any level: <pre><code>memory.trace_provenance(discovery_id=\"BRCA1_exon11_cryptic\")\n# Returns full chain with file paths\n</code></pre></p>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#memory-versioning","title":"Memory Versioning","text":"<p>Memory files are Markdown \u2192 version with Git:</p> <pre><code>git log memory/discoveries/brca1_novel_isoforms.md\n# See how knowledge evolved over time\n</code></pre>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#collaborative-memory","title":"Collaborative Memory","text":"<p>Share memory across team:</p> <pre><code># Clone lab memory\ngit clone lab-server:/memory ./shared_memory\n\n# Use it\nmemory = AgenticMemory(root_dir=\"./shared_memory\")\n\n# Contribute discoveries\nmemory.store_discovery(...)\ngit commit -m \"Added TP53 cryptic site validation\"\ngit push\n</code></pre>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#learn-more","title":"\ud83d\udd17 Learn More","text":"<ul> <li>Framework: MemU (https://github.com/NevaMind-AI/memU)</li> <li>Integration: See <code>dev/planning/agentic_workflows/MEMORY_LAYER.md</code></li> <li>Use Cases: See <code>docs/agency/MEMORY_PATTERNS.md</code></li> <li>API Reference: See <code>docs/api/memory.md</code></li> </ul>"},{"location":"agency/AGENTIC_MEMORY_TUTORIAL/#summary","title":"\ud83c\udf89 Summary","text":"<p>Agentic memory transforms AI agents from tools into research collaborators that: - \u2705 Remember what you've learned - \u2705 Build cumulative knowledge - \u2705 Provide full provenance - \u2705 Get smarter over time</p> <p>The result: Better science, faster discovery, stronger publications.</p> <p>Try it today: <code>pip install agentic-spliceai[memory]</code></p> <p>Questions? Open an issue on GitHub or email research@agentic-spliceai.org</p> <p>\"The best research assistant is one that remembers everything you've taught it.\"</p>"},{"location":"agency/MEMORY_PATTERNS/","title":"Memory Patterns for Scientific Workflows","text":"<p>A Guide to Using Agentic Memory in Genomics Research</p>"},{"location":"agency/MEMORY_PATTERNS/#overview","title":"\ud83c\udfaf Overview","text":"<p>This document provides concrete patterns for using agentic memory in splice site prediction, isoform discovery, and variant interpretation workflows.</p> <p>Target Audience: Researchers, bioinformaticians, computational biologists</p>"},{"location":"agency/MEMORY_PATTERNS/#pattern-catalog","title":"\ud83d\udcda Pattern Catalog","text":""},{"location":"agency/MEMORY_PATTERNS/#pattern-1-experiment-memory-chain","title":"Pattern 1: Experiment Memory Chain","text":"<p>Use Case: Track ablation studies, remember what works</p> <p>Example: Testing multimodal fusion</p> <pre><code>from agentic_spliceai.memory import AgenticMemory\n\nmemory = AgenticMemory()\n\n# Baseline experiment\nbaseline = train_model(modalities=[\"base\", \"sequence\"])\nmemory.store_experiment({\n    \"name\": \"baseline_chr21\",\n    \"modalities\": [\"base\", \"sequence\"],\n    \"f1\": 0.83,\n    \"conclusion\": \"Baseline performance established\"\n})\n\n# Add chromatin\nchromatin = train_model(modalities=[\"base\", \"sequence\", \"chromatin\"])\nmemory.store_experiment({\n    \"name\": \"chromatin_chr21\",\n    \"modalities\": [\"base\", \"sequence\", \"chromatin\"],\n    \"f1\": 0.91,\n    \"delta_from_baseline\": +0.08,\n    \"conclusion\": \"Chromatin improves tissue-specific prediction\"\n})\n\n# Add histone marks\nhistone = train_model(modalities=[\"base\", \"sequence\", \"chromatin\", \"histone\"])\nmemory.store_experiment({\n    \"name\": \"histone_chr21\",\n    \"modalities\": [\"base\", \"sequence\", \"chromatin\", \"histone\"],\n    \"f1\": 0.93,\n    \"delta_from_chromatin\": +0.02,\n    \"conclusion\": \"Histone marks provide marginal improvement\"\n})\n\n# Query accumulated knowledge\ninsights = memory.query(\"which modalities improved performance most?\")\n# \u2192 \"Chromatin: +8%, Histone: +2%. Chromatin has best ROI.\"\n</code></pre> <p>Memory Benefit: Agent learns which modalities to prioritize for future experiments.</p>"},{"location":"agency/MEMORY_PATTERNS/#pattern-2-discovery-validation-loop","title":"Pattern 2: Discovery Validation Loop","text":"<p>Use Case: Build confidence through cross-validation</p> <p>Example: Novel splice site in multiple contexts</p> <pre><code># Discovery in Study 1 (BRCA1 tumor samples)\nsite = {\n    \"gene\": \"BRCA1\",\n    \"location\": \"chr17:43,094,582\",\n    \"type\": \"cryptic_donor\"\n}\n\n# Initial validation\nvalidation1 = agent.validate(site, context=\"TCGA-BRCA\")\nmemory.store_discovery(\n    site=site,\n    evidence=validation1,\n    confidence=0.75,  # Medium confidence\n    note=\"Found in tumor RNA-seq, needs more validation\"\n)\n\n# Study 2 (GTEx normal breast)\n# Agent checks memory first\nprior = memory.query(\"BRCA1 chr17:43,094,582\")\n# \u2192 Finds Study 1 discovery\n\nvalidation2 = agent.validate(site, context=\"GTEx-Breast\", prior=prior)\nmemory.update_discovery(\n    site=site,\n    new_evidence=validation2,\n    confidence=0.88,  # Higher confidence (two sources)\n    note=\"Confirmed in normal tissue too\"\n)\n\n# Study 3 (Literature search)\nliterature = agent.search_literature(site, prior=memory.get_all(site))\nmemory.update_discovery(\n    site=site,\n    new_evidence=literature,\n    confidence=0.94,  # Very high confidence (three sources)\n    status=\"validated\",\n    note=\"Validated by Wang et al. 2023\"\n)\n</code></pre> <p>Memory Benefit: Confidence builds over time through independent validation. Agent tracks the full evidence chain.</p>"},{"location":"agency/MEMORY_PATTERNS/#pattern-3-error-pattern-learning","title":"Pattern 3: Error Pattern Learning","text":"<p>Use Case: Learn from mistakes, avoid repeated errors</p> <p>Example: False positives in repetitive regions</p> <pre><code># Week 1: Notice error pattern\nfps = analyze_false_positives(results)\nmemory.store_error_pattern({\n    \"pattern\": \"FPs clustered in Alu repeats\",\n    \"frequency\": 0.23,\n    \"cause\": \"Sequence similarity to canonical sites\",\n    \"first_observed\": \"2026-02-01\"\n})\n\n# Week 2: Test mitigation\nresult = apply_repeat_filter(model)\nmemory.update_error_pattern({\n    \"pattern\": \"FPs in Alu repeats\",\n    \"mitigation\": \"RepeatMasker filter\",\n    \"effectiveness\": \"Reduced FPs by 12%\",\n    \"side_effects\": \"May lose rare cryptic sites in repeats\"\n})\n\n# Week 3: Improve mitigation\nresult = apply_chromatin_filter(model)\nmemory.update_error_pattern({\n    \"pattern\": \"FPs in Alu repeats\",\n    \"mitigation\": \"Chromatin accessibility filter\",\n    \"effectiveness\": \"Reduced FPs by 18% (combined with repeat mask)\",\n    \"recommended\": True\n})\n\n# Future experiments automatically apply learned mitigation\nfuture_predictions = model.predict(\n    gene=\"TP53\",\n    filters=memory.get_recommended_filters()  # \u2190 Uses learned knowledge!\n)\n</code></pre> <p>Memory Benefit: Agent learns best practices from experience, applies them automatically.</p>"},{"location":"agency/MEMORY_PATTERNS/#pattern-4-modality-effectiveness-tracking","title":"Pattern 4: Modality Effectiveness Tracking","text":"<p>Use Case: Learn when to trust which evidence sources</p> <p>Example: Building modality profiles</p> <pre><code># Experiment across tissues\nfor tissue in [\"brain\", \"liver\", \"breast\", \"heart\"]:\n    result = train_tissue_specific_model(\n        tissue=tissue,\n        modalities=[\"base\", \"sequence\", \"chromatin\"]\n    )\n\n    memory.store_modality_effectiveness({\n        \"modality\": \"chromatin\",\n        \"context\": f\"tissue_specific_{tissue}\",\n        \"f1\": result.f1,\n        \"analysis\": result.feature_importance[\"chromatin\"]\n    })\n\n# Later: Agent learns patterns\neffectiveness = memory.query(\n    \"chromatin modality effectiveness across tissues\"\n)\n\n# Agent discovers: \"Chromatin most effective in brain (F1=0.92) \n#                   and immune (F1=0.89), less in liver (F1=0.74)\"\n\n# Future experiments use this knowledge\nif tissue == \"brain\":\n    modalities = [\"base\", \"sequence\", \"chromatin\"]  # High priority!\nelif tissue == \"liver\":\n    modalities = [\"base\", \"sequence\"]  # Skip chromatin (low ROI)\n</code></pre> <p>Memory Benefit: Agent learns resource allocation strategy (which modalities worth the compute cost).</p>"},{"location":"agency/MEMORY_PATTERNS/#pattern-5-literature-knowledge-base","title":"Pattern 5: Literature Knowledge Base","text":"<p>Use Case: Build cumulative literature knowledge</p> <p>Example: Mining alternative splicing papers</p> <pre><code># Week 1: Read and store key paper\npaper = extract_from_pdf(\"Wang_2023_Cell.pdf\")\nmemory.store_literature({\n    \"citation\": \"Wang et al., Cell 2023\",\n    \"key_finding\": \"Chromatin remodeling activates cryptic sites in cancer\",\n    \"datasets\": [\"TCGA pan-cancer\"],\n    \"methods\": [\"RNA-seq\", \"ChIP-seq\"],\n    \"relevance\": \"Supports chromatin modality for cancer-specific isoforms\"\n})\n\n# Week 5: New experiment validates paper's finding\nresult = test_chromatin_in_cancer()\nmemory.link_experiment_to_literature(\n    experiment_id=\"chromatin_cancer_2026\",\n    paper_id=\"Wang2023Cell\",\n    relationship=\"validates\"\n)\n\n# Week 10: Writing paper\nevidence = memory.query(\"evidence for chromatin modality in cancer\")\n# \u2192 Returns: Your experiment + Wang 2023 + full provenance\n# \u2192 Can cite both in paper with confidence\n</code></pre> <p>Memory Benefit: Agent builds reusable literature knowledge base, connects your findings to prior art.</p>"},{"location":"agency/MEMORY_PATTERNS/#advanced-patterns","title":"\ud83d\udd2c Advanced Patterns","text":""},{"location":"agency/MEMORY_PATTERNS/#pattern-6-hypothesis-evolution","title":"Pattern 6: Hypothesis Evolution","text":"<p>Use Case: Track how hypotheses evolve with new evidence</p> <pre><code># Initial hypothesis (weak)\nmemory.store_hypothesis({\n    \"id\": \"delta_predicts_novel\",\n    \"version\": 1,\n    \"statement\": \"Delta &gt; 0.5 predicts novel sites\",\n    \"confidence\": 0.5,\n    \"based_on\": \"Intuition + small pilot (n=10)\"\n})\n\n# Experiment 1 (supports)\nresult1 = validate_on_chr21()\nmemory.evolve_hypothesis({\n    \"id\": \"delta_predicts_novel\",\n    \"version\": 2,\n    \"confidence\": 0.7,\n    \"evidence\": [result1],\n    \"note\": \"Validated on chr21 (AUC=0.84)\"\n})\n\n# Experiment 2 (strongly supports)\nresult2 = validate_on_brca1_tp53()\nmemory.evolve_hypothesis({\n    \"id\": \"delta_predicts_novel\",\n    \"version\": 3,\n    \"confidence\": 0.9,\n    \"evidence\": [result1, result2],\n    \"note\": \"Strong validation across multiple genes\"\n})\n\n# Publication: Show full evolution\nhypothesis_history = memory.get_hypothesis_evolution(\"delta_predicts_novel\")\n# \u2192 Version 1 (intuition) \u2192 Version 2 (chr21) \u2192 Version 3 (validated)\n# \u2192 Full story for Methods section\n</code></pre>"},{"location":"agency/MEMORY_PATTERNS/#pattern-7-multi-agent-knowledge-sharing","title":"Pattern 7: Multi-Agent Knowledge Sharing","text":"<p>Use Case: Coordinate multiple specialized agents</p> <p>Example: Discovery \u2192 Validation \u2192 Clinical pipeline</p> <pre><code># Agent 1: Discovery Agent\ndiscovery_agent = DiscoveryAgent(memory=shared_memory)\nnovel_sites = discovery_agent.find_novel_sites(gene=\"BRCA1\")\n\nfor site in novel_sites:\n    shared_memory.store({\n        \"category\": \"pending_validation\",\n        \"site\": site,\n        \"discovered_by\": \"discovery_agent\"\n    })\n\n# Agent 2: Validation Agent (reads from shared memory)\nvalidation_agent = ValidationAgent(memory=shared_memory)\npending = shared_memory.query(\"sites pending validation\")\n\nfor site in pending:\n    validation = validation_agent.validate(site)\n    shared_memory.move(\n        from_category=\"pending_validation\",\n        to_category=\"validated_discoveries\" if validation.confidence &gt; 0.8 else \"low_confidence\",\n        update={\"validation\": validation}\n    )\n\n# Agent 3: Clinical Agent (reads validated discoveries)\nclinical_agent = ClinicalAgent(memory=shared_memory)\nvalidated = shared_memory.query(\"validated discoveries with high delta\")\n\nfor discovery in validated:\n    clinical = clinical_agent.assess_pathogenicity(discovery)\n    shared_memory.update({\n        \"discovery\": discovery,\n        \"clinical_assessment\": clinical,\n        \"assessed_by\": \"clinical_agent\"\n    })\n</code></pre> <p>Memory Benefit: Agents coordinate through shared memory, each adds their expertise to the same knowledge base.</p>"},{"location":"agency/MEMORY_PATTERNS/#pattern-8-time-series-analysis","title":"Pattern 8: Time-Series Analysis","text":"<p>Use Case: Track metrics over time, detect trends</p> <p>Example: Model performance evolution</p> <pre><code># Store monthly performance\nfor month in [\"2026-01\", \"2026-02\", \"2026-03\"]:\n    result = evaluate_model(month=month)\n    memory.store({\n        \"category\": \"model_performance\",\n        \"date\": month,\n        \"f1\": result.f1,\n        \"novel_sites_detected\": result.novel_count,\n        \"validation_rate\": result.validation_rate\n    })\n\n# Query trend\ntrend = memory.query(\"model performance over time\")\n# \u2192 Agent detects: \"F1 improving (+5% per month), \n#                   validation rate stable (0.85)\"\n\n# Visualize\nmemory.plot_metric_over_time(\n    metric=\"f1\",\n    category=\"model_performance\",\n    output=\"reports/model_improvement.png\"\n)\n</code></pre>"},{"location":"agency/MEMORY_PATTERNS/#scientific-workflow-templates","title":"\ud83c\udfaf Scientific Workflow Templates","text":""},{"location":"agency/MEMORY_PATTERNS/#template-1-modality-ablation-study","title":"Template 1: Modality Ablation Study","text":"<pre><code>def run_ablation_study(gene: str, modalities: list):\n    \"\"\"Systematic modality ablation with memory tracking.\"\"\"\n\n    memory = AgenticMemory()\n    results = {}\n\n    # Baseline\n    baseline = train_model(modalities=[\"base\", \"sequence\"])\n    memory.store_experiment({\n        \"study\": f\"ablation_{gene}\",\n        \"variant\": \"baseline\",\n        \"modalities\": [\"base\", \"sequence\"],\n        \"f1\": baseline.f1\n    })\n    results[\"baseline\"] = baseline.f1\n\n    # Add each modality\n    for modality in modalities:\n        current_modalities = [\"base\", \"sequence\", modality]\n        model = train_model(modalities=current_modalities)\n\n        delta = model.f1 - baseline.f1\n\n        memory.store_experiment({\n            \"study\": f\"ablation_{gene}\",\n            \"variant\": f\"+{modality}\",\n            \"modalities\": current_modalities,\n            \"f1\": model.f1,\n            \"delta\": delta,\n            \"interpretation\": f\"{modality} contributes {delta:+.2%}\"\n        })\n\n        results[modality] = model.f1\n\n    # Generate report from memory\n    report = memory.generate_ablation_report(study=f\"ablation_{gene}\")\n    return report\n</code></pre>"},{"location":"agency/MEMORY_PATTERNS/#template-2-discovery-pipeline-with-memory","title":"Template 2: Discovery Pipeline with Memory","text":"<pre><code>def discover_novel_isoforms(gene: str, context: dict):\n    \"\"\"Full discovery pipeline with memory integration.\"\"\"\n\n    agent = NexusAgent(memory=AgenticMemory())\n\n    # Step 1: Check if we've analyzed this gene before\n    prior = agent.memory.query(f\"discoveries in {gene}\")\n    if prior:\n        print(f\"Found {len(prior)} prior discoveries in {gene}\")\n\n    # Step 2: Run meta-model\n    predictions = meta_model.predict(gene=gene, context=context)\n\n    # Step 3: Flag high-delta sites\n    candidates = [s for s in predictions if s.delta_score &gt; 0.5]\n\n    # Step 4: Validate each candidate\n    discoveries = []\n    for site in candidates:\n        # Check memory for similar patterns\n        similar = agent.memory.query(\n            f\"sites with delta &gt; 0.5 in {context['tissue']} tissue\"\n        )\n\n        # Validate with context\n        validation = agent.validate(\n            site,\n            similar_cases=similar,\n            sources=[\"literature\", \"rnaseq\", \"databases\"]\n        )\n\n        # Store if high confidence\n        if validation.confidence &gt; 0.8:\n            agent.memory.store_discovery(site, validation)\n            discoveries.append(site)\n\n    # Step 5: Generate report\n    report = agent.memory.generate_discovery_report(\n        gene=gene,\n        discoveries=discoveries,\n        include_provenance=True\n    )\n\n    return report\n</code></pre>"},{"location":"agency/MEMORY_PATTERNS/#template-3-variant-interpretation-with-historical-context","title":"Template 3: Variant Interpretation with Historical Context","text":"<pre><code>def interpret_variant(variant: dict):\n    \"\"\"Interpret variant with memory of similar cases.\"\"\"\n\n    agent = NexusAgent(memory=AgenticMemory())\n\n    # Check memory for similar variants\n    similar = agent.memory.query(\n        f\"pathogenic variants affecting {variant['gene']} splice sites\"\n    )\n\n    # Check if we've seen this exact variant\n    exact = agent.memory.query(\n        f\"variant {variant['hgvs']} in {variant['gene']}\"\n    )\n\n    if exact:\n        print(f\"We've analyzed this variant before!\")\n        print(f\"Prior assessment: {exact[0]['pathogenicity']}\")\n        print(f\"Evidence: {exact[0]['evidence']}\")\n        return exact[0]\n\n    # New variant - validate with context\n    interpretation = agent.validate_variant(\n        variant,\n        similar_cases=similar  # Historical context helps!\n    )\n\n    # Store for future\n    agent.memory.store({\n        \"category\": \"variant_interpretations\",\n        \"variant\": variant,\n        \"interpretation\": interpretation,\n        \"assessed_date\": datetime.now()\n    })\n\n    return interpretation\n</code></pre>"},{"location":"agency/MEMORY_PATTERNS/#template-4-literature-guided-experiments","title":"Template 4: Literature-Guided Experiments","text":"<p>Use Case: Design experiments based on literature memory</p> <pre><code>def design_next_experiment(research_question: str):\n    \"\"\"Agent designs experiment using literature memory.\"\"\"\n\n    agent = NexusAgent(memory=AgenticMemory())\n\n    # Query literature memory\n    relevant_papers = agent.memory.query(\n        f\"literature about {research_question}\"\n    )\n\n    # Query past experiments\n    past_experiments = agent.memory.query(\n        f\"experiments related to {research_question}\"\n    )\n\n    # Agent reasons: What's been tried? What worked? What's missing?\n    design = agent.llm.propose_experiment(\n        research_question=research_question,\n        literature=relevant_papers,\n        past_experiments=past_experiments,\n        prompt=\"\"\"\n        Based on:\n        1. What the literature has shown\n        2. What we've already tried\n        3. What gaps remain\n\n        Propose the next experiment that will maximize learning.\n        \"\"\"\n    )\n\n    return design\n</code></pre> <p>Example Output: <pre><code>Research Question: \"Can epigenetic marks predict cryptic site activation?\"\n\nLiterature Memory:\n- Wang 2023: H3K27ac marks active enhancers\n- Smith 2022: H3K36me3 marks exons\n- Jones 2024: Cancer cells show altered H3K27ac\n\nPast Experiments:\n- Tried chromatin (F1=0.91) \u2713\n- Tried conservation (F1=0.85) \u2713\n- Haven't tried histone marks yet \u2717\n\nProposed Experiment:\n\"Test H3K27ac + H3K36me3 as joint modality for cryptic site prediction \n in cancer samples. Hypothesis: H3K27ac will distinguish activated \n cryptic sites from dormant ones. Expected improvement: +5-10% F1.\"\n\nRationale: Literature strongly supports, we have the data (ENCODE), \n           and it fills a gap in our current experiments.\n</code></pre></p>"},{"location":"agency/MEMORY_PATTERNS/#practical-examples","title":"\ud83e\uddea Practical Examples","text":""},{"location":"agency/MEMORY_PATTERNS/#example-1-daily-research-workflow","title":"Example 1: Daily Research Workflow","text":"<pre><code># Morning: Check what happened yesterday\nyesterday = memory.query(\"experiments completed yesterday\")\nagent.summarize(yesterday)\n\n# Run today's experiment\nresult = train_new_model_variant()\nmemory.store_experiment(result)\n\n# Query for insights\ninsights = memory.query(\"what patterns emerged this week?\")\nagent.analyze(insights)\n\n# Evening: Generate weekly summary\nsummary = memory.generate_summary(timeframe=\"this_week\")\n</code></pre>"},{"location":"agency/MEMORY_PATTERNS/#example-2-pre-experiment-check","title":"Example 2: Pre-Experiment Check","text":"<pre><code>def before_experiment(config: dict):\n    \"\"\"Check memory before spending compute.\"\"\"\n\n    # Have we tried this before?\n    similar = memory.query(\n        f\"experiments with modalities {config['modalities']} on {config['test_set']}\"\n    )\n\n    if similar:\n        print(\"\u26a0\ufe0f  Similar experiment found!\")\n        print(f\"   {similar[0]['name']}: F1 = {similar[0]['f1']}\")\n        print(\"   Consider adjusting hyperparameters or skipping.\")\n\n        user_input = input(\"Continue anyway? (y/n): \")\n        if user_input.lower() != 'y':\n            return \"Experiment skipped (duplicate)\"\n\n    # Proceed\n    return run_experiment(config)\n</code></pre> <p>Benefit: Avoid wasting compute on duplicate experiments.</p>"},{"location":"agency/MEMORY_PATTERNS/#example-3-evidence-aggregation-for-papers","title":"Example 3: Evidence Aggregation for Papers","text":"<pre><code>def prepare_paper_evidence(gene: str):\n    \"\"\"Aggregate all evidence for a gene into paper-ready format.\"\"\"\n\n    # Query all relevant memory\n    discoveries = memory.query(f\"validated discoveries in {gene}\")\n    experiments = memory.query(f\"experiments validating {gene} sites\")\n    literature = memory.query(f\"literature supporting {gene} findings\")\n\n    # Generate structured output\n    evidence = {\n        \"discoveries\": discoveries,\n        \"experiments\": experiments,\n        \"literature\": literature,\n        \"full_provenance\": memory.trace_all(gene)\n    }\n\n    # Export to Markdown (ready for Supplementary Materials)\n    memory.export(\n        evidence,\n        format=\"markdown\",\n        output=f\"supplementary/{gene}_evidence.md\"\n    )\n\n    return evidence\n</code></pre> <p>Output: <code>supplementary/BRCA1_evidence.md</code> <pre><code># BRCA1: Evidence Summary\n\n## Discoveries (3)\n1. Cryptic donor at exon 11 (validated, confidence 0.94)\n2. Novel acceptor at exon 18 (validated, confidence 0.87)\n3. Exon 7 skipping isoform (pending validation, confidence 0.72)\n\n## Experimental Evidence (5)\n- chromatin_ablation_2026-02-15: F1 = 0.91 (+8%)\n- histone_marks_2026-02-20: F1 = 0.93 (+2%)\n- ...\n\n## Literature Support (3)\n- Wang et al. 2023, Cell: Chromatin remodeling in cancer\n- Venkitaraman 2014, Nature: BRCA1 exon 11 cryptic donor\n- ...\n\n## Full Provenance\n[Discovery 1] \u2192 [Experiment chromatin_2026] \u2192 [TCGA-BRCA RNA-seq] \u2192 [Raw BAM files]\n[Discovery 1] \u2192 [Literature Wang2023] \u2192 [Paper PDF] \u2192 [Figure 3B]\n</code></pre></p> <p>Benefit: One command generates complete supplementary materials!</p>"},{"location":"agency/MEMORY_PATTERNS/#memory-query-patterns","title":"\ud83d\udcca Memory Query Patterns","text":""},{"location":"agency/MEMORY_PATTERNS/#query-type-1-factual-lookup","title":"Query Type 1: Factual Lookup","text":"<pre><code># Simple fact retrieval\nmemory.query(\"What was the F1 score in experiment chromatin_chr21?\")\n# \u2192 \"0.91\"\n</code></pre>"},{"location":"agency/MEMORY_PATTERNS/#query-type-2-logical-reasoning","title":"Query Type 2: Logical Reasoning","text":"<pre><code># Complex logic\nmemory.query(\n    \"experiments where chromatin improved F1 by more than 5% \"\n    \"AND were tested on cancer samples\"\n)\n# \u2192 LLM reasons over memory files, returns matching experiments\n</code></pre>"},{"location":"agency/MEMORY_PATTERNS/#query-type-3-temporal","title":"Query Type 3: Temporal","text":"<pre><code># Time-based\nmemory.query(\"what did we learn about histone marks last month?\")\n# \u2192 Returns experiments, papers, insights from February\n</code></pre>"},{"location":"agency/MEMORY_PATTERNS/#query-type-4-comparative","title":"Query Type 4: Comparative","text":"<pre><code># Comparison\nmemory.query(\"compare chromatin vs histone modality effectiveness\")\n# \u2192 Agent synthesizes from multiple memory files\n</code></pre>"},{"location":"agency/MEMORY_PATTERNS/#query-type-5-provenance-trace","title":"Query Type 5: Provenance Trace","text":"<pre><code># Full evidence chain\nmemory.trace(\"BRCA1 exon 11 cryptic donor discovery\")\n# \u2192 Discovery \u2192 Validation \u2192 Experiments \u2192 Papers \u2192 Raw data (full chain)\n</code></pre>"},{"location":"agency/MEMORY_PATTERNS/#integration-with-agentic-spliceai","title":"\ud83c\udfaf Integration with Agentic-SpliceAI","text":""},{"location":"agency/MEMORY_PATTERNS/#when-to-use-memory","title":"When to Use Memory","text":"Task Use Memory? Why Running predictions Optional Memory won't affect predictions Validating discoveries \u2705 Yes Use prior validations as context Designing experiments \u2705 Yes Learn from past experiments Writing papers \u2705 Yes Export provenance chains Training new models \u2705 Yes Apply learned best practices Debugging errors \u2705 Yes Check error pattern memory"},{"location":"agency/MEMORY_PATTERNS/#recommended-workflow","title":"Recommended Workflow","text":"<pre><code># 1. Initialize agent with memory\nagent = NexusAgent(memory=AgenticMemory())\n\n# 2. Query memory before acting\ncontext = agent.memory.query(\"relevant context for this task\")\n\n# 3. Perform task (prediction, validation, etc.)\nresult = agent.perform_task(context=context)\n\n# 4. Store results in memory\nagent.memory.store(result)\n\n# 5. Query insights\ninsights = agent.memory.query(\"what did we learn?\")\n</code></pre>"},{"location":"agency/MEMORY_PATTERNS/#resources","title":"\ud83d\udcda Resources","text":""},{"location":"agency/MEMORY_PATTERNS/#documentation","title":"Documentation","text":"<ul> <li>Tutorial: This document</li> <li>Implementation: <code>dev/planning/agentic_workflows/MEMORY_LAYER.md</code></li> <li>Research Notes: <code>dev/research/agency/agentic_memory.md</code></li> <li>Integration Strategy: <code>dev/research/agency/integration_plans/MEMU_INTEGRATION_STRATEGY.md</code></li> </ul>"},{"location":"agency/MEMORY_PATTERNS/#framework","title":"Framework","text":"<ul> <li>MemU: https://github.com/NevaMind-AI/memU</li> <li>Examples: See MemU repo <code>examples/</code></li> <li>API Docs: See MemU repo <code>docs/</code></li> </ul>"},{"location":"agency/MEMORY_PATTERNS/#related-work","title":"Related Work","text":"<ul> <li>LangMem: Memory for LangChain agents</li> <li>GPT Memory: OpenAI's memory feature</li> <li>Mem0: Another memory framework</li> <li>MemU: Best for scientific workflows (file-system, provenance)</li> </ul>"},{"location":"agency/MEMORY_PATTERNS/#getting-started","title":"\ud83c\udf89 Getting Started","text":""},{"location":"agency/MEMORY_PATTERNS/#quick-start-3-steps","title":"Quick Start (3 Steps)","text":"<p>Step 1: Install <pre><code>pip install agentic-spliceai[memory]\n</code></pre></p> <p>Step 2: Initialize <pre><code>from agentic_spliceai.memory import AgenticMemory\nmemory = AgenticMemory()\n</code></pre></p> <p>Step 3: Use <pre><code># Store\nmemory.store(category=\"experiments\", item={...})\n\n# Query\nresults = memory.query(\"your question\")\n\n# Enjoy cumulative intelligence! \ud83c\udf89\n</code></pre></p> <p>Questions? Open an issue on GitHub or see the full documentation.</p> <p>\"Make your AI agents remember. Make your science cumulative.\"</p>"},{"location":"base_layer/DATA_PREPARATION_CLI/","title":"Data Preparation CLI Guide","text":"<p>Command-line interface for preparing genomic data for splice site prediction.</p>"},{"location":"base_layer/DATA_PREPARATION_CLI/#quick-start","title":"Quick Start","text":"<pre><code># Install package (if not already installed)\npip install -e .\n\n# Extract data for specific genes\nagentic-spliceai-prepare --genes BRCA1 TP53 --output data/prepared/\n\n# Extract data for chromosome\nagentic-spliceai-prepare --chromosomes 21 --output data/prepared/\n</code></pre>"},{"location":"base_layer/DATA_PREPARATION_CLI/#overview","title":"Overview","text":"<p>The <code>agentic-spliceai-prepare</code> command extracts and prepares three types of genomic data:</p> <ol> <li>Gene annotations - Gene metadata from GTF files</li> <li>Sequences - DNA sequences from FASTA files</li> <li>Splice sites - Donor and acceptor splice site positions</li> </ol> <p>Output formats: TSV (default), Parquet, or both</p>"},{"location":"base_layer/DATA_PREPARATION_CLI/#basic-usage","title":"Basic Usage","text":""},{"location":"base_layer/DATA_PREPARATION_CLI/#extract-genes-and-sequences","title":"Extract Genes and Sequences","text":"<pre><code># Single gene\nagentic-spliceai-prepare --genes BRCA1 --output data/prepared/\n\n# Multiple genes\nagentic-spliceai-prepare --genes BRCA1 TP53 UNC13A --output data/prepared/\n\n# Entire chromosome\nagentic-spliceai-prepare --chromosomes 21 --output data/prepared/\n\n# Multiple chromosomes\nagentic-spliceai-prepare --chromosomes 1 2 3 --output data/prepared/\n</code></pre>"},{"location":"base_layer/DATA_PREPARATION_CLI/#output-files","title":"Output Files","text":"<p>By default, creates: - <code>genes.tsv</code> - Gene annotations - <code>sequences.tsv</code> - Gene sequences - <code>splice_sites_enhanced.tsv</code> - Splice site annotations - <code>preparation_summary.json</code> - Extraction summary</p>"},{"location":"base_layer/DATA_PREPARATION_CLI/#advanced-usage","title":"Advanced Usage","text":""},{"location":"base_layer/DATA_PREPARATION_CLI/#extract-only-splice-sites","title":"Extract Only Splice Sites","text":"<p>Useful for full genome splice site extraction:</p> <pre><code>agentic-spliceai-prepare --build GRCh38 \\\n  --output data/ensembl/GRCh38/ \\\n  --splice-sites-only\n</code></pre> <p>Why? Splice sites are expensive to extract (10-30 minutes for full genome), but once extracted, they can be reused indefinitely.</p>"},{"location":"base_layer/DATA_PREPARATION_CLI/#skip-sequence-extraction","title":"Skip Sequence Extraction","text":"<p>Faster if you only need annotations:</p> <pre><code>agentic-spliceai-prepare --genes BRCA1 TP53 \\\n  --output data/prepared/ \\\n  --skip-sequences\n</code></pre>"},{"location":"base_layer/DATA_PREPARATION_CLI/#force-re-extraction","title":"Force Re-extraction","text":"<p>Override cached files:</p> <pre><code>agentic-spliceai-prepare --genes BRCA1 \\\n  --output data/prepared/ \\\n  --force\n</code></pre>"},{"location":"base_layer/DATA_PREPARATION_CLI/#parquet-output","title":"Parquet Output","text":"<p>For efficient storage and loading:</p> <pre><code>agentic-spliceai-prepare --genes BRCA1 TP53 \\\n  --output data/prepared/ \\\n  --format parquet\n</code></pre> <p>Or both formats:</p> <pre><code>agentic-spliceai-prepare --genes BRCA1 TP53 \\\n  --output data/prepared/ \\\n  --format both\n</code></pre>"},{"location":"base_layer/DATA_PREPARATION_CLI/#custom-gtffasta-files","title":"Custom GTF/FASTA Files","text":"<pre><code>agentic-spliceai-prepare --genes BRCA1 \\\n  --gtf /path/to/custom/annotations.gtf \\\n  --fasta /path/to/custom/genome.fa \\\n  --output data/custom/\n</code></pre>"},{"location":"base_layer/DATA_PREPARATION_CLI/#options-reference","title":"Options Reference","text":""},{"location":"base_layer/DATA_PREPARATION_CLI/#target-selection","title":"Target Selection","text":"<pre><code>--genes GENE [GENE ...]       Gene symbols or IDs (e.g., BRCA1 TP53)\n--chromosomes CHR [CHR ...]   Chromosomes (e.g., 21 22 X Y)\n</code></pre> <p>Note: Provide either <code>--genes</code> or <code>--chromosomes</code>, not both.</p>"},{"location":"base_layer/DATA_PREPARATION_CLI/#build-and-source","title":"Build and Source","text":"<pre><code>--build BUILD                 Genome build (default: GRCh38)\n                              Options: GRCh38, GRCh37, GRCh38_MANE\n\n--annotation-source SOURCE    Annotation source (default: mane)\n                              Options: mane, ensembl, gencode\n</code></pre>"},{"location":"base_layer/DATA_PREPARATION_CLI/#custom-paths","title":"Custom Paths","text":"<pre><code>--gtf PATH                    Custom GTF file (overrides build/source)\n--fasta PATH                  Custom FASTA file (overrides build/source)\n</code></pre>"},{"location":"base_layer/DATA_PREPARATION_CLI/#output-options","title":"Output Options","text":"<pre><code>--output DIR, -o DIR          Output directory (required)\n--force                       Force re-extraction even if files exist\n--format FORMAT               Output format: tsv, parquet, both (default: tsv)\n</code></pre>"},{"location":"base_layer/DATA_PREPARATION_CLI/#content-selection","title":"Content Selection","text":"<pre><code>--splice-sites-only           Extract only splice sites (skip genes/sequences)\n--skip-sequences              Skip sequence extraction (annotations only)\n--skip-splice-sites           Skip splice site extraction\n</code></pre>"},{"location":"base_layer/DATA_PREPARATION_CLI/#verbosity","title":"Verbosity","text":"<pre><code>--verbosity {0,1,2}           Output verbosity (default: 1)\n                              0: minimal, 1: normal, 2: detailed\n</code></pre>"},{"location":"base_layer/DATA_PREPARATION_CLI/#output-formats","title":"Output Formats","text":""},{"location":"base_layer/DATA_PREPARATION_CLI/#genestsv","title":"genes.tsv","text":"<p>Gene annotations with columns: - <code>seqname</code> - Chromosome (e.g., 'chr17') - <code>gene_id</code> - Gene ID (e.g., 'ENSG00000012048') - <code>gene_name</code> - Gene symbol (e.g., 'BRCA1') - <code>start</code> - Start position (1-based) - <code>end</code> - End position (1-based) - <code>strand</code> - Strand ('+' or '-') - <code>gene_biotype</code> - Gene type (e.g., 'protein_coding')</p>"},{"location":"base_layer/DATA_PREPARATION_CLI/#sequencestsv","title":"sequences.tsv","text":"<p>Gene sequences with columns: - <code>gene_id</code> - Gene identifier - <code>gene_name</code> - Gene symbol - <code>seqname</code> - Chromosome - <code>start</code> - Start position - <code>end</code> - End position - <code>strand</code> - Strand - <code>sequence</code> - DNA sequence (uppercase)</p>"},{"location":"base_layer/DATA_PREPARATION_CLI/#splice_sites_enhancedtsv","title":"splice_sites_enhanced.tsv","text":"<p>Splice site annotations with columns: - <code>chrom</code> - Chromosome name - <code>start</code> - Start position (BED interval) - <code>end</code> - End position (BED interval) - <code>position</code> - Exact splice site position (1-based) - <code>strand</code> - Strand ('+' or '-') - <code>site_type</code> - 'donor' or 'acceptor' - <code>gene_id</code> - Gene identifier - <code>transcript_id</code> - Transcript identifier - <code>gene_name</code> - Gene symbol - <code>gene_biotype</code> - Gene biotype - <code>transcript_biotype</code> - Transcript biotype - <code>exon_id</code> - Exon identifier - <code>exon_number</code> - Exon number - <code>exon_rank</code> - Exon rank</p>"},{"location":"base_layer/DATA_PREPARATION_CLI/#preparation_summaryjson","title":"preparation_summary.json","text":"<p>Summary of extraction with: - Timestamp - Input parameters (build, genes, chromosomes) - Output file paths - Statistics (gene counts, splice site counts)</p>"},{"location":"base_layer/DATA_PREPARATION_CLI/#common-workflows","title":"Common Workflows","text":""},{"location":"base_layer/DATA_PREPARATION_CLI/#1-quick-gene-exploration","title":"1. Quick Gene Exploration","text":"<p>Extract data for a few genes:</p> <pre><code>agentic-spliceai-prepare --genes BRCA1 TP53 --output /tmp/quick_explore/\n</code></pre>"},{"location":"base_layer/DATA_PREPARATION_CLI/#2-prepare-training-data","title":"2. Prepare Training Data","text":"<p>Extract splice sites for specific genes:</p> <pre><code>agentic-spliceai-prepare --genes BRCA1 TP53 UNC13A \\\n  --output data/training/ \\\n  --format parquet\n</code></pre>"},{"location":"base_layer/DATA_PREPARATION_CLI/#3-full-genome-splice-sites","title":"3. Full Genome Splice Sites","text":"<p>Extract and cache all splice sites once:</p> <pre><code>agentic-spliceai-prepare --build GRCh38 \\\n  --output data/ensembl/GRCh38/ \\\n  --splice-sites-only\n</code></pre> <p>This creates <code>data/ensembl/GRCh38/splice_sites_enhanced.tsv</code> which can be reused by all subsequent operations.</p>"},{"location":"base_layer/DATA_PREPARATION_CLI/#4-chromosome-level-processing","title":"4. Chromosome-Level Processing","text":"<p>Process one chromosome at a time:</p> <pre><code>for chr in 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 X Y; do\n  agentic-spliceai-prepare --chromosomes $chr \\\n    --output data/chromosomes/chr${chr}/ \\\n    --format parquet\ndone\n</code></pre>"},{"location":"base_layer/DATA_PREPARATION_CLI/#performance-tips","title":"Performance Tips","text":""},{"location":"base_layer/DATA_PREPARATION_CLI/#caching","title":"Caching","text":"<ul> <li>Splice sites are cached automatically. If you run the command twice with the same output directory, it will reuse the existing <code>splice_sites_enhanced.tsv</code> file.</li> <li>Use <code>--force</code> to override caching.</li> </ul>"},{"location":"base_layer/DATA_PREPARATION_CLI/#storage","title":"Storage","text":"<ul> <li>TSV format: Human-readable, larger file size</li> <li>Parquet format: Binary, smaller size, faster loading</li> <li>For large datasets, use Parquet</li> </ul>"},{"location":"base_layer/DATA_PREPARATION_CLI/#memory","title":"Memory","text":"<ul> <li>Splice site extraction for full genome requires ~2-4 GB RAM</li> <li>Chromosome-level extraction requires ~500 MB - 1 GB RAM</li> <li>Gene-level extraction requires minimal RAM</li> </ul>"},{"location":"base_layer/DATA_PREPARATION_CLI/#troubleshooting","title":"Troubleshooting","text":""},{"location":"base_layer/DATA_PREPARATION_CLI/#issue-gtf-file-not-found","title":"Issue: GTF file not found","text":"<p>Solution: Specify custom path with <code>--gtf</code>:</p> <pre><code>agentic-spliceai-prepare --genes BRCA1 \\\n  --gtf /path/to/annotations.gtf \\\n  --fasta /path/to/genome.fa \\\n  --output data/prepared/\n</code></pre>"},{"location":"base_layer/DATA_PREPARATION_CLI/#issue-slow-extraction","title":"Issue: Slow extraction","text":"<p>Solution: Extract splice sites once for the entire build, then reuse:</p> <pre><code># One-time: Extract all splice sites (~10-30 minutes)\nagentic-spliceai-prepare --build GRCh38 \\\n  --output data/ensembl/GRCh38/ \\\n  --splice-sites-only\n\n# Fast: Extract genes (reuses cached splice sites)\nagentic-spliceai-prepare --genes BRCA1 \\\n  --output data/prepared/\n</code></pre>"},{"location":"base_layer/DATA_PREPARATION_CLI/#issue-out-of-memory","title":"Issue: Out of memory","text":"<p>Solution: Process one chromosome at a time:</p> <pre><code>agentic-spliceai-prepare --chromosomes 21 \\\n  --output data/chr21/\n</code></pre>"},{"location":"base_layer/DATA_PREPARATION_CLI/#integration-with-python-api","title":"Integration with Python API","text":"<p>The CLI wraps the Python API. You can also use directly:</p> <pre><code>from agentic_spliceai.splice_engine.base_layer.data import (\n    prepare_gene_data,\n    prepare_splice_site_annotations\n)\n\n# Extract genes and sequences\ngene_df = prepare_gene_data(genes=['BRCA1', 'TP53'])\n\n# Extract splice sites\nresult = prepare_splice_site_annotations(\n    output_dir='data/prepared',\n    genes=['BRCA1', 'TP53']\n)\nsplice_df = result['splice_sites_df']\n</code></pre> <p>See the Python API documentation for more details.</p>"},{"location":"base_layer/DATA_PREPARATION_CLI/#see-also","title":"See Also","text":"<ul> <li>Phase 2 Completion Report - Technical details</li> <li>Base Layer Architecture - System architecture</li> <li>Python API - Python interface</li> </ul>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/","title":"Base Layer: Processing Architecture","text":"<p>This document explains the core processing architecture of the base layer for splice site prediction. The base layer supports multiple foundational models (SpliceAI, OpenSpliceAI, and custom models) that predict splice sites from genomic sequences. Understanding this architecture is essential for:</p> <ul> <li>Configuring prediction workflows</li> <li>Optimizing memory usage for large-scale analyses</li> <li>Troubleshooting and debugging</li> <li>Extending the system with custom features</li> </ul>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Overview</li> <li>Base Model Protocol</li> <li>Data Preparation Pipeline</li> <li>Three-Level Processing Loop</li> <li>Memory Management Strategy</li> <li>Checkpoint and Resume</li> <li>Output Files</li> <li>Configuration Options</li> </ol>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#overview","title":"Overview","text":"<p>The base layer processes genomic data through a carefully designed pipeline that balances accuracy, memory efficiency, and fault tolerance. The architecture handles the challenge of processing ~20,000 genes across 24 chromosomes while keeping memory usage manageable.</p>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#key-design-principles","title":"Key Design Principles","text":"Principle Implementation Data Locality Process one chromosome at a time Fault Tolerance Checkpoint after every 500 genes Memory Efficiency Process in mini-batches of 50 genes Streaming Use lazy loading to avoid loading entire genome"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#high-level-flow","title":"High-Level Flow","text":"<pre><code>Input Files                    Processing                      Output Files\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                      \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nGTF annotations \u2500\u2500\u2510\n                  \u251c\u2500\u2500\u25ba Data Preparation \u2500\u2500\u25ba Nested Loop \u2500\u2500\u25ba Splice predictions\nFASTA genome \u2500\u2500\u2500\u2500\u2500\u2524                         (3 levels)       Error analysis\n                  \u2502                                          Position features\nBase models* \u2500\u2500\u2500\u2500\u2500\u2518                                          Sequence contexts\n\n* Base models: SpliceAI, OpenSpliceAI, or any model with standard I/O protocol\n</code></pre>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#base-model-protocol","title":"Base Model Protocol","text":"<p>The base layer is designed to support any splice prediction model that follows a standard input/output protocol. This enables extensibility and integration of new models as they become available.</p>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#supported-base-models","title":"Supported Base Models","text":"<p>Currently supported models:</p> <ol> <li>SpliceAI (Keras/TensorFlow)</li> <li>Original splice prediction model</li> <li>Ensemble of 5 models</li> <li> <p>Context windows: 80, 400, 2000, 10000 bp</p> </li> <li> <p>OpenSpliceAI (PyTorch)</p> </li> <li>Reimplementation of SpliceAI in PyTorch</li> <li>Improved training and inference</li> <li>Compatible with SpliceAI architecture</li> </ol>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#standard-io-protocol","title":"Standard I/O Protocol","text":"<p>Any base model can be integrated if it follows this protocol:</p>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#input-requirements","title":"Input Requirements","text":"<p>Format: One-hot encoded DNA sequence - Shape: <code>(batch_size, sequence_length, 4)</code> - Channels: <code>[A, C, G, T]</code> in order - Data type: <code>float32</code> - Sequence structure: <code>[flanking_context] + [5000bp core] + [flanking_context]</code></p> <p>Example: <pre><code># Input: Gene sequence\nsequence = \"ATCGATCG...\"  # DNA string\n\n# Convert to one-hot encoding\nencoded = one_hot_encode(sequence)  # Shape: (length, 4)\n\n# Add context and create blocks\nblocks = prepare_input_sequence(sequence, context=10000)\n# Output shape: (num_blocks, 20000, 4)\n</code></pre></p>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#output-requirements","title":"Output Requirements","text":"<p>Format: Splice site probabilities - Shape: <code>(batch_size, sequence_length, 3)</code> - Channels: <code>[donor_prob, acceptor_prob, neither_prob]</code> - Data type: <code>float32</code> - Range: <code>[0, 1]</code> (probabilities) - Constraint: <code>donor_prob + acceptor_prob + neither_prob \u2248 1.0</code> (per position)</p> <p>Example: <pre><code># Model prediction\npredictions = model.predict(blocks)\n# Shape: (num_blocks, 5000, 3)\n\n# Extract probabilities\ndonor_scores = predictions[:, :, 0]      # Donor splice site probability\nacceptor_scores = predictions[:, :, 1]   # Acceptor splice site probability\nneither_scores = predictions[:, :, 2]    # Neither (no splice site)\n</code></pre></p>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#integration-requirements","title":"Integration Requirements","text":"<p>To add a new base model:</p> <ol> <li> <p>Model Loading: Implement a loader function    <pre><code>def _load_custom_model(model_dir, verbosity):\n    \"\"\"Load custom model from disk.\"\"\"\n    # Load model weights, config, etc.\n    return model\n</code></pre></p> </li> <li> <p>Prediction Interface: Model must work with <code>predict_with_model()</code> <pre><code>def predict_with_model(model, x):\n    \"\"\"Universal prediction function.\"\"\"\n    if hasattr(model, 'predict'):\n        # Keras-style model\n        return model.predict(x, verbose=0)\n    elif hasattr(model, 'forward'):\n        # PyTorch-style model\n        import torch\n        with torch.no_grad():\n            device = next(model.parameters()).device\n            x_tensor = torch.FloatTensor(x).to(device)\n            return model(x_tensor).cpu().numpy()\n    else:\n        # Add custom inference logic here\n        raise NotImplementedError(f\"Unsupported model type: {type(model)}\")\n</code></pre></p> </li> <li> <p>Update Model Registry: Add to <code>load_spliceai_models()</code> <pre><code>def load_spliceai_models(model_dir=None, model_type='spliceai', verbosity=1):\n    if model_type.lower() == 'openspliceai':\n        return _load_openspliceai_models(model_dir, verbosity)\n    elif model_type.lower() == 'custom_model':\n        return _load_custom_model(model_dir, verbosity)\n    else:\n        return _load_spliceai_models(model_dir, verbosity)\n</code></pre></p> </li> </ol>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#model-compatibility-matrix","title":"Model Compatibility Matrix","text":"Model Framework Ensemble Context Status SpliceAI Keras 5 models 10k bp \u2705 Supported OpenSpliceAI PyTorch 5 models 10k bp \u2705 Supported Your Model Any Optional Flexible \ud83d\udd27 Easy to add"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#adding-your-own-model","title":"Adding Your Own Model","text":"<p>Step 1: Ensure your model follows the I/O protocol - Input: <code>(batch, seq_len, 4)</code> one-hot encoded DNA - Output: <code>(batch, seq_len, 3)</code> splice probabilities</p> <p>Step 2: Create a model loader <pre><code># In base_layer/prediction/core.py\ndef _load_my_model(model_dir, verbosity):\n    # Load your model\n    model = MyModelClass.from_pretrained(model_dir)\n    return [model]  # Return as list for consistency\n</code></pre></p> <p>Step 3: Update the model type switch <pre><code># In load_spliceai_models()\nif model_type.lower() == 'mymodel':\n    return _load_my_model(model_dir, verbosity)\n</code></pre></p> <p>Step 4: Test with a gene <pre><code>from agentic_spliceai.splice_engine.base_layer import BaseModelRunner\n\nrunner = BaseModelRunner()\nresult = runner.run_single_model(\n    model_name='mymodel',\n    target_genes=['BRCA1']\n)\n</code></pre></p>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#data-preparation-pipeline","title":"Data Preparation Pipeline","text":"<p>Before prediction begins, six preparation functions load and validate all required genomic resources.</p>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#preparation-steps","title":"Preparation Steps","text":"<pre><code>Step 1: prepare_gene_annotations()\n        \u2514\u2500\u2500 Extract transcript annotations from GTF file\n        \u2514\u2500\u2500 Output: annotations_all_transcripts.tsv\n\nStep 2: prepare_splice_site_annotations()\n        \u2514\u2500\u2500 Extract known splice sites (donors and acceptors)\n        \u2514\u2500\u2500 Output: splice_sites_enhanced.tsv\n\nStep 3: prepare_genomic_sequences()\n        \u2514\u2500\u2500 Extract gene sequences from FASTA\n        \u2514\u2500\u2500 Output: gene_sequences_chr{N}.parquet (per chromosome)\n\nStep 4: handle_overlapping_genes()\n        \u2514\u2500\u2500 Identify genes with overlapping regions\n        \u2514\u2500\u2500 Output: overlapping_gene_counts.tsv\n\nStep 5: determine_target_chromosomes()\n        \u2514\u2500\u2500 Determine which chromosomes to process\n        \u2514\u2500\u2500 Can infer from target genes if specified\n\nStep 6: load_base_models()\n        \u2514\u2500\u2500 Load pre-trained models (SpliceAI, OpenSpliceAI, or custom)\n        \u2514\u2500\u2500 Models predict donor/acceptor/neither probabilities\n        \u2514\u2500\u2500 Supports both Keras and PyTorch frameworks\n</code></pre>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#function-details","title":"Function Details","text":""},{"location":"base_layer/PROCESSING_ARCHITECTURE/#1-prepare_gene_annotations","title":"1. prepare_gene_annotations()","text":"<p>Extracts transcript-level annotations from GTF/GFF3 files.</p> <p>Key Parameters: - <code>local_dir</code>: Directory for output files (build-specific) - <code>gtf_file</code>: Path to GTF annotation file - <code>do_extract</code>: Whether to extract from GTF (vs. load existing) - <code>target_chromosomes</code>: Optional filter for specific chromosomes</p> <p>Output Schema: | Column | Type | Description | |--------|------|-------------| | chrom | str | Chromosome (1-22, X, Y) | | start | int | Feature start position | | end | int | Feature end position | | strand | str | Strand (+ or -) | | feature | str | Feature type (exon, CDS, UTR) | | gene_id | str | Ensembl gene ID | | transcript_id | str | Ensembl transcript ID |</p>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#2-prepare_splice_site_annotations","title":"2. prepare_splice_site_annotations()","text":"<p>Extracts known splice site positions from gene annotations.</p> <p>Key Parameters: - <code>gene_annotations_df</code>: Pre-loaded annotations for filtering - <code>consensus_window</code>: Window size for consensus calling (default: 2) - <code>fasta_file</code>: Required for OpenSpliceAI fallback mode</p> <p>Output Schema: | Column | Type | Description | |--------|------|-------------| | chrom | str | Chromosome | | position | int | Splice site position | | strand | str | Strand | | splice_type | str | 'donor' or 'acceptor' | | gene_id | str | Gene identifier |</p>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#3-prepare_genomic_sequences","title":"3. prepare_genomic_sequences()","text":"<p>Extracts nucleotide sequences for each gene from the reference genome.</p> <p>Key Parameters: - <code>mode</code>: 'gene' (full gene) or 'transcript' (per-transcript) - <code>seq_type</code>: 'full' (start to end) or 'minmax' (exon boundaries) - <code>seq_format</code>: Output format ('parquet' recommended) - <code>single_sequence_file</code>: Combine all chromosomes into one file</p> <p>Memory Consideration: Sequences are stored per-chromosome to enable streaming during prediction.</p>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#4-handle_overlapping_genes","title":"4. handle_overlapping_genes()","text":"<p>Identifies genes with overlapping genomic regions, which can cause ambiguous splice site assignments.</p> <p>Output: DataFrame with overlap counts per gene, useful for filtering or flagging ambiguous predictions.</p>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#5-determine_target_chromosomes","title":"5. determine_target_chromosomes()","text":"<p>Intelligently determines which chromosomes to process based on: 1. Explicitly specified chromosomes (highest priority) 2. Chromosomes containing target genes (if specified) 3. All standard chromosomes (default fallback)</p>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#6-load_spliceai_models","title":"6. load_spliceai_models()","text":"<p>Loads the pre-trained SpliceAI ensemble (5 Keras models). Each model independently predicts splice site probabilities, and predictions are averaged for robustness.</p>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#three-level-processing-loop","title":"Three-Level Processing Loop","text":"<p>The core prediction workflow uses a nested loop structure to efficiently process the genome while managing memory.</p>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#loop-structure","title":"Loop Structure","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  LEVEL 1: CHROMOSOME LOOP                                           \u2502\n\u2502  for chromosome in ['1', '2', ..., '22', 'X', 'Y']:                \u2502\n\u2502      \u2022 Load sequences for this chromosome only                      \u2502\n\u2502      \u2022 Stream from disk (lazy loading)                              \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  LEVEL 2: CHUNK LOOP (500 genes per chunk)                         \u2502\n\u2502  for chunk in range(0, n_genes, 500):                              \u2502\n\u2502      \u2022 Check if chunk already processed (checkpoint)                \u2502\n\u2502      \u2022 Materialize chunk from lazy frame                            \u2502\n\u2502      \u2022 Save results to disk after processing                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  LEVEL 3: MINI-BATCH LOOP (50 genes per batch)                     \u2502\n\u2502  for mini_batch in range(0, chunk_size, 50):                       \u2502\n\u2502      \u2022 Run SpliceAI prediction                                      \u2502\n\u2502      \u2022 Evaluate against known splice sites                          \u2502\n\u2502      \u2022 Extract sequence contexts                                    \u2502\n\u2502      \u2022 Free memory immediately                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#what-happens-at-each-level","title":"What Happens at Each Level","text":""},{"location":"base_layer/PROCESSING_ARCHITECTURE/#level-1-chromosome-data-locality","title":"Level 1: Chromosome (Data Locality)","text":"<p>Purpose: Load only one chromosome's data at a time to avoid memory overflow.</p> <pre><code>for chr_ in chromosomes:\n    # Load sequences for this chromosome only\n    lazy_seq_df = scan_chromosome_sequence(chromosome=chr_)\n\n    # Count genes on this chromosome\n    n_genes = lazy_seq_df.select(pl.col(\"gene_id\").n_unique()).collect().item()\n</code></pre> <p>Key Operations: - Lazy loading via Polars <code>LazyFrame</code> - Schema standardization - Gene counting</p>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#level-2-chunk-checkpointing","title":"Level 2: Chunk (Checkpointing)","text":"<p>Purpose: Enable resume capability and manage disk I/O.</p> <pre><code>chunk_size = 500  # genes per chunk\n\nfor chunk_start in range(0, n_genes, chunk_size):\n    # Check if already processed\n    if os.path.exists(chunk_artifact_file):\n        continue  # Skip - already done\n\n    # Materialize this chunk\n    seq_chunk = lazy_seq_df.slice(chunk_start, chunk_size).collect()\n\n    # ... process mini-batches ...\n\n    # Save chunk results to disk\n    data_handler.save_analysis_sequences(df_seq, chunk_start, chunk_end)\n    data_handler.save_splice_positions(positions_df, chunk_start, chunk_end)\n</code></pre> <p>Key Operations: - Checkpoint checking (resume support) - Chunk materialization - Result consolidation - Disk I/O (save artifacts)</p>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#level-3-mini-batch-memory-optimization","title":"Level 3: Mini-Batch (Memory Optimization)","text":"<p>Purpose: Keep peak memory usage low by processing small batches.</p> <pre><code>MINI_BATCH_SIZE = 50  # genes per mini-batch\n\nfor mini_batch_idx in range(n_mini_batches):\n    # Extract mini-batch\n    seq_mini_batch = seq_chunk[start:end]\n\n    # 1. PREDICT: Run SpliceAI models\n    predictions = predict_splice_sites_for_genes(\n        seq_mini_batch,\n        models=models,\n        context=10_000  # \u00b110kb context window\n    )\n\n    # 2. EVALUATE: Compare to known splice sites\n    error_df, positions_df = enhanced_process_predictions_with_all_scores(\n        predictions=predictions,\n        ss_annotations_df=ss_annotations_df,\n        add_derived_features=True  # Generate ~58 features\n    )\n\n    # 3. EXTRACT: Get sequence contexts\n    df_seq = extract_analysis_sequences(\n        seq_mini_batch,\n        positions_df,\n        window_size=250  # \u00b1250bp around splice site\n    )\n\n    # 4. FREE MEMORY\n    del seq_mini_batch, predictions\n    gc.collect()\n</code></pre> <p>Key Operations: - SpliceAI prediction (GPU-intensive) - Evaluation against ground truth - Sequence extraction - Aggressive memory cleanup</p>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#memory-management-strategy","title":"Memory Management Strategy","text":"<p>The architecture employs several strategies to keep memory usage bounded:</p>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#strategy-1-streaming-chromosome-level","title":"Strategy 1: Streaming (Chromosome Level)","text":"<pre><code>Instead of:  Load entire genome (100+ GB)\nWe do:       Load one chromosome at a time (2-8 GB)\n</code></pre>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#strategy-2-lazy-loading-chunk-level","title":"Strategy 2: Lazy Loading (Chunk Level)","text":"<pre><code>Instead of:  Load all genes into memory\nWe do:       Use LazyFrame, materialize only current chunk\n</code></pre>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#strategy-3-immediate-cleanup-mini-batch-level","title":"Strategy 3: Immediate Cleanup (Mini-Batch Level)","text":"<pre><code># After each mini-batch:\ndel seq_mini_batch, predictions_mini\ndel error_df_mini, positions_df_mini, df_seq_mini\ngc.collect()  # Force garbage collection\n</code></pre>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#memory-usage-pattern","title":"Memory Usage Pattern","text":"<pre><code>Memory\n  \u25b2\n  \u2502     \u250c\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2510\n  \u2502     \u2502MB1\u2502     \u2502MB2\u2502     \u2502MB3\u2502    Mini-batches\n  \u2502     \u2502   \u2502     \u2502   \u2502     \u2502   \u2502\n  \u2502  \u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2534\u2500\u2500\u2500\n  \u2502     \u2191   \u2193     \u2191   \u2193     \u2191   \u2193\n  \u2502   load free load free load free\n  \u2502\n  \u2502  \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n  \u2502  \u2502    Chunk accumulation      \u2502\n  \u2502  \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n  \u2502                              \u2193\n  \u2502                         Save &amp; Free\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba Time\n</code></pre>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#configuring-memory-usage","title":"Configuring Memory Usage","text":"Parameter Default Effect <code>chunk_size</code> 500 Larger = fewer disk writes, more memory <code>mini_batch_size</code> 50 Larger = faster GPU utilization, more memory <code>MAX_GENES_FOR_PRELOAD</code> 1000 Threshold for pre-loading vs. streaming"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#checkpoint-and-resume","title":"Checkpoint and Resume","text":"<p>The workflow automatically saves progress and can resume from interruptions.</p>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#how-checkpointing-works","title":"How Checkpointing Works","text":"<ol> <li> <p>Before processing each chunk, check if output file exists:    <pre><code>chunk_file = f\"analysis_sequences_{chr}_chunk_{start}_{end}.tsv\"\nif os.path.exists(chunk_file):\n    print(f\"Chunk already exists - SKIPPING\")\n    continue\n</code></pre></p> </li> <li> <p>After processing each chunk, save results:    <pre><code>data_handler.save_analysis_sequences(df_seq, chr, chunk_start, chunk_end)\ndata_handler.save_error_analysis(error_df, chr, chunk_start, chunk_end)\ndata_handler.save_splice_positions(positions_df, chr, chunk_start, chunk_end)\n</code></pre></p> </li> </ol>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#resume-behavior","title":"Resume Behavior","text":"<p>If the workflow is interrupted (crash, timeout, manual stop):</p> <ol> <li>Restart the workflow with the same parameters</li> <li>Completed chunks are automatically skipped</li> <li>Processing resumes from the first incomplete chunk</li> </ol>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#example-resume-scenario","title":"Example Resume Scenario","text":"<pre><code>First run (interrupted at chr3):\n  chr1: \u2713 Complete (all chunks saved)\n  chr2: \u2713 Complete (all chunks saved)\n  chr3: \u2717 Interrupted at chunk 501-1000\n\nSecond run (automatic resume):\n  chr1: SKIP (all chunks exist)\n  chr2: SKIP (all chunks exist)\n  chr3: SKIP chunks 1-500 (exists)\n        PROCESS chunks 501+ (resume here)\n</code></pre>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#output-files","title":"Output Files","text":"<p>The workflow generates several types of output files:</p>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#per-chunk-files-intermediate","title":"Per-Chunk Files (Intermediate)","text":"<pre><code>output_dir/\n\u251c\u2500\u2500 meta/\n\u2502   \u251c\u2500\u2500 analysis_sequences_1_chunk_1_500.tsv\n\u2502   \u251c\u2500\u2500 analysis_sequences_1_chunk_501_1000.tsv\n\u2502   \u251c\u2500\u2500 error_analysis_1_chunk_1_500.tsv\n\u2502   \u251c\u2500\u2500 splice_positions_1_chunk_1_500.tsv\n\u2502   \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#aggregated-files-final","title":"Aggregated Files (Final)","text":"<pre><code>output_dir/\n\u251c\u2500\u2500 splice_positions_enhanced_aggregated.tsv    # All positions\n\u251c\u2500\u2500 error_analysis_aggregated.tsv               # All error metrics\n\u2514\u2500\u2500 analysis_sequences_aggregated.tsv           # All sequences\n</code></pre>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#file-contents","title":"File Contents","text":"File Contents Use Case <code>splice_positions_*.tsv</code> Position-level features (~58 columns) Meta-model training <code>error_analysis_*.tsv</code> Per-gene error metrics (TP, FP, FN) Quality assessment <code>analysis_sequences_*.tsv</code> Nucleotide sequences around splice sites Sequence analysis"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#configuration-options","title":"Configuration Options","text":""},{"location":"base_layer/PROCESSING_ARCHITECTURE/#spliceaiconfig-parameters","title":"SpliceAIConfig Parameters","text":"<pre><code>from agentic_spliceai import SpliceAIConfig\n\nconfig = SpliceAIConfig(\n    # Core parameters\n    threshold=0.5,              # Detection threshold\n    consensus_window=2,         # Splice site consensus window\n    error_window=10,            # Error analysis window\n\n    # Memory optimization\n    mini_batch_size=50,         # Genes per mini-batch\n    chunk_size=500,             # Genes per chunk\n\n    # Output control\n    save_nucleotide_scores=False,  # Save per-nucleotide scores (large!)\n\n    # Paths\n    local_dir=\"./data/grch38\",  # Build-specific directory\n    gtf_file=\"./annotations.gtf\",\n    fasta_file=\"./genome.fa\"\n)\n</code></pre>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#workflow-parameters","title":"Workflow Parameters","text":"<pre><code>results = run_enhanced_splice_prediction_workflow(\n    config=config,\n\n    # Filtering\n    target_genes=['BRCA1', 'BRCA2'],  # Specific genes (optional)\n    target_chromosomes=['17', '13'],   # Specific chromosomes (optional)\n\n    # Behavior\n    verbosity=1,                # 0=quiet, 1=normal, 2=debug\n    no_final_aggregate=False,   # Skip final aggregation\n    no_tn_sampling=False,       # Include all true negatives\n\n    # Position ID format\n    position_id_mode='genomic'  # 'genomic' or 'relative'\n)\n</code></pre>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#summary","title":"Summary","text":"<p>The Meta-SpliceAI base layer processing architecture provides:</p> Feature Benefit Three-level nesting Balances memory, speed, and fault tolerance Lazy loading Handles genome-scale data without memory overflow Checkpointing Resume from interruptions without data loss Mini-batching Efficient GPU utilization with bounded memory Streaming output Results available as processing progresses <p>This architecture enables processing of the entire human genome (~20,000 genes) on machines with as little as 16GB RAM, while providing robust fault tolerance for long-running analyses.</p>"},{"location":"base_layer/PROCESSING_ARCHITECTURE/#see-also","title":"See Also","text":"<ul> <li>Base Layer Integration Summary</li> <li>Feature Set Documentation</li> <li>API Reference</li> </ul>"},{"location":"installation/LATEX_SETUP/","title":"LaTeX Setup for Equation Rendering","text":"<p>This guide explains how to set up LaTeX support for rendering mathematical equations in PDF reports generated by the Nexus Research Agent.</p>"},{"location":"installation/LATEX_SETUP/#why-latex","title":"Why LaTeX?","text":"<p>Scientific research reports often contain mathematical equations, formulas, and technical symbols. LaTeX is the gold standard for typesetting mathematics and ensures publication-quality equation rendering in PDFs.</p>"},{"location":"installation/LATEX_SETUP/#overview","title":"Overview","text":"<p>The Nexus Research Agent uses an intelligent format decision system:</p> <ol> <li>Analyzes the topic to detect if equations are needed</li> <li>Checks LLM capabilities for direct PDF generation</li> <li>Chooses optimal format:</li> <li>LaTeX \u2192 For math-heavy topics (Physics, Math, ML theory, etc.)</li> <li>Markdown \u2192 For general research (Business, Social sciences, etc.)</li> <li>Direct PDF \u2192 If LLM supports it (future capability)</li> </ol> <p>When LaTeX is chosen, the system: - Generates complete LaTeX source code - Compiles it with XeLaTeX - Produces a professional PDF with perfect equation rendering</p>"},{"location":"installation/LATEX_SETUP/#installation","title":"Installation","text":""},{"location":"installation/LATEX_SETUP/#macos","title":"macOS","text":""},{"location":"installation/LATEX_SETUP/#option-1-basictex-recommended-116mb","title":"Option 1: BasicTeX (Recommended - 116MB)","text":"<p>BasicTeX is a minimal TeX distribution perfect for most research reports.</p> <pre><code># Download BasicTeX\ncurl -L -o /tmp/BasicTeX.pkg https://mirror.ctan.org/systems/mac/mactex/BasicTeX.pkg\n\n# Install (requires sudo)\nsudo installer -pkg /tmp/BasicTeX.pkg -target /\n\n# Add to PATH\necho 'export PATH=\"/usr/local/texlive/2025basic/bin/universal-darwin:$PATH\"' &gt;&gt; ~/.zshrc\nsource ~/.zshrc\n\n# Verify installation\nxelatex --version\n</code></pre> <p>Expected output: <pre><code>XeTeX 3.141592653-2.6-0.999996 (TeX Live 2025)\n</code></pre></p>"},{"location":"installation/LATEX_SETUP/#option-2-full-mactex-7gb-if-you-need-extensive-latex-packages","title":"Option 2: Full MacTeX (7GB - if you need extensive LaTeX packages)","text":"<pre><code># Download from https://www.tug.org/mactex/\n# Or use Homebrew\nbrew install --cask mactex\n\n# Add to PATH\necho 'export PATH=\"/Library/TeX/texbin:$PATH\"' &gt;&gt; ~/.zshrc\nsource ~/.zshrc\n</code></pre>"},{"location":"installation/LATEX_SETUP/#linux-ubuntudebian","title":"Linux (Ubuntu/Debian)","text":"<pre><code># Install TeX Live\nsudo apt-get update\nsudo apt-get install texlive-xetex texlive-latex-extra\n\n# Verify\nxelatex --version\n</code></pre>"},{"location":"installation/LATEX_SETUP/#linux-fedorarhel","title":"Linux (Fedora/RHEL)","text":"<pre><code># Install TeX Live\nsudo dnf install texlive-xetex texlive-collection-latexextra\n\n# Verify\nxelatex --version\n</code></pre>"},{"location":"installation/LATEX_SETUP/#windows","title":"Windows","text":"<ol> <li>Download MiKTeX from https://miktex.org/download</li> <li>Run the installer</li> <li>During installation, choose \"Install missing packages automatically\"</li> <li>Add MiKTeX bin directory to PATH:</li> <li>Usually: <code>C:\\Program Files\\MiKTeX\\miktex\\bin\\x64\\</code></li> <li>Verify in PowerShell:    <pre><code>xelatex --version\n</code></pre></li> </ol>"},{"location":"installation/LATEX_SETUP/#verification","title":"Verification","text":"<p>Test that LaTeX compilation works:</p> <pre><code># Create a test LaTeX file\ncat &gt; /tmp/test.tex &lt;&lt; 'EOF'\n\\documentclass{article}\n\\usepackage{amsmath}\n\\begin{document}\nTest equation: $E = mc^2$\n\nDisplay equation:\n\\[\n\\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi}\n\\]\n\\end{document}\nEOF\n\n# Compile it\ncd /tmp\nxelatex test.tex\n\n# Check if PDF was created\nls -lh test.pdf\n</code></pre> <p>If <code>test.pdf</code> exists, LaTeX is working correctly!</p>"},{"location":"installation/LATEX_SETUP/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/LATEX_SETUP/#xelatex-command-not-found","title":"\"xelatex: command not found\"","text":"<p>Problem: LaTeX is not in your PATH.</p> <p>Solution (macOS): <pre><code># Find where XeLaTeX is installed\nfind /usr/local/texlive -name xelatex 2&gt;/dev/null\n\n# Add the directory to PATH\nexport PATH=\"/usr/local/texlive/2025basic/bin/universal-darwin:$PATH\"\n\n# Make it permanent\necho 'export PATH=\"/usr/local/texlive/2025basic/bin/universal-darwin:$PATH\"' &gt;&gt; ~/.zshrc\n</code></pre></p> <p>Solution (Linux): <pre><code># Usually in /usr/bin\nwhich xelatex\n\n# If not found, reinstall\nsudo apt-get install --reinstall texlive-xetex\n</code></pre></p>"},{"location":"installation/LATEX_SETUP/#latex-error-file-amsmathsty-not-found","title":"\"! LaTeX Error: File `amsmath.sty' not found\"","text":"<p>Problem: Missing LaTeX packages.</p> <p>Solution (BasicTeX on macOS): <pre><code># Install package manager\nsudo tlmgr update --self\n\n# Install missing packages\nsudo tlmgr install amsmath amssymb amsfonts\n</code></pre></p> <p>Solution (Linux): <pre><code># Install extra packages\nsudo apt-get install texlive-latex-extra\n</code></pre></p>"},{"location":"installation/LATEX_SETUP/#compilation-fails-with-emergency-stop","title":"Compilation fails with \"! Emergency stop\"","text":"<p>Problem: Syntax error in LaTeX code.</p> <p>Solution: 1. Check the <code>.log</code> file for specific error 2. The error usually indicates the line number 3. Common issues:    - Unescaped special characters: <code>_</code>, <code>%</code>, <code>&amp;</code>, <code>#</code>    - Mismatched braces: <code>{</code> without <code>}</code>    - Missing packages in preamble</p>"},{"location":"installation/LATEX_SETUP/#pdf-not-generated-after-compilation","title":"PDF not generated after compilation","text":"<p>Problem: Compilation completed but no PDF.</p> <p>Solution: <pre><code># Check for errors in the log\ncat document.log | grep \"Error\"\n\n# Try compiling twice (for references)\nxelatex document.tex\nxelatex document.tex\n</code></pre></p>"},{"location":"installation/LATEX_SETUP/#cross-platform-compatibility","title":"Cross-Platform Compatibility","text":"<p>Nexus automatically detects LaTeX installations across different platforms:</p>"},{"location":"installation/LATEX_SETUP/#automatic-path-detection","title":"Automatic Path Detection","text":"<p>The system searches for XeLaTeX in these locations:</p> <p>macOS: - <code>/usr/local/texlive/*/bin/*</code> (BasicTeX, TeX Live - all versions) - <code>/Library/TeX/texbin</code> (MacTeX) - <code>/usr/local/bin</code> (Homebrew)</p> <p>Linux: - <code>/usr/bin</code> (Standard package manager installations) - <code>/usr/local/texlive/bin</code></p> <p>Windows (WSL): - <code>/mnt/c/Program Files/MiKTeX/miktex/bin/x64</code> - <code>/mnt/c/texlive/bin/win32</code></p> <p>No configuration needed - the system will find your LaTeX installation automatically!</p> <p>If XeLaTeX is in your system PATH, it will work immediately. If not, the system tries common installation locations.</p>"},{"location":"installation/LATEX_SETUP/#how-nexus-uses-latex","title":"How Nexus Uses LaTeX","text":""},{"location":"installation/LATEX_SETUP/#1-format-decision","title":"1. Format Decision","text":"<p>When you request a research report, Nexus analyzes the topic:</p> <pre><code># Example: Physics topic\ntopic = \"Physics-Informed Neural Networks for PDEs\"\n\n# System detects: equations needed \u2192 chooses LaTeX format\nformat_decision = {\n    \"format\": \"latex\",\n    \"reasoning\": \"Topic involves physics and PDEs, equations needed\",\n    \"needs_equations\": True\n}\n</code></pre>"},{"location":"installation/LATEX_SETUP/#2-latex-generation","title":"2. LaTeX Generation","text":"<p>The Writer agent generates complete LaTeX source:</p> <pre><code>\\documentclass[11pt,article]{article}\n\\usepackage{amsmath,amssymb,amsfonts}\n\\usepackage{graphicx}\n\\usepackage{hyperref}\n\n\\title{Physics-Informed Neural Networks for Partial Differential Equations}\n\\author{Nexus Research Agent}\n\\date{\\today}\n\n\\begin{document}\n\\maketitle\n\n\\section{Introduction}\nPhysics-Informed Neural Networks (PINNs) combine...\n\nThe loss function is defined as:\n\\[\n\\mathcal{L} = \\mathcal{L}_{\\text{data}} + \\lambda \\mathcal{L}_{\\text{PDE}}\n\\]\n\n\\end{document}\n</code></pre>"},{"location":"installation/LATEX_SETUP/#3-compilation","title":"3. Compilation","text":"<p>Nexus compiles the LaTeX to PDF:</p> <pre><code>from nexus.agents.research.pdf_utils import latex_to_pdf\n\nsuccess, error = latex_to_pdf(\n    latex_content=latex_source,\n    output_path=Path(\"output/report.pdf\")\n)\n</code></pre>"},{"location":"installation/LATEX_SETUP/#4-result","title":"4. Result","text":"<p>You get a professional PDF with beautifully rendered equations!</p>"},{"location":"installation/LATEX_SETUP/#package-requirements","title":"Package Requirements","text":"<p>The system automatically includes these LaTeX packages:</p> <ul> <li><code>amsmath</code> - Advanced math typesetting</li> <li><code>amssymb</code> - Math symbols</li> <li><code>amsfonts</code> - Math fonts</li> <li><code>graphicx</code> - Graphics support</li> <li><code>hyperref</code> - Hyperlinks and PDF metadata</li> <li><code>geometry</code> - Page layout</li> </ul> <p>If your reports need additional packages, they can be added to the LaTeX template in <code>format_decision.py</code>.</p>"},{"location":"installation/LATEX_SETUP/#performance","title":"Performance","text":"<ul> <li>BasicTeX installation: ~2 minutes</li> <li>First LaTeX compilation: ~5-10 seconds (package loading)</li> <li>Subsequent compilations: ~2-3 seconds</li> <li>Disk space: 116MB (BasicTeX) to 7GB (Full MacTeX)</li> </ul>"},{"location":"installation/LATEX_SETUP/#best-practices","title":"Best Practices","text":"<ol> <li>Use BasicTeX unless you need specialized packages</li> <li>Keep TeX Live updated: <code>sudo tlmgr update --all</code></li> <li>Check PATH if commands aren't found</li> <li>Run compilation twice for references and citations</li> <li>Check logs if PDF isn't generated</li> </ol>"},{"location":"installation/LATEX_SETUP/#alternative-markdown-for-non-technical-reports","title":"Alternative: Markdown for Non-Technical Reports","text":"<p>If your research doesn't need equations, Nexus automatically uses Markdown:</p> <pre><code># Example: Social science topic\nnexus-research \"Impact of Social Media on Society\" --pdf\n\n# System detects: no equations needed \u2192 uses Markdown\n# Faster generation, smaller dependencies\n</code></pre>"},{"location":"installation/LATEX_SETUP/#support","title":"Support","text":"<p>If you encounter issues:</p> <ol> <li>Check this guide's Troubleshooting section</li> <li>Verify LaTeX installation: <code>xelatex --version</code></li> <li>Test with the verification example above</li> <li>Check the Nexus logs for specific errors</li> </ol>"},{"location":"installation/LATEX_SETUP/#references","title":"References","text":"<ul> <li>TeX Live Documentation</li> <li>LaTeX Project</li> <li>BasicTeX</li> <li>XeLaTeX</li> </ul>"},{"location":"isoform_discovery/","title":"Isoform Discovery: Context-Aware Splice Prediction","text":"<p>Status: Research &amp; Planning Phase Goal: Discover novel isoforms induced by variants, disease, stress, and other external factors</p>"},{"location":"isoform_discovery/#vision","title":"\ud83c\udfaf Vision","text":"<p>Problem: Current annotations (MANE, Ensembl) capture known isoforms, but miss: - Disease-specific isoforms - Variant-induced alternative splicing - Stress-response transcripts - Tissue-specific rare isoforms - Developmental stage-specific variants - Environmental condition-specific splicing</p> <p>Solution: Use Meta-SpliceAI's adaptive prediction to discover novel isoforms by: 1. Meta Layer: Detect context-dependent splice sites beyond canonical annotations 2. Agentic Layer: Validate predictions with literature and experimental evidence 3. Integration: Combine with RNA-seq, clinical variants, and disease databases</p>"},{"location":"isoform_discovery/#current-state-vs-future-vision","title":"\ud83d\udcca Current State vs Future Vision","text":""},{"location":"isoform_discovery/#current-canonical-isoform-prediction","title":"Current: Canonical Isoform Prediction","text":"<pre><code>Base Layer (MANE)\n    \u2193\nPredict 1-2 canonical transcripts per gene\n    \u2193\nValidated against known annotations\n</code></pre> <p>Coverage: ~44 splice sites per gene (BRCA1 example)</p>"},{"location":"isoform_discovery/#future-context-aware-isoform-discovery","title":"Future: Context-Aware Isoform Discovery","text":"<pre><code>Base Layer (MANE baseline)\n    \u2193\nMeta Layer (adaptive refinement)\n    \u2193\nNovel Splice Site Detection\n    \u2193\nIsoform Reconstruction\n    \u2193\nAgentic Validation (literature + experimental)\n    \u2193\nNovel Isoform Catalog\n</code></pre> <p>Potential Coverage: 1,218+ splice sites per gene (Ensembl-level + novel discoveries)</p>"},{"location":"isoform_discovery/#proposed-architecture","title":"\ud83c\udfd7\ufe0f Proposed Architecture","text":""},{"location":"isoform_discovery/#layer-1-meta-layer-novel-splice-site-detection","title":"Layer 1: Meta Layer - Novel Splice Site Detection","text":"<p>Goal: Identify high-confidence splice sites beyond MANE canonical set</p> <p>Components:</p> <ol> <li>Delta Score Analysis</li> <li>Compare meta-layer predictions to base-layer</li> <li>High delta = context-dependent splice site</li> <li> <p>Flag sites with high confidence but not in MANE</p> </li> <li> <p>Context Clustering</p> </li> <li>Group similar contexts (same variants, disease, tissue)</li> <li>Identify context-specific splice patterns</li> <li> <p>Discover condition-dependent isoforms</p> </li> <li> <p>Isoform Reconstruction</p> </li> <li>Assemble detected splice sites into full transcripts</li> <li>Validate transcript structure (ORF, NMD rules)</li> <li> <p>Generate novel isoform annotations</p> </li> <li> <p>Confidence Scoring</p> </li> <li>Multi-factor confidence score:<ul> <li>Meta-layer prediction strength</li> <li>Conservation across species</li> <li>RNA-seq evidence (if available)</li> <li>Splice motif strength</li> <li>Consistency with biological rules</li> </ul> </li> </ol> <p>Output: <pre><code>Novel Splice Sites:\n- Position: chr17:43,067,608 (donor)\n- Confidence: 0.92\n- Context: BRCA1-c.68_69del variant\n- Evidence: High delta score (0.45), strong motif\n- Status: Candidate for validation\n</code></pre></p>"},{"location":"isoform_discovery/#layer-2-agentic-layer-evidence-aggregation","title":"Layer 2: Agentic Layer - Evidence Aggregation","text":"<p>Goal: Validate novel isoforms with external evidence</p> <p>Components:</p> <ol> <li>Literature Research Agent</li> <li>Query PubMed for reports of:<ul> <li>Novel isoforms in gene of interest</li> <li>Disease-associated alternative splicing</li> <li>Functional impact of splice variants</li> </ul> </li> <li>Extract evidence from papers</li> <li> <p>Build evidence graph</p> </li> <li> <p>Database Integration Agent</p> </li> <li>RNA-seq Data:<ul> <li>GTEX (tissue-specific expression)</li> <li>Cancer atlases (tumor-specific isoforms)</li> <li>Disease cohorts</li> </ul> </li> <li>Variant Databases:<ul> <li>ClinVar (pathogenic splice variants)</li> <li>gnomAD (population-level splice variants)</li> </ul> </li> <li> <p>Isoform Databases:</p> <ul> <li>GENCODE (comprehensive annotations)</li> <li>RefSeq (curated transcripts)</li> </ul> </li> <li> <p>Validation Workflow Agent</p> </li> <li>Propose validation experiments:<ul> <li>RT-PCR primers for novel junction</li> <li>Minigene assay design</li> <li>CRISPR screens for splice modifiers</li> </ul> </li> <li> <p>Prioritize candidates by:</p> <ul> <li>Clinical relevance</li> <li>Functional impact</li> <li>Validation feasibility</li> </ul> </li> <li> <p>Evidence Aggregation</p> </li> <li>Combine predictions with external evidence</li> <li>Score novel isoforms by evidence strength:<ul> <li>RNA-seq support: High (direct evidence)</li> <li>Literature support: Medium (indirect evidence)</li> <li>Prediction only: Low (needs validation)</li> </ul> </li> </ol> <p>Output: <pre><code>Novel Isoform Report:\n- Gene: BRCA1\n- Novel Junction: chr17:43,067,608-43,082,575\n- Context: Tumor samples, BRCA1-mutant\n- Meta-Layer Confidence: 0.92\n- RNA-seq Evidence: 15/120 tumor samples (12.5%)\n- Literature: 3 papers report similar junction\n- Validation Priority: HIGH (clinical relevance)\n- Proposed Experiment: RT-PCR primers designed\n</code></pre></p>"},{"location":"isoform_discovery/#data-integration-strategy","title":"\ud83d\udd2c Data Integration Strategy","text":""},{"location":"isoform_discovery/#primary-data-sources","title":"Primary Data Sources","text":"<ol> <li>RNA-seq Data</li> <li>GTEX: Tissue-specific isoforms (53 tissues, 17,382 samples)</li> <li>TCGA: Cancer-specific isoforms (33 cancer types)</li> <li>GTEx + TCGA junction files: Direct evidence of novel splice junctions</li> <li> <p>Custom cohorts: Disease-specific RNA-seq</p> </li> <li> <p>Variant Data</p> </li> <li>ClinVar: Pathogenic splice variants</li> <li>gnomAD: Population-level splice variants</li> <li>COSMIC: Cancer-associated mutations</li> <li> <p>Personal genomes: Individual-level predictions</p> </li> <li> <p>Epigenetic Data</p> </li> <li>ENCODE: Histone marks, chromatin state</li> <li>Roadmap Epigenomics: Tissue-specific epigenomes</li> <li> <p>ChIP-seq: Splicing factor binding</p> </li> <li> <p>Conservation Data</p> </li> <li>PhyloP: Evolutionary conservation</li> <li>PhastCons: Conserved elements</li> <li>Cross-species comparison: Mouse, zebrafish homologs</li> </ol>"},{"location":"isoform_discovery/#data-processing-pipeline","title":"Data Processing Pipeline","text":"<pre><code>1. Load Base Predictions (MANE canonical)\n   \u2193\n2. Load Meta Predictions (context-aware)\n   \u2193\n3. Compute Delta Scores\n   \u2193\n4. Filter High-Confidence Novel Sites\n   \u2193\n5. Cross-reference with RNA-seq Junctions\n   \u2193\n6. Query Literature for Evidence\n   \u2193\n7. Score and Rank Candidates\n   \u2193\n8. Generate Discovery Reports\n</code></pre>"},{"location":"isoform_discovery/#use-cases","title":"\ud83d\udccb Use Cases","text":""},{"location":"isoform_discovery/#1-disease-specific-isoforms","title":"1. Disease-Specific Isoforms","text":"<p>Scenario: Identify breast cancer-specific BRCA1 isoforms</p> <p>Workflow: <pre><code># Predict on tumor samples with BRCA1 variants\nagentic-spliceai meta predict \\\n  --gene BRCA1 \\\n  --variants tumor_variants.vcf \\\n  --context tumor \\\n  --discover-isoforms\n\n# Validate with TCGA RNA-seq\nagentic-spliceai isoform validate \\\n  --predictions brca1_novel_isoforms.tsv \\\n  --rnaseq-cohort TCGA-BRCA \\\n  --output validation_report.html\n</code></pre></p> <p>Expected Output: - 10-20 high-confidence novel splice sites - 3-5 novel isoforms with RNA-seq support - Literature evidence for functional impact - Clinical relevance scores</p>"},{"location":"isoform_discovery/#2-variant-induced-splicing-changes","title":"2. Variant-Induced Splicing Changes","text":"<p>Scenario: Patient has VUS (variant of uncertain significance) near splice site</p> <p>Workflow: <pre><code># Predict impact of variant on splicing\nagentic-spliceai variant predict \\\n  --vcf patient_vus.vcf \\\n  --gene-list candidate_genes.txt \\\n  --detect-novel-junctions\n\n# Research evidence\nagentic-spliceai agentic research \\\n  --novel-junctions detected_junctions.tsv \\\n  --query \"pathogenic splicing impact\" \\\n  --sources pubmed,clinvar,splicevardb\n</code></pre></p> <p>Expected Output: - Novel splice junctions induced by variant - Predicted functional impact (NMD, truncation, etc.) - Literature evidence for similar variants - Clinical interpretation (likely pathogenic, benign, etc.)</p>"},{"location":"isoform_discovery/#3-tissue-specific-isoform-discovery","title":"3. Tissue-Specific Isoform Discovery","text":"<p>Scenario: Discover brain-specific isoforms relevant to neurological disease</p> <p>Workflow: <pre><code># Compare across tissues\nagentic-spliceai isoform discover \\\n  --gene-list neurological_genes.txt \\\n  --tissues brain,cerebellum,cortex \\\n  --reference-tissues heart,liver,kidney \\\n  --rnaseq-source GTEX\n\n# Validate with brain RNA-seq\nagentic-spliceai isoform validate \\\n  --candidates brain_specific_isoforms.tsv \\\n  --rnaseq-cohort GTEX-Brain \\\n  --min-samples 5\n</code></pre></p> <p>Expected Output: - Brain-specific isoforms not in MANE - Expression patterns across brain regions - Developmental trajectories - Disease associations</p>"},{"location":"isoform_discovery/#implementation-roadmap","title":"\ud83d\udee0\ufe0f Implementation Roadmap","text":""},{"location":"isoform_discovery/#phase-5-meta-layer-foundation","title":"Phase 5 (Meta Layer) - Foundation","text":"<p>Add to existing meta-layer work: - \u2705 Delta score computation (already exists in ValidatedDeltaPredictor) - \ud83c\udd95 Novel splice site detector - \ud83c\udd95 Confidence scoring module - \ud83c\udd95 Context clustering</p> <p>Deliverables: - Detect novel splice sites with high confidence - Score sites by multiple evidence lines - Group by context (variants, disease, tissue)</p>"},{"location":"isoform_discovery/#phase-8-new-isoform-discovery-validation","title":"Phase 8 (NEW) - Isoform Discovery &amp; Validation","text":"<p>Dedicated phase for isoform discovery:</p>"},{"location":"isoform_discovery/#81-meta-layer-extensions","title":"8.1: Meta Layer Extensions","text":"<ul> <li>Isoform reconstruction from splice sites</li> <li>Transcript structure validation (ORF, NMD)</li> <li>Novel isoform annotation format</li> <li>Integration with base predictions</li> </ul>"},{"location":"isoform_discovery/#82-rna-seq-integration","title":"8.2: RNA-seq Integration","text":"<ul> <li>GTEX junction file parser</li> <li>TCGA junction file parser</li> <li>Custom cohort support</li> <li>Junction-level validation</li> </ul>"},{"location":"isoform_discovery/#83-agentic-validation","title":"8.3: Agentic Validation","text":"<ul> <li>Isoform research agent (literature mining)</li> <li>Evidence aggregation (multi-source)</li> <li>Validation workflow generator</li> <li>Discovery report generation</li> </ul>"},{"location":"isoform_discovery/#84-clinical-applications","title":"8.4: Clinical Applications","text":"<ul> <li>Variant-induced isoform prediction</li> <li>Disease-specific isoform catalog</li> <li>VUS interpretation workflow</li> <li>Therapeutic target identification</li> </ul> <p>Estimated Timeline: 4-6 weeks (after Phase 7)</p>"},{"location":"isoform_discovery/#success-metrics","title":"\ud83d\udcca Success Metrics","text":""},{"location":"isoform_discovery/#discovery-metrics","title":"Discovery Metrics","text":"<ul> <li>Novel isoforms discovered: Target 100+ across 50 genes</li> <li>RNA-seq validation rate: &gt;70% with junction support</li> <li>Literature confirmation: &gt;50% with supporting papers</li> <li>Functional impact: &gt;30% affect protein function</li> </ul>"},{"location":"isoform_discovery/#validation-metrics","title":"Validation Metrics","text":"<ul> <li>Precision: % of predictions with RNA-seq support</li> <li>Recall: % of known rare isoforms detected</li> <li>Clinical utility: Impact on VUS interpretation</li> <li>Novel discoveries: Isoforms not in any database</li> </ul>"},{"location":"isoform_discovery/#research-questions","title":"\ud83d\udd2c Research Questions","text":""},{"location":"isoform_discovery/#biological-questions","title":"Biological Questions","text":"<ol> <li>How common are context-dependent isoforms?</li> <li>Proportion of genes with disease-specific isoforms</li> <li>Tissue-specificity of alternative splicing</li> <li> <p>Variant-induced splicing changes</p> </li> <li> <p>What contexts induce novel splicing?</p> </li> <li>Stress response</li> <li>Developmental stages</li> <li>Disease progression</li> <li> <p>Therapeutic interventions</p> </li> <li> <p>Can we predict functional impact?</p> </li> <li>NMD escape</li> <li>Protein domain loss/gain</li> <li>Clinical significance</li> </ol>"},{"location":"isoform_discovery/#technical-questions","title":"Technical Questions","text":"<ol> <li>What confidence threshold for novel sites?</li> <li>Balance precision vs recall</li> <li>Context-dependent thresholds</li> <li> <p>Multi-factor scoring</p> </li> <li> <p>How to reconstruct full isoforms?</p> </li> <li>Splice site pairing</li> <li>Exon inclusion/exclusion</li> <li> <p>ORF prediction</p> </li> <li> <p>How to validate computationally?</p> </li> <li>RNA-seq evidence strength</li> <li>Conservation requirements</li> <li>Motif scoring</li> </ol>"},{"location":"isoform_discovery/#related-work","title":"\ud83d\udcda Related Work","text":""},{"location":"isoform_discovery/#literature","title":"Literature","text":"<ol> <li>Disease-specific splicing:</li> <li>Cancer-specific isoforms (TCGA studies)</li> <li>Neurological disease splicing (autism, ALS)</li> <li> <p>Cardiac disease isoforms (cardiomyopathy)</p> </li> <li> <p>Computational approaches:</p> </li> <li>LeafCutter (differential splicing)</li> <li>MAJIQ (local splice variation)</li> <li>rMATS (alternative splicing events)</li> <li> <p>SUPPA2 (isoform quantification)</p> </li> <li> <p>Validation methods:</p> </li> <li>RT-PCR validation</li> <li>Long-read sequencing (PacBio, Nanopore)</li> <li>Minigene assays</li> </ol>"},{"location":"isoform_discovery/#databases","title":"Databases","text":"<ul> <li>GENCODE: Comprehensive gene annotations</li> <li>RefSeq: Curated transcript sequences</li> <li>APPRIS: Principal isoform annotations</li> <li>IsoformAtlas: Tissue-specific isoforms</li> <li>SpliceVault: Alternative splicing database</li> </ul>"},{"location":"isoform_discovery/#key-innovations","title":"\ud83d\udca1 Key Innovations","text":""},{"location":"isoform_discovery/#1-context-aware-discovery","title":"1. Context-Aware Discovery","text":"<p>Beyond static annotations: Predict isoforms specific to: - Individual genetic backgrounds - Disease states - Tissue/cell types - Environmental conditions</p>"},{"location":"isoform_discovery/#2-multi-modal-evidence-integration","title":"2. Multi-Modal Evidence Integration","text":"<p>Combine: - Base-layer predictions (canonical) - Meta-layer predictions (adaptive) - RNA-seq data (direct evidence) - Literature (biological context) - Conservation (evolutionary support)</p>"},{"location":"isoform_discovery/#3-agentic-validation","title":"3. Agentic Validation","text":"<p>LLM-powered: - Hypothesis generation - Evidence synthesis - Experimental design - Report generation</p>"},{"location":"isoform_discovery/#next-steps","title":"\ud83c\udfaf Next Steps","text":""},{"location":"isoform_discovery/#immediate-research-phase","title":"Immediate (Research Phase)","text":"<ol> <li>Literature review:</li> <li>Survey isoform discovery methods</li> <li>Identify key use cases</li> <li> <p>Define success criteria</p> </li> <li> <p>Data inventory:</p> </li> <li>Available RNA-seq datasets</li> <li>Variant databases</li> <li> <p>Existing novel isoform catalogs</p> </li> <li> <p>Prototype design:</p> </li> <li>Novel splice site detector</li> <li>Confidence scoring</li> <li>Validation workflow</li> </ol>"},{"location":"isoform_discovery/#short-term-phase-5-integration","title":"Short Term (Phase 5 Integration)","text":"<ol> <li>Extend meta layer:</li> <li>Add delta score analysis</li> <li>Implement confidence scoring</li> <li> <p>Create novel site detector</p> </li> <li> <p>Test on known cases:</p> </li> <li>Disease-associated isoforms</li> <li>Variant-induced changes</li> <li>Tissue-specific examples</li> </ol>"},{"location":"isoform_discovery/#long-term-phase-8","title":"Long Term (Phase 8)","text":"<ol> <li>Full implementation:</li> <li>Isoform reconstruction</li> <li>RNA-seq integration</li> <li>Agentic validation</li> <li> <p>Clinical workflows</p> </li> <li> <p>Validation study:</p> </li> <li>Experimental validation</li> <li>Clinical cohorts</li> <li>Publication</li> </ol>"},{"location":"isoform_discovery/#documentation-structure","title":"\ud83d\udcc1 Documentation Structure","text":"<pre><code>docs/isoform_discovery/\n\u251c\u2500\u2500 README.md                          \u2190 This file - Vision &amp; overview\n\u251c\u2500\u2500 ARCHITECTURE.md                    \u2190 Technical architecture\n\u251c\u2500\u2500 USE_CASES.md                       \u2190 Detailed use cases\n\u251c\u2500\u2500 DATA_INTEGRATION.md                \u2190 Data sources &amp; formats\n\u251c\u2500\u2500 VALIDATION_METHODS.md              \u2190 How to validate discoveries\n\u2514\u2500\u2500 RESEARCH_QUESTIONS.md              \u2190 Open questions &amp; experiments\n\ndev/research/isoform_discovery/\n\u251c\u2500\u2500 brainstorming/                     \u2190 Ideas, sketches\n\u251c\u2500\u2500 literature_review/                 \u2190 Paper summaries\n\u251c\u2500\u2500 prototype/                         \u2190 Experimental code\n\u2514\u2500\u2500 experiments/                       \u2190 Pilot studies\n</code></pre>"},{"location":"isoform_discovery/#collaboration-opportunities","title":"\ud83e\udd1d Collaboration Opportunities","text":""},{"location":"isoform_discovery/#academic-partners","title":"Academic Partners","text":"<ul> <li>Tissue-specific isoform experts</li> <li>Disease cohort access</li> <li>Experimental validation labs</li> </ul>"},{"location":"isoform_discovery/#clinical-partners","title":"Clinical Partners","text":"<ul> <li>VUS interpretation needs</li> <li>Patient cohorts</li> <li>Clinical validation</li> </ul>"},{"location":"isoform_discovery/#industry-partners","title":"Industry Partners","text":"<ul> <li>RNA-seq datasets</li> <li>Therapeutic target discovery</li> <li>Diagnostic development</li> </ul> <p>Status: \ud83d\udd2c Research &amp; Planning Next: Extend meta layer (Phase 5) with novel site detection Long-term: Dedicated isoform discovery phase (Phase 8)</p> <p>Questions? Ideas? Add to: <code>dev/research/isoform_discovery/brainstorming/</code></p>"}]}